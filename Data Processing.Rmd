---
title: "Data Processing"
author: "Milla Pihlajamaki and Caitlin Dawson"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: pdf_document
---

# Notes 
* takes data downloaded from Gorilla **blinded, semicolon separated, csv format, short form, versions 22-25**
* cleans and processes data for the study 'Scientific thinking and decision-making in everyday life'

Version history: 
* version 22: Links fixed in information and consent
* version 23: HJ updated the experiment (source deleted from CI task--only 5 sources because of trouble getting a stable URL; Viiskunta source was taken down)
* version 24: HJ updated CI task to include all 6 sources again, one of them as image
* version 25: HJ. Word adult taken off from Study Information and Content

# Load packages

```{r Load Packages}

library(corrplot)
library(dplyr)
library(ggplot2)
library(lavaan)
library(naniar)
library(psych)
library(psycho)
library(tidyr)

```

# Load Data 

This chunk loads the data and omits experiment-general columns from all files (except for participant private ID).

```{r Load Data}

# Versions 22-25
versions <- paste0("v", 22:25)
questionnaire_types <- c("2wlk", "79hv", "7qsg", "81k4", "c7cw", "cl1u", "eygv", "go4a", "mz16", "o43u", "otb8", "oyla", "w8n1")
task_types <- c("7mar", "8mu5", "9nll", "n8ns", "tg12", "zryk")
questionnaire_names <- c("epiQ", "heurQ", "openQ", "validQ", "demoQ", "sciattQ", "needcogQ", "scicurQ", "needcloQ", "infoQ", "rpQ", "inthumQ", "big5Q")
task_names <- c("uutT", "NavonT", "NeckerT", "nogoT", "matreasT", "citiT")
names_all <- c(questionnaire_names, task_names)

for (j in 1:length(versions)) {
  for (i in 1:length(questionnaire_types)) {
  assign(paste0("data_", questionnaire_names[i], "_", versions[j]),
         read.csv(paste0("data_exp_55551-", versions[j], "_questionnaire-", questionnaire_types[i], ".csv"), sep = ";")[, -c(1:12, 14:31)])
    }
}

for (j in 1:length(versions)) {
  for (i in 1:length(task_types)) {
    assign(paste0("data_", task_names[i], "_", versions[j]),
           read.csv(paste0("data_exp_55551-", versions[j], "_task-", task_types[i], ".csv"), sep = ";")[, -c(1:12, 14:31)])
  }
}

```

# Create Questionnaire Dataframe

This series of chunks computes the necessary data cleaning to merge the questionnaire dataframes and merges them.

## Merge the Different Versions of Each Questionnaire

This chunk combines the different versions of each questionnaire (leaving us with one df per questionnaire).

```{r Merge the Versions}

# Concatenate the versions of each questionnaire
data_big5Q <- rbind(data_big5Q_v22, data_big5Q_v23, 
                    data_big5Q_v24, data_big5Q_v25) 
data_citiT <- rbind(data_citiT_v22, data_citiT_v23, 
                    data_citiT_v24, data_citiT_v25) 
data_demoQ <- rbind(data_demoQ_v22, data_demoQ_v23, 
                    data_demoQ_v24, data_demoQ_v25)
data_epiQ <- rbind(data_epiQ_v22, data_epiQ_v23, 
                   data_epiQ_v24, data_epiQ_v25)
data_nogoT <- rbind(data_nogoT_v22, data_nogoT_v23, 
                    data_nogoT_v24, data_nogoT_v25)
data_heurQ <- rbind(data_heurQ_v22, data_heurQ_v23, 
                    data_heurQ_v24, data_heurQ_v25)
data_inthumQ <- rbind(data_inthumQ_v22, data_inthumQ_v23, 
                      data_inthumQ_v24, data_inthumQ_v25)
data_matreasT <- rbind(data_matreasT_v22, data_matreasT_v23, 
                       data_matreasT_v24, data_matreasT_v25)
data_NavonT <- rbind(data_NavonT_v22, data_NavonT_v23, 
                     data_NavonT_v24, data_NavonT_v25)
data_NeckerT <- rbind(data_NeckerT_v22, data_NeckerT_v23, 
                      data_NeckerT_v24, data_NeckerT_v25)
data_needcloQ <- rbind(data_needcloQ_v22, data_needcloQ_v23, 
                       data_needcloQ_v24, data_needcloQ_v25)
data_needcogQ <- rbind(data_needcogQ_v22, data_needcogQ_v23, 
                       data_needcogQ_v24, data_needcogQ_v25)
data_openQ <- rbind(data_openQ_v22, data_openQ_v23, 
                    data_openQ_v24, data_openQ_v25)
data_rpQ <- rbind(data_rpQ_v22, data_rpQ_v23, 
                  data_rpQ_v24, data_rpQ_v25)
data_sciattQ <- rbind(data_sciattQ_v22, data_sciattQ_v23, 
                      data_sciattQ_v24, data_sciattQ_v25)
data_scicurQ <- rbind(data_scicurQ_v22, data_scicurQ_v23, 
                      data_scicurQ_v24, data_scicurQ_v25)
data_infoQ <- rbind(data_infoQ_v22, data_infoQ_v23, 
                    data_infoQ_v24, data_infoQ_v25)
data_uutT <- rbind(data_uutT_v22, data_uutT_v23, 
                   data_uutT_v24, data_uutT_v25)
data_validQ <- rbind(data_validQ_v22, data_validQ_v23, 
                     data_validQ_v24, data_validQ_v25)

# Remove the empty last line of each file
for (i in 1:length(names_all)) {
  assign(paste0("data_", names_all[i]),
         get(paste0("data_", names_all[i]))[which(!is.na(get(paste0("data_", names_all[i]))[, 1])), ])
}

# Remove version files to clean up the environment
for (j in 1:length(versions)) {
  for (i in 1:length(names_all)) {
    rm(list = paste0("data_", names_all[i], "_", versions[j]))
  }
}

``` 


## Merge Questionnaire Dataframes

This chunk merges all questionnaire dataframes, leaving us with one dataframe for all questionnaire data.

```{r Merge Questionnaire Dataframes}

# Rename two columns before combining all the questionnaires into one df (the column has the same name in each questionnaire file). 
    # END.QUESTIONNAIRE column gives you the response time for that questionnaire. 
    # Randomise.questionnaire.elements. column gives you a logical value indicating whether the questionnaire items were randomized for that questionnaire.

# END.QUESTIONNAIRE
names(data_demoQ)[names(data_demoQ) == "END.QUESTIONNAIRE"] <- "END.QUESTIONNAIRE.DEMOGRAPHICS"
names(data_big5Q)[names(data_big5Q) == "END.QUESTIONNAIRE"] <- "END.QUESTIONNAIRE.BIG5"
names(data_epiQ)[names(data_epiQ) == "END.QUESTIONNAIRE"] <- "END.QUESTIONNAIRE.EPISTEMIC"
names(data_heurQ)[names(data_heurQ) == "END.QUESTIONNAIRE"] <- "END.QUESTIONNAIRE.HEURISTIC"
names(data_inthumQ)[names(data_inthumQ) == "END.QUESTIONNAIRE"] <- "END.QUESTIONNAIRE.INT.HUM"
names(data_needcloQ)[names(data_needcloQ) == "END.QUESTIONNAIRE"] <- "END.QUESTIONNAIRE.NEED.CLO"
names(data_needcogQ)[names(data_needcogQ) == "END.QUESTIONNAIRE"] <- "END.QUESTIONNAIRE.NEED.COG"
names(data_openQ)[names(data_openQ) == "END.QUESTIONNAIRE"] <- "END.QUESTIONNAIRE.OPEN.THINK"
names(data_rpQ)[names(data_rpQ) == "END.QUESTIONNAIRE"] <- "END.QUESTIONNAIRE.RANDOM.PROB"
names(data_sciattQ)[names(data_sciattQ) == "END.QUESTIONNAIRE"] <- "END.QUESTIONNAIRE.SCIENCE.ATT"
names(data_scicurQ)[names(data_scicurQ) == "END.QUESTIONNAIRE"] <- "END.QUESTIONNAIRE.SCIENCE.CUR"
names(data_infoQ)[names(data_infoQ) == "END.QUESTIONNAIRE"] <- "END.QUESTIONNAIRE.INFO"
names(data_validQ)[names(data_validQ) == "END.QUESTIONNAIRE"] <- "END.QUESTIONNAIRE.VALIDITY"

# Randomise.questionnaire.elements.
names(data_demoQ)[names(data_demoQ) == "Randomise.questionnaire.elements."] <- "Randomise.questionnaire.elements..DEMOGRAPHICS"
names(data_big5Q)[names(data_big5Q) == "Randomise.questionnaire.elements."] <- "Randomise.questionnaire.elements..BIG5"
names(data_epiQ)[names(data_epiQ) == "Randomise.questionnaire.elements."] <- "Randomise.questionnaire.elements..EPISTEMIC"
names(data_heurQ)[names(data_heurQ) == "Randomise.questionnaire.elements."] <- "Randomise.questionnaire.elements..HEURISTIC"
names(data_inthumQ)[names(data_inthumQ) == "Randomise.questionnaire.elements."] <- "Randomise.questionnaire.elements..INT.HUM"
names(data_needcloQ)[names(data_needcloQ) == "Randomise.questionnaire.elements."] <- "Randomise.questionnaire.elements..NEED.CLO"
names(data_needcogQ)[names(data_needcogQ) == "Randomise.questionnaire.elements."] <- "Randomise.questionnaire.elements..NEED.COG"
names(data_openQ)[names(data_openQ) == "Randomise.questionnaire.elements."] <- "Randomise.questionnaire.elements..OPEN.THINK"
names(data_rpQ)[names(data_rpQ) == "Randomise.questionnaire.elements."] <- "Randomise.questionnaire.elements..RANDOM.PROB"
names(data_sciattQ)[names(data_sciattQ) == "Randomise.questionnaire.elements."] <- "Randomise.questionnaire.elements..SCIENCE.ATT"
names(data_scicurQ)[names(data_scicurQ) == "Randomise.questionnaire.elements."] <- "Randomise.questionnaire.elements..SCIENCE.CUR"
names(data_infoQ)[names(data_infoQ) == "Randomise.questionnaire.elements."] <- "Randomise.questionnaire.elements..INFO"
names(data_validQ)[names(data_validQ) == "Randomise.questionnaire.elements."] <- "Randomise.questionnaire.elements..VALIDITY"

# Merge by participant ID
# All rows for all dataframes are kept
data1 <- merge(data_demoQ, data_big5Q, 
               by = "Participant.Private.ID", 
               all = TRUE)
data2 <- merge(data_epiQ, data_heurQ, 
               by = "Participant.Private.ID", 
               all = TRUE)
data3 <- merge(data_inthumQ, data_needcloQ, 
               by = "Participant.Private.ID", 
               all = TRUE)
data4 <- merge(data_needcogQ, data_openQ, 
               by = "Participant.Private.ID", 
               all = TRUE)
data5 <- merge(data_rpQ, data_sciattQ, 
               by = "Participant.Private.ID", 
               all = TRUE)
data6 <- merge(data_scicurQ, data_infoQ, 
               by = "Participant.Private.ID", 
               all = TRUE)
data7 <- merge(data1, data2, 
               by = "Participant.Private.ID", 
               all = TRUE)
data8 <- merge(data3, data4, 
               by = "Participant.Private.ID", 
               all = TRUE)
data9 <- merge(data5, data6, 
               by = "Participant.Private.ID", 
               all = TRUE)
data10 <- merge(data7, data8, 
                by = "Participant.Private.ID", 
                all = TRUE)
data11 <- merge(data9, data_validQ, 
                by = "Participant.Private.ID", 
                all = TRUE)
data_Q_total <- merge(data10, data11, 
                      by = "Participant.Private.ID", 
                      all = TRUE)

# Clean the environment
rm(data1, data2, data3, data4, data5, data6, data7, data8, data9, data10, data11)
rm(data_big5Q, data_epiQ, data_heurQ, data_infoQ, data_inthumQ, data_needcloQ, data_needcogQ, data_openQ, data_rpQ, data_sciattQ, data_scicurQ)

```

# Questionnaire Data

## Exclusion Criteria

**ALL QUESTIONNAIRES:** This chunk goes through all questionnaire data, removing **all data for a participant** if they met one or more of the following exclusion criteria:
* stated their data are not valid
* stated their Finnish is not Äidinkieli, Keskusteleva, or Muu, Mikä.

## Questionnaire Data Processing

After excluding data in accordance with the exclusion criteria above, the following steps are taken for each questionnaire:
* reverse-code items (if necessary)
* visualize and describe each item
* check the structure of the questionnaire data with Cronbach's alpha (separately for each measured construct)
* calculate mean or sum scores according to the questionnaire instructions
* replace sum score 0s with NAs (no questionnaires included 0 as a lower bound in the response options, so sum/mean scores of 0 are not valid)
* visualize and describe mean or sum scores


### Validity and Language

```{r Validity and Language}
 
# Save data for nonvalid ppts ("älä huomioi aineistoani") in a separate file prior to removal
data_nonvalid <- data_Q_total %>%
  filter(Validity == "Ã„lÃ¤ huomioi aineistoani. Jokin muu syy esti minua osallistumasta kunnolla." | Validity == "Ã„lÃ¤ huomioi aineistoani. En suurimmaksi osaksi keskittynyt tai lukenut kysymyksiÃ¤ kunnolla.")

# Save data for participants with nonvalid language answers in a separate file. One participant who said "muu" but said they were fluent but not native level was included.
data_Q_lang_omit <- data_Q_total %>%
  filter(Language != "Sujuva / Ã¤idinkieli" & Language != "Keskitaso / keskusteleva" & Language != "Muu, mikÃ¤?")

# Remove nonvalid participants
data_Q_total <- data_Q_total %>%
  filter(Validity %in% c("Aineistoani voi kÃ¤yttÃ¤Ã¤.", "Muu, mikÃ¤? ", NA, "") & Language %in% c("Sujuva / Ã¤idinkieli", "Keskitaso / keskusteleva", "Muu, mikÃ¤?"))

```

### TIPI (Big Five)

Reliability are not calculated for TIPI, see Gosling's website for explanation. http://gosling.psy.utexas.edu/scales-weve-developed/ten-item-personality-measure-tipi/ 

```{r TIPI}

# Big Five
b5 <- c("agreeablenessNormal", "emotionalStabilityNormal", "extroversionNormal", "conscientiousnessNormal", "opennessNormal", "agreeablenessReverse", "emotionalStabilityReverse", "extroversionReverse", "conscientiousnessReverse", "opennessReverse")
b5_rev <- c("agreeablenessReverse", "emotionalStabilityReverse", "extroversionReverse", "conscientiousnessReverse", "opennessReverse")

# Reverse-code
for (i in 1:length(b5_rev)) {
  data_Q_total[, b5_rev[i]] <- reverse.code(keys = c(-1), 
                                            items = data_Q_total[, b5_rev[i]],
                                            mini = c(1), 
                                            maxi = c(7)) 
}

# Describe: item-level
par(mfrow = c(2, 5))
for (i in 1:length(b5)) {
  hist(as.numeric(data_Q_total[, b5[i]]),
       main = colnames(data_Q_total[b5[i]]),
       col = sample(colors(), 1),
       breaks = seq(0, 7, 1),
       xlab = ""
       )
}
describe(data_Q_total[, b5])

# Calculate means
data_Q_total$AMean <- rowMeans(cbind(data_Q_total$agreeablenessNormal, 
                                     data_Q_total$agreeablenessReverse), 
                               na.rm = TRUE)
data_Q_total$CMean <- rowMeans(cbind(data_Q_total$conscientiousnessNormal,                                     data_Q_total$conscientiousnessReverse),
                               na.rm = TRUE)
data_Q_total$EMean <- rowMeans(cbind(data_Q_total$extroversionNormal,
                                     data_Q_total$extroversionReverse), 
                               na.rm = TRUE)
data_Q_total$ESMean <- rowMeans(cbind(data_Q_total$emotionalStabilityNormal,
                                      data_Q_total$emostabilityReverse), 
                                na.rm = TRUE)
data_Q_total$OMean <- rowMeans(cbind(data_Q_total$opennessNormal,
                                     data_Q_total$opennessReverse), 
                               na.rm = TRUE)

# Replace mean score 0s with NAs (these participants stopped mid-questionnaire or before questionnaire and only have missing values)
data_Q_total$AMean[which(data_Q_total$AMean == 0)] <- NA
data_Q_total$OMean[which(data_Q_total$OMean == 0)] <- NA
data_Q_total$EMean[which(data_Q_total$EMean == 0)] <- NA
data_Q_total$CMean[which(data_Q_total$CMean == 0)] <- NA
data_Q_total$ESMean[which(data_Q_total$ESMean == 0)] <- NA

# Describe: mean score level
b5_mean <- c("AMean", "OMean", "EMean", "CMean", "ESMean")
par(mfrow = c(2, 3))
for (i in 1:length(b5_mean)) {
  hist(data_Q_total[, b5_mean[i]],
       main = colnames(data_Q_total[b5_mean[i]]),
       col = sample(colors(), 1),
       breaks = seq(0, 7, 1),
       xlab = ""
       )
}
describe(data_Q_total[, b5_mean])

```

### Epistemic Curiosity

```{r Epistemic Curiosity}

# Epistemic Curiosity
epicur <- c(paste0("Icuriosity", 1:5), paste0("Dcuriosity", 1:5))

# Describe: item-level
par(mfrow = c(2, 5))
for (i in 1:length(epicur)) {
  hist(data_Q_total[, epicur[i]],
       main = colnames(data_Q_total[epicur[i]]),
       col = sample(colors(), 1),
       breaks = seq(0, 5, 1),
       xlab = ""
       )
}
describe(data_Q_total[, epicur])

# Cronbach's alphas
psych::alpha(data_Q_total[, epicur[1:5]])
psych::alpha(data_Q_total[, epicur[6:10]])

# Calculate sum scores
data_Q_total$ICuriositySum <- rowSums(cbind(data_Q_total$Icuriosity1, data_Q_total$Icuriosity2, data_Q_total$Icuriosity3, data_Q_total$Icuriosity4, data_Q_total$Icuriosity5), na.rm = TRUE)
data_Q_total$DCuriositySum <- rowSums(cbind(data_Q_total$Dcuriosity1, data_Q_total$Dcuriosity2, data_Q_total$Dcuriosity3, data_Q_total$Dcuriosity4, data_Q_total$Dcuriosity5), na.rm = TRUE)

# Replace sum score 0s with NAs (these participants stopped mid-questionnaire or before questionnaire and only have missing values)
data_Q_total$ICuriositySum[which(data_Q_total$ICuriositySum == 0)] <- NA
data_Q_total$DCuriositySum[which(data_Q_total$DCuriositySum == 0)] <- NA

# Describe: sum score level
epicur_sum <- c("ICuriositySum", "DCuriositySum")
par(mfrow = c(1, 2))
for (i in 1:length(epicur_sum)) {
  hist(data_Q_total[, epicur_sum[i]],
       main = colnames(data_Q_total[epicur_sum[i]]),
       col = sample(colors(), 1),
       xlab = ""
       )
}
describe(data_Q_total[, c(epicur_sum)])

```

### Intellectual Humility

```{r Intellectual Humility}

# Intellectual Humility
IH <- c(paste0("IH", 1:5, "rev1"), paste0("IH", 6:10, "norm2"), 
        paste0("IH", 11:16, "norm3"), paste0("IH", 17:22, "rev4"))
IH_rev <- c(paste0("IH", 1:5, "rev1"), paste0("IH", 17:22, "rev4"))

# Reverse-code
for (i in 1:length(IH_rev)) {
  data_Q_total[, IH_rev[i]] <- reverse.code(keys = c(-1), 
                                            items = data_Q_total[, IH_rev[i]],
                                            mini = c(1), 
                                            maxi = c(5))
}

# Describe: item-level
par(mfrow = c(2, 3))
for (i in 1:length(IH)) {
  hist(data_Q_total[, IH[i]],
       main = colnames(data_Q_total[IH[i]]),
       col = sample(colors(), 1),
       breaks = seq(0, 5, 1),
       xlab = ""
       )
}
describe(data_Q_total[, IH])

# Cronbach's alphas
psych::alpha(data_Q_total[, IH[1:5]])
psych::alpha(data_Q_total[, IH[6:10]])
psych::alpha(data_Q_total[, IH[11:16]])
psych::alpha(data_Q_total[, IH[17:22]])

# Intellectual Humility Scoring
# https://seaver.pepperdine.edu/social-science/content/comprehensive-intellectual-humility.pdf -> sum scores

# Calculate sum scores
data_Q_total$IH1Sum <- rowSums(cbind(data_Q_total$IH1rev1, data_Q_total$IH2rev1, data_Q_total$IH3rev1, data_Q_total$IH4rev1, data_Q_total$IH5rev1), na.rm = TRUE)
data_Q_total$IH2Sum <- rowSums(cbind(data_Q_total$IH6norm2, data_Q_total$IH7norm2, data_Q_total$IH8norm2, data_Q_total$IH9norm2, data_Q_total$IH10norm2), na.rm = TRUE)
data_Q_total$IH3Sum <- rowSums(cbind(data_Q_total$IH11norm3, data_Q_total$IH12norm3, data_Q_total$IH13norm3, data_Q_total$IH14norm3, data_Q_total$IH15norm3), na.rm = TRUE)
data_Q_total$IH4Sum <- rowSums(cbind(data_Q_total$IH17rev4, data_Q_total$IH18rev4, data_Q_total$IH19rev4, data_Q_total$IH20rev4, data_Q_total$IH21rev4), na.rm = TRUE)

# Replace sum score 0s with NAs
data_Q_total$IH1Sum[which(data_Q_total$IH1Sum == 0)] <- NA
data_Q_total$IH2Sum[which(data_Q_total$IH2Sum == 0)] <- NA
data_Q_total$IH3Sum[which(data_Q_total$IH3Sum == 0)] <- NA
data_Q_total$IH4Sum[which(data_Q_total$IH4Sum == 0)] <- NA

# Describe: sum score level
IH_sum <- c("IH1Sum", "IH2Sum", "IH3Sum", "IH4Sum")
par(mfrow = c(2, 2))
for (i in 1:length(IH_sum)) {
  hist(data_Q_total[, IH_sum[i]],
       main = colnames(data_Q_total[IH_sum[i]]),
       col = sample(colors(), 1),
       xlab = ""
       )
}
describe(data_Q_total[, IH_sum])

```

### Need for closure

```{r Need for Closure}

# Need for Closure
need_clo <- paste0("closure", 1:15)

# Describe: item-level
par(mfrow = c(3, 5))
for (i in 1:length(need_clo)) {
  hist(data_Q_total[, need_clo[i]],
       main = colnames(data_Q_total[need_clo[i]]),
       breaks = seq(0, 6, 1),
       col = sample(colors(), 1),
       xlab = ""
       )
}
describe(data_Q_total[, need_clo])

# Cronbach's alphas
psych::alpha(data_Q_total[, need_clo])

# Need for Closure Scoring
# https://www.midss.org/sites/default/files/need_for_closure_scale.pdf
# Not entirely sure if this is the correct file but it says to sum so I'll do that here

# Calculate sum score
data_Q_total$CloSum <- rowSums(cbind(data_Q_total$closure1, data_Q_total$closure2, data_Q_total$closure3, data_Q_total$closure4, data_Q_total$closure5, data_Q_total$closure6, data_Q_total$closure7, data_Q_total$closure8, data_Q_total$closure9, data_Q_total$closure10, data_Q_total$closure11, data_Q_total$closure12, data_Q_total$closure13, data_Q_total$closure14, data_Q_total$closure15), na.rm = TRUE)

# Replace sum score 0s with NAs
data_Q_total$CloSum[which(data_Q_total$CloSum == 0)] <- NA

# Describe: sum score level
describe(data_Q_total$CloSum)
hist(data_Q_total$CloSum, 
     main = "Need for Closure", 
     col = rainbow(14), 
     ylim = c(0, 12), 
     breaks = seq(0, 80, 1), 
     xlab = "")

```

### Need for cognition

```{r Need for Cognition}

# Need for Cognition
need_cog <- c(paste0("cognition", c(1:2, 6, 10:11, 13:15, 18)), 
              paste0("cognition", c(3:5, 7:9, 12, 16:17), ".rev"))
need_cog_rev <- paste0("cognition", c(3:5, 7:9, 12, 16:17), ".rev")

# Reverse-code
for (i in 1:length(need_cog_rev)) {
  data_Q_total[, need_cog_rev[i]] <- reverse.code(keys = c(-1),
                                                  items = data_Q_total[, need_cog_rev[i]],
                                                  mini = c(1),
                                                  maxi = c(6))
}

# Describe: item-level
par(mfrow = c(3, 6))
for (i in 1:length(need_cog)) {
  hist(data_Q_total[, need_cog[i]],
       main = colnames(data_Q_total[need_cog[i]]),
       breaks = seq(0, 6, 1),
       col = sample(colors(), 1),
       xlab = ""
       )
}
describe(data_Q_total[, need_cog])

# Cronbach's alphas
psych::alpha(data_Q_total[, need_cog])

# Need for Cognition Scoring
# https://centerofinquiry.org/uncategorized/need-for-cognition-scale-wabash-national-study/
# According to this source you should calculate the sum score for this scale

# Calculate sum score
data_Q_total$CogSum <- rowSums(cbind(data_Q_total$cognition1, data_Q_total$cognition2, data_Q_total$cognition3.rev, data_Q_total$cognition4.rev, data_Q_total$cognition5.rev, data_Q_total$cognition6, data_Q_total$cognition7.rev, data_Q_total$cognition8.rev, data_Q_total$cognition9.rev, data_Q_total$cognition10, data_Q_total$cognition11, data_Q_total$cognition12.rev, data_Q_total$cognition13, data_Q_total$cognition14, data_Q_total$cognition15, data_Q_total$cognition16.rev, data_Q_total$cognition17.rev, data_Q_total$cognition18), na.rm = TRUE)

# Replace sum score 0s with NAs
data_Q_total$CogSum[which(data_Q_total$CogSum == 0)] <- NA

# Describe: sum-score level
describe(data_Q_total$CogSum)
hist(data_Q_total$CogSum,
     main = "Need for Cognition", 
     col = rainbow(14), 
     xlab = "")

```

### Actively openminded thinking

```{r Actively Openminded Thinking}

# Actively Openminded Thinking
aot <- c(paste0("openReverse", 1:4), paste0("openNormal", 1:3))
aot_rev <- paste0("openReverse", 1:4)

# Reverse-code
for (i in 1:length(aot_rev)) {
  data_Q_total[, aot_rev[i]] <- reverse.code(keys = c(-1), 
                                             items = data_Q_total[, aot_rev[i]], 
                                             mini = c(1), 
                                             maxi = c(7))
}

# Describe: item-level
par(mfrow = c(2, 4))
for (i in 1:length(aot)) {
  hist(data_Q_total[, aot[i]], 
       main = colnames(data_Q_total[aot[i]]), 
       col = sample(colors(), 1), 
       breaks = seq(0, 7, 1), 
       xlab = "")
}
describe(data_Q_total[, aot])

# Cronbach's alpha
psych::alpha(data_Q_total[, aot])

# Actively Openminded Thinking Scoring
# https://www.sciencedirect.com/science/article/pii/S1871187119303700
# Again, this article says to do a sum score

# Calculate sum score
data_Q_total$AOTSum <- rowSums(cbind(data_Q_total$openNormal1, data_Q_total$openNormal2, data_Q_total$openNormal3, data_Q_total$openReverse1, data_Q_total$openReverse2, data_Q_total$openReverse3, data_Q_total$openReverse4), na.rm = TRUE)

# Replace sum score 0s with NAs
data_Q_total$AOTSum[which(data_Q_total$AOTSum == 0)] <- NA

# Describe: sum-score level
describe(data_Q_total$AOTSum)
hist(data_Q_total$AOTSum,
     main = "Actively Openminded Thinking", 
     col = rainbow(14), 
     xlab = "")

```

### Science Attitudes Questionnaire

This section has 12 questions with Likert responses that represent a (possible) 3 factor model based on the sources of the questions, from an early version of the Science Capital Scale from the FINSCI population survey study. The first 8 questions originally come from Archer, 2015, and the last 4 are modified from the Trust in Science and Scientists Scale (Nadelson et al., 2014). 

```{r Science Attitudes}

# Science Attitudes
sci_att <- c(paste0("sa.1", letters[1:8]), paste0("sa.2", letters[1:4]))
sci_att_rev <- c("sa.1b", "sa.1d", "sa.2a", "sa.2b", "sa.2d")

# Reverse-code
for (i in 1:length(sci_att_rev)) {
  data_Q_total[, sci_att_rev[i]] <- reverse.code(keys = c(-1), 
                                             items = data_Q_total[, sci_att_rev[i]], 
                                             mini = c(1), 
                                             maxi = c(5))
}

# Describe: item-level
par(mfrow = c(2, 2))
for (i in 1:length(sci_att)) {
  hist(data_Q_total[, sci_att[i]], 
       main = colnames(data_Q_total[sci_att[i]]), 
       col = sample(colors(), 1), 
       breaks = seq(0, 7, 1), 
       xlab = "")
}
describe(data_Q_total[, sci_att])

# Cronbach's alphas
psych::alpha(data_Q_total[, sci_att])

# Calculate sum score
data_Q_total$sci_id <- rowSums(cbind(data_Q_total$sa.1a, data_Q_total$sa.1b, data_Q_total$sa.1d, data_Q_total$sa.1g), na.rm = TRUE)
data_Q_total$sci_impo <- rowSums(cbind(data_Q_total$sa.1c, data_Q_total$sa.1e, data_Q_total$sa.1h, data_Q_total$sa.1f), na.rm = TRUE)
data_Q_total$sci_tru <- rowSums(cbind(data_Q_total$sa.2a, data_Q_total$sa.2b, data_Q_total$sa.2c, data_Q_total$sa.2d), na.rm = TRUE)

# Replace total score zeroes with NA
data_Q_total$sci_id[which(data_Q_total$sci_id == 0)] <- NA
data_Q_total$sci_impo[which(data_Q_total$sci_impo == 0)] <- NA
data_Q_total$sci_tru[which(data_Q_total$sci_tru == 0)] <- NA

# Describe: sum-score level
sci_att_sum <- c("sci_id", "sci_impo", "sci_tru")
describe(data_Q_total[, sci_att_sum])
par(mfrow = c(3, 1))
for (i in 1:length(sci_att_sum)) {
  hist(data_Q_total[, sci_att_sum[i]], 
       main = colnames(data_Q_total[sci_att_sum[i]]), 
       col = sample(colors(), 1), 
       xlab = "")
}

```

### Science Curiosity

This is a 4-item scale, scored as a single sum score from the quantised responses. No reverse scoring. Modified from Landrum et al., 2016 and Motta et al., 2019

```{r Science Curiosity}

# Science Curiosity 
sci_cur <- paste0("sc.", 1:4, ".quantised")

# Describe: item-level
par(mfrow = c(2, 2))
for (i in 1:length(sci_cur)) {
  hist(data_Q_total[, sci_cur[i]], 
       main = colnames(data_Q_total[sci_cur[i]]), 
       col = sample(colors(), 1), 
       breaks = seq(0, 7, 1), 
       xlab = "")
}
describe(data_Q_total[, sci_cur])

# Cronbach's alpha
psych::alpha(data_Q_total[, sci_cur])

# Calculate sum score
data_Q_total$sci_cur <- rowSums(cbind(data_Q_total$sc.1.quantised, data_Q_total$sc.2.quantised, data_Q_total$sc.3.quantised, data_Q_total$sc.4.quantised), na.rm = TRUE)

# Replace sum score 0s with NAs
data_Q_total$sci_cur[which(data_Q_total$sci_cur == 0)] <- NA

# Describe: sum-score level
describe(data_Q_total$sci_cur)
hist(data_Q_total$sci_cur, 
     col = sample(colors(), 1), 
     xlab = "")

```

### Heuristic Reasoning

```{r Heuristic Reasoning}

# Q1
data_Q_total$hr.1 <- as.factor(data_Q_total$hr.1)
levels(data_Q_total$hr.1) <- list("N" = "Kumpikin on yhtÃ¤ todennÃ¤kÃ¶istÃ¤", "H-R" = "Kuudes lapsi on tyttÃ¶", "Neither" = "Kuudes lapsi on poika")

# Q2
data_Q_total$hr.2 <- as.factor(data_Q_total$hr.2)
levels(data_Q_total$hr.2) <- list("N" = "Kummatkin sarjat ovat yhtÃ¤ todennÃ¤kÃ¶isiÃ¤", "H-R" = "THHTHT", "Neither" = "HTHTHT")

# Q3
# 3.1
data_Q_total$hr.3.1 <- as.factor(data_Q_total$hr.3.1)
levels(data_Q_total$hr.3.1) <- list("b" = "Herra F. sai sydÃ¤nkohtauksen", "a" = "Herra F. on yli 55-vuotias ja on saanut sydÃ¤nkohtauksen", "c" = "Herra F.:llÃ¤ on suuri perhe")
# 3.2
data_Q_total$hr.3.2 <- as.factor(data_Q_total$hr.3.2)
levels(data_Q_total$hr.3.2) <- list("b" = "Herra F. sai sydÃ¤nkohtauksen", "a" = "Herra F. on yli 55-vuotias ja on saanut sydÃ¤nkohtauksen", "c" = "Herra F.:llÃ¤ on suuri perhe")
# 3.3
data_Q_total$hr.3.3 <- as.factor(data_Q_total$hr.3.3)
levels(data_Q_total$hr.3.3) <- list("b" = "Herra F. sai sydÃ¤nkohtauksen", "a" = "Herra F. on yli 55-vuotias ja on saanut sydÃ¤nkohtauksen", "c" = "Herra F.:llÃ¤ on suuri perhe")
# Create the variables for the combinations of answers
# abc / acb / cab = N; bac / bca /cba = H-R
data_Q_total <- data_Q_total %>%
  mutate(hr.3.updated = case_when(
  ((hr.3.1 == "b" & (hr.3.2 == "a" | hr.3.3 == "a")) | hr.3.2 == "b" & hr.3.3 == "a") ~ "H-R", 
  ((hr.3.1 == "a" & (hr.3.2 == "b" | hr.3.3 == "b")) | (hr.3.2 == "a" & hr.3.3 == "b")) ~ "N"
 ))
data_Q_total$hr.3.updated <- factor(data_Q_total$hr.3.updated)

# Q4
data_Q_total$hr.4 <- as.factor(data_Q_total$hr.4)
levels(data_Q_total$hr.4) <- list("H-E" = "Kumpikin on yhtÃ¤ tehokas", "N" = "Kognitiivis-behavioraalista terapiaa", "Neither" = "LÃ¤Ã¤kehoitoa")
# Kognitiivis-behavioraalista terapiaa = N; Yhta tehokas = H-E

# Q5
data_Q_total$hr.5 <- as.factor(data_Q_total$hr.5)
levels(data_Q_total$hr.5) <- list("N" = "Huomenna luultavasti sataa", "H-E" = "On mahdotonta sanoa sataako huomenna vai ei", "Neither" = "Huomenna sataa")
# Huomenna luultavasti sataa = N; On mahdotonta sanoa sataako huomenna vai ei = H-E

# Q6
data_Q_total$hr.6 <- as.factor(data_Q_total$hr.6)
levels(data_Q_total$hr.6) <- list("N" = "TyttÃ¶", "H-E" = "Kumpikin on yhtÃ¤ todennÃ¤kÃ¶istÃ¤", "Neither" = "Poika")
# Tytto = N; Kumpikin on yht? todenn?k?ist? = H-E

# Calculate sums
data_Q_total <- data_Q_total %>%
  mutate(
    total_N = apply(., 1, function(x) length(which(x == "N"))), 
    total_HR = apply(., 1, function(x) length(which(x == "H-R"))), 
    total_HE = apply(., 1, function(x) length(which(x == "H-E"))), 
    total_Neither = apply(., 1, function(x) length(which(x == "Neither")))
 )

# Omit participants with 0s in all the sum columns
heur <- c("total_N", "total_HR", "total_HE", "total_Neither")
data_Q_total[which(data_Q_total$total_N == 0 & data_Q_total$total_HR == 0 & data_Q_total$total_HE == 0 & data_Q_total$total_Neither == 0), heur] <- NA

# Check that sum scores add up to 6
sum_heur <- data_Q_total %>% 
  select(total_N, total_HR, total_HE, total_Neither) %>% 
  rowwise() %>% 
  mutate(sum_heur = total_N + total_HR + total_HE + total_Neither) %>% 
  select(sum_heur)
table(sum_heur$sum_heur)
  
# Describe: sum score level
describe(data_Q_total[, heur])
par(mfrow = c(2, 2))
for (i in 1:length(heur)) {
  hist(data_Q_total[, heur[i]], 
       main = colnames(data_Q_total[heur[i]]), 
       col = sample(colors(), 1), 
       xlab = "")
}

# Create binary heuristic scores (0=0, 1=everything else)
data_Q_total <- data_Q_total %>% 
  mutate(HEscore = case_when(total_HE == 0 ~ 0,
                             total_HE == 1 ~ 1,
                             total_HE == 2 ~ 1),
         HRscore = case_when(total_HR == 0 ~ 0,
                             total_HR == 1 ~ 1,
                             total_HR == 2 ~ 1))

# Add a measure of total heuristic response, where 1 = at least one mistake in one heuristic category, 2 = at least 1 heuristic mistake in both heuristic categories, 0 = all normative responses
data_Q_total <- data_Q_total %>%
  group_by(Participant.Private.ID) %>% 
  mutate(HEHRscore = HEscore + HRscore)

```

### Randomness and Probability

New columns are created for each item, specifying whether the participant got the question correct or not (0=incorrect, 1=correct). A sum score is calculated for all the items.

```{r Randomness and Probability}

# Q1: Sairaala B is the correct answer
# Create a new column
data_Q_total$rp.1 <- as.factor(data_Q_total$rp.1)
data_Q_total <- data_Q_total %>%
  mutate(rp.1.int = case_when(rp.1 == "Sairaalassa B (jossa syntyy 10 lasta pÃ¤ivÃ¤ssÃ¤)." ~ 1,
                              rp.1 %in% c("Sairaalassa A (jossa syntyy 50 lasta pÃ¤ivÃ¤ssÃ¤).", "TÃ¤mÃ¤ on yhtÃ¤ todennÃ¤kÃ¶istÃ¤ kummassakin sairaalassa.") ~ 0)) 

# Q2: Pyydys1 AND Pyydys 2 is the correct combination
data_Q_total$rp.2.1 <- as.factor(data_Q_total$rp.2.1)
data_Q_total$rp.2.2 <- as.factor(data_Q_total$rp.2.2)
data_Q_total$rp.2.3 <- as.factor(data_Q_total$rp.2.3)
data_Q_total$rp.2.4 <- as.factor(data_Q_total$rp.2.4)
data_Q_total$rp.2.5 <- as.factor(data_Q_total$rp.2.5)
data_Q_total$rp.2.6 <- as.factor(data_Q_total$rp.2.6)
data_Q_total$rp.2.7 <- as.factor(data_Q_total$rp.2.7)
data_Q_total$rp.2.8 <- as.factor(data_Q_total$rp.2.8)
# Create a new column
# NAs are initially coded as 2 and wrong answers (0s) as NAs; then these are recoded
data_Q_total <- data_Q_total %>% 
  mutate(rp.2.int = case_when((rp.2.1 == "Pyydys 1" & rp.2.2 == "Pyydys 2" & rp.2.3 == "" & rp.2.4 == "" & rp.2.5 == "" & rp.2.6 == "" & rp.2.7 == "" & rp.2.8 == "") ~ 1,
                              is.na(rp.2.8) ~ 2))
data_Q_total[which(is.na(data_Q_total$rp.2.int)), "rp.2.int"] <- 0
data_Q_total[which(data_Q_total$rp.2.int == 2), ] <- NA

# Q3: Kanava 1; 2 tai 3 is the correct answer
# Create a new column
data_Q_total <- data_Q_total %>%
  mutate(rp.3.int = case_when(rp.3.quantised == "4" ~ 1, 
                              rp.3.quantised != "4" ~ 0))

# Q4: Ruudukot A; B ja C is the correct answer
# Create a new column
data_Q_total <- data_Q_total %>%
  mutate(rp.4.int = case_when(rp.4.quantised == "5" ~ 1, 
                              rp.4.quantised != "5" ~ 0))

# Calculate the sum score for the randomness/probability questions
data_Q_total <- data_Q_total %>%
  mutate(RPSum = rp.1.int + rp.2.int + rp.3.int + rp.4.int)

# Describe
describe(data_Q_total$RPSum)
hist(data_Q_total$RPSum,
     main = "RPSum",
     col = sample(colors(), 1),
     xlab = "")

```

### Matrix Reasoning

For the matrix reasoning task (treated as a questionnaire by Gorilla so included in this section), a participant's data for that task is replaced with missing values (NAs) if they met any of the following exclusion criteria:
* medium reaction time equal to or smaller than 500ms
* stated they had technical problems or did not understand the task

```{r Matrix Reasoning}

# Omit columns that are not relevant (experiment-general columns) + rows that are not relevant (e.g., practice trials)
data_matreasT <- data_matreasT %>%
  select(Participant.Private.ID, Spreadsheet:ANSWER) %>% 
  filter(display %in% c("TehtÃ¤vÃ¤_6", "TehtÃ¤vÃ¤_8"))

# Omit participants who commented that they had technical issues with this task or did not understand the task
data_matreasT <- data_matreasT %>%
  filter(Participant.Private.ID != "5555882" & Participant.Private.ID != "5608075" & Participant.Private.ID != "4887607")

# Extract the relevant information
data_matreasT_final <- data_matreasT %>%
  group_by(Participant.Private.ID) %>%
  summarise(MatrixCorrectCount = sum(Correct, na.rm = TRUE)) %>%
  select(Participant.Private.ID, MatrixCorrectCount)

# Exclude participants with invalid responses
data_ValidityFlag <- data_validQ %>%
  select(Participant.Private.ID, Validity.quantised)
data_Language <- data_demoQ %>%
  select(Participant.Private.ID, Language)
data_matrixValidity <- merge(data_Language, data_ValidityFlag, 
                             by = "Participant.Private.ID", 
                             all = TRUE)
data_matreasT_final <- merge(data_matreasT_final, data_matrixValidity, 
                             by = "Participant.Private.ID", 
                             all = TRUE)

# Create separate df for nonvalid ppts
data_nonvalid_matrix <- data_matreasT_final %>%
  filter(Validity.quantised %in% c(2, 3, 4))
data_Q_lang_omit_matrix <- data_matreasT_final %>%
  filter(Language != "Sujuva / Ã¤idinkieli" & Language != "Keskitaso / keskusteleva" & Language != "Muu, mikÃ¤?")
data_matreasT_final <- data_matreasT_final %>%
  filter(Validity.quantised %in% c("1", NA, "") & Language %in% c("Sujuva / Ã¤idinkieli", "Keskitaso / keskusteleva", "Muu, mikÃ¤?"))

# Delete the irrelevant columns
data_matreasT_final <- data_matreasT_final %>%
  select(Participant.Private.ID, MatrixCorrectCount)

# Describe
describe(data_matreasT_final$MatrixCorrectCount)
hist(data_matreasT_final$MatrixCorrectCount,
     main = "Matrix Score", 
     col = rainbow(14), 
     xlab = "")

```

## Merge Questionnaire Variables

```{r Merge questionnaire variables}

# Merge the important columns
data_Q_sub <- data_Q_total %>%
  select(Participant.Private.ID, gender, Age, Country, Language, Education, AMean, CMean, EMean, ESMean, OMean, ICuriositySum, DCuriositySum, IH1Sum, IH2Sum, IH3Sum, IH4Sum, CloSum, CogSum, AOTSum, HEscore, HRscore, RPSum, sci_cur, sci_tru, sci_impo, sci_id)

# Add matrix reasoning data to the dataframe
data_Q_sub <- merge(data_Q_sub, data_matreasT_final, 
                    by = "Participant.Private.ID", 
                    all = TRUE)

# Get rid of NaNs or string NAs introduced by calculations
is.nan.data.frame <- function(x)
  do.call(cbind, lapply(x, is.nan))
data_Q_sub[is.nan(data_Q_sub)] <- NA

# Numeric variables into numeric
data_Q_sub$MatrixCorrectCount <- as.numeric(data_Q_sub$MatrixCorrectCount)
data_Q_sub$Age <- as.numeric(data_Q_sub$Age)

```

## Analyze Missing Data in Questionnaires

Participants with large amounts of missing data or suspicious patterns of missing data are removed here. It should be noted that missing data refers to data that was missing from the start *and* data that was replaced with missing values if the participant met one of the exclusion criteria for (a) questionnaire(s).

```{r Analyze Missing Data in Questionnaires}

# Questionnaire variables
q_var <- c("AMean", "CMean", "EMean", "ESMean", "OMean", "ICuriositySum", "DCuriositySum", "IH1Sum", "IH2Sum", "IH3Sum", "IH4Sum", "CloSum", "CogSum", "AOTSum", "HEscore", "HRscore", "RPSum", "sci_cur", "sci_tru", "sci_impo", "sci_id", "MatrixCorrectCount")

# Add a column for count of missing data for questionnaire variables per participant
data_Q_sub$q_missingdata <- rowSums(is.na(data_Q_sub[, q_var]))

# Plot number of missing values
ggplot(data_Q_sub, aes(x = as.factor(Participant.Private.ID), y = q_missingdata)) + 
   geom_boxplot(fill = "slateblue", alpha = 0.2) + 
   xlab("Missing variables count")

# Plot to look for an elbow value
plot(sort(data_Q_sub$q_missingdata))

# Correlation between age and amount of missing values
gg_miss_fct(x = data_Q_sub, fct = Age)
cor.test(data_Q_sub$Age, data_Q_sub$q_missingdata, 
         method = "spearman", exact = FALSE)

# Visualize missing data by demographics
gg_miss_fct(x = data_Q_sub, fct = Language)
gg_miss_fct(x = data_Q_sub, fct = Country)
gg_miss_fct(x = data_Q_sub, fct = gender)
gg_miss_fct(x = data_Q_sub, fct = Education)

# Remove ppts with 8 or more variables missing out of 22 (~32%)
data_Q_sub <- data_Q_sub %>%
  filter(q_missingdata < 8) %>% 
  select(-q_missingdata)

```

## Check Questionnaire Distributions and Correlations

This chunk checks distributions of questionnaire variables and correlations between them:
* If distributions are highly skewed, transformations are applied to decrease skewness (skew > 1)
* If two variables are highly correlated, one of them is dropped (Eisenberg et al.) (r > .85)

```{r Normality and Correlations of Questionnaires}

# Questionnaire variables without binary ones
q_var_con <- c("AMean", "CMean", "EMean", "ESMean", "OMean", "ICuriositySum", "DCuriositySum", "IH1Sum", "IH2Sum", "IH3Sum", "IH4Sum", "CloSum", "CogSum", "AOTSum", "RPSum", "sci_cur", "sci_tru", "sci_impo", "sci_id", "MatrixCorrectCount")

# Normality of variables: visualize with histograms
par(mfrow = c(2, 3))
for (i in 1:length(q_var_con)) {
  hist(data_Q_sub[, q_var_con[i]],
       main = colnames(data_Q_sub[q_var_con[i]]),
       col = sample(colors(), 1),
       xlab = "")
}

# Describe: mean, skew, kurtosis
describe(data_Q_sub[, q_var_con])[c("mean", "skew", "kurtosis")]

# Correlation matrix
round(cor(data_Q_sub[, q_var_con], method = "pearson", 
          use = "pairwise.complete.obs"), 2)
abs(round(cor(data_Q_sub[, q_var_con], method = "pearson", 
              use = "pairwise.complete.obs"), 2)) > .85
corrplot(cor(data_Q_sub[, q_var_con], method = "pearson", 
             use = "pairwise.complete.obs"))
corrplot(cor(data_Q_sub[, q_var], method = "spearman", 
             use = "pairwise.complete.obs"))

# Subset data: exclude demographic variables
data_Q_sub <- data_Q_sub[, c("Participant.Private.ID", q_var)]

``` 

# Task Data

## Exclusion Criteria

**ALL TASKS:** This chunk goes through all task data, removing **all data for a participant** if they met one or more of the following exclusion criteria:
* stated their data are not valid
* stated their Finnish is not Äidinkieli or Keskusteleva

**TASKS**: A participant's data for a questionnaire was replaced with missing values (NAs) if they met the following exclusion criterion:
* stopped during that task

**For further exclusion criteria for the tasks, please see each chunk.**

## Task Data Processing

See each chunk.

### Go No Go

Trial-level exclusion criteria:
* The reaction time for each row is replaced with missing values (NAs) if it is equal to or higher than 1025ms (screen time 1000ms + 16.67ms refresh rate + ~8ms latency between computer and mouse -> reaction times higher than this are likely not valid) *or* if it is equal to or lower than 150ms (Jaana's paper).

Participant-level exclusion criteria:
* A participant's data for the Go No-Go task is replaced with missing values (NAs) if their accuracy (% of correct trials out of all trials) was equal to or lower than 60%.

Data processing: for each participant, we calculate the sum of hits/misses/FAs/CRs as well as dprime, beta, c, aprime, and bppd (see the dprime() function help file). These are described and visualized.

```{r Go No-Go}

# Omit experiment-general columns & practice (etc.) rows
data_nogoT <- data_nogoT %>%
  select(Participant.Private.ID, Spreadsheet:Answer) %>%
  filter(display == "Trials")

# Exclusion Criteria: Trial-Level
  # RT <= 150ms or >= 1025ms
data_nogoT$Reaction.Time[which(data_nogoT$Reaction.Time >= 1025 | data_nogoT$Reaction.Time <= 150)] <- NA

# Exclusion Criteria: Participant-Level
  # <=60% accuracy
data_nogoT <- data_nogoT %>%
  group_by(Participant.Private.ID) %>% 
  mutate(Accuracy = sum(Correct, na.rm = TRUE) / (sum(Correct, na.rm = TRUE) + sum(Incorrect, na.rm = TRUE))) %>% 
  mutate(go.nogo_omit = case_when(Accuracy > .60 ~ 0, 
                                  Accuracy <= .60 ~ 1)) 
 
# Omit rows based on exclusion criteria
data_nogoT$Response[which(data_nogoT$go.nogo_omit == 1)] <- NA

# Create Signal Detection Theory categories for each row
data_nogoT <- data_nogoT %>%
  filter(display == "Trials") %>%
  mutate(SDT = case_when((Array %in% c("H.png",  "T.png") & Response == "Go") ~ "Hit", 
                         (Array %in% c("H.png", "T.png") & Response == "No Go") ~ "Miss", 
                         (Array == "N.png" & Response == "Go") ~ "False Alarm", 
                         (Array == "N.png" & Response == "No Go") ~ "Correct Rejection")) %>% 
    mutate(Hit = case_when(SDT == "Hit" ~ 1, 
                           SDT != "Hit" ~ 0), 
           Miss = case_when(SDT == "Miss" ~ 1, 
                            SDT != "Miss" ~ 0), 
           FA = case_when(SDT == "False Alarm" ~ 1, 
                          SDT != "False Alarm" ~ 0), 
           CR = case_when(SDT == "Correct Rejection" ~ 1, 
                          SDT != "Correct Rejection" ~ 0))

# Calculate the mean and SD of hits and FA (trials with a go response)
data_go_RT <- data_nogoT %>% 
  filter(SDT %in% c("Hit", "FA")) %>% 
  group_by(Participant.Private.ID) %>% 
  summarise(MeanGoRT = mean(Reaction.Time, na.rm = TRUE),
            SDGoRT = sd(Reaction.Time, na.rm = TRUE))

# Compute D-Prime and Bias for each participant
# https://www.rdocumentation.org/packages/psycho/versions/0.6.1/topics/dprime
# http://wise.cgu.edu/wise-tutorials/tutorial-signal-detection-theory/signal-detection-d-defined-2/
data_dprime <- data_nogoT %>%
  group_by(Participant.Private.ID) %>%
  summarise(dp = dprime(n_hit = sum(Hit, na.rm = TRUE), 
                        n_fa = sum(FA, na.rm = TRUE), 
                        n_miss = sum(Miss, na.rm = TRUE), 
                        n_cr = sum(CR, na.rm = TRUE),
                        n_targets = 300, 
                        n_distractors = 100))

# Add a row to specify what the five different values given mean
data_dprime$value <- rep_len(c("GNGdprime", "GNGbeta", "GNGaprime", "GNGbppd", "GNGc"),
                             length.out = nrow(data_dprime))

# Into wide format
data_dprime <- data_dprime %>%
  pivot_wider(names_from = value, values_from = dp)
data_GNGdprime <- as.data.frame(data_dprime)

# Create a sum score of each response category for each participant
data_nogoT_final <- data_nogoT %>%
  select(Participant.Private.ID, Hit, Miss, FA, CR) %>%
  group_by(Participant.Private.ID) %>%
  summarise(GNGHitSum = sum(Hit, na.rm = TRUE), 
            GNGMissSum = sum(Miss, na.rm = TRUE), 
            GNGFASum = sum(FA, na.rm = TRUE), 
            GNGCRSum = sum(CR, na.rm = TRUE))

# Combine the dataframes (keep all rows)
data_nogoT_final <- merge(data_nogoT_final, data_GNGdprime, 
                          by = "Participant.Private.ID", all = TRUE)
data_nogoT_final <- merge(data_nogoT_final, data_go_RT, 
                          by = "Participant.Private.ID", all = TRUE)

# Exclude participants with invalid responses
data_ConsentValidity <- data_Q_total %>%
  select(Participant.Private.ID, Language, Validity.quantised)
 
# Final dataset
data_nogoT_final <- merge(data_nogoT_final, data_ConsentValidity,
                          by = "Participant.Private.ID")

# Exclude participants according to validity + language criteria; exclude the participants who stopped mid-task; exclude participants whose sum score for all hit/miss/cr/fa are 0s
data_nogoT_final <- data_nogoT_final %>% 
  filter(Validity.quantised %in% c(1, NA, "") & Language %in% c("Sujuva / Ã¤idinkieli", "Keskitaso / keskusteleva", "Muu, mikÃ¤?")) %>% 
  filter(!Participant.Private.ID %in% c("5201242", "4886650", "5117494", "5282634", "5500754", "5552405", "5635997")) %>% 
  filter(!(GNGHitSum == 0 & GNGMissSum == 0 & GNGFASum == 0 & GNGCRSum == 0))

# Final set of columns
data_nogoT_final <- data_nogoT_final %>%
  select(Participant.Private.ID, GNGHitSum, GNGMissSum, GNGFASum, GNGCRSum, MeanGoRT, SDGoRT, GNGdprime, GNGbeta, GNGaprime, GNGbppd, GNGc) 

# Change columns into numeric
nogo <- c("GNGHitSum", "GNGMissSum", "GNGFASum", "GNGCRSum", "MeanGoRT", "SDGoRT", "GNGdprime", "GNGbeta", "GNGaprime", "GNGbppd", "GNGc")
for (i in 1:length(nogo)) {
  data_nogoT_final[, nogo[i]] <- as.numeric(data_nogoT_final[, nogo[i]])
}

# Describe
describe(data_nogoT_final[, nogo])
par(mfrow = c(2, 3))
for (i in 1:length(nogo)) {
  hist(data_nogoT_final[, nogo[i]], 
       main = colnames(data_nogoT_final[nogo[i]]), 
       col = sample(colors(), 1), 
       xlab = "")
}

```

### Navon

Trial-level exclusion criteria:
* The reaction time for each row is replaced with missing values (NAs) if it is equal to or lower than 150ms

Participant-level exclusion criteria:
* A participant's data for the Navon task is replaced with missing values (NAs) if their accuracy (% of correct trials out of all trials) was equal to or lower than 60% *or* if the ratio of the most frequent response to all responses is equal to or higher than .95.

Data processing: for each participant, we calculate the global-local precedence index (bias toward a global processing level), and global-to-local interference index (positive values indicate the extent to which the bias toward global stimuli interferes with processing local information).
* Global-local precedence index: Standardized mean difference (cohen’s d) in RT between global and local judgments on consistent trials only 
* Global-to-local interference index: Standardized mean difference (cohen’s d) in RT between inconsistent and consistent trials in local condition only 
These are described and visualized.

```{r Navon Task}

# Omit experiment-general columns & practice (etc.) rows
data_NavonT <- data_NavonT %>%
  select(Participant.Private.ID, Spreadsheet:Image) %>%
  filter(display %in% c("task1", "task2")) %>% 
  filter(Screen.Name == "Screen 3")

# Create a column for consistency
data_NavonT <- data_NavonT %>%
  mutate(Consistency = case_when((Image == "bigHsmallH.png" | Image == "bigSsmallS.png") ~ "Consistent", 
                                 (Image == "bigHsmallS.png" | Image == "bigSsmallH.png") ~ "Inconsistent"))
data_NavonT$Consistency <- as.factor(data_NavonT$Consistency)
data_NavonT$display <- as.factor(data_NavonT$display)

# Accuracy for each task and consistency
Accuracy_Navon <- data_NavonT %>%
  group_by(Consistency, display) %>%
  summarise(Accuracy_cond = sum(Correct, na.rm = TRUE) / (sum(Correct, na.rm = TRUE) + sum(Incorrect, na.rm = TRUE)))

# Exclusion Criteria: Trial-Level
  # RT <= 150ms
data_NavonT$Reaction.Time[which(data_NavonT$Reaction.Time <= 150)] <- NA

# Exclusion Criteria: Participant-Level
  # <= 60% accuracy
data_NavonT <- data_NavonT %>%
  group_by(Participant.Private.ID) %>%
  mutate(Accuracy = sum(Correct, na.rm = TRUE) / (sum(Correct, na.rm = TRUE) + sum(Incorrect, na.rm = TRUE))) %>%
  mutate(Navon_omit = case_when(Accuracy > .60 ~ 0, 
                                Accuracy <= .60 ~ 1))

# Omit data based on Navon_omit
data_NavonT$Reaction.Time[which(data_NavonT$Navon_omit == 1)] <- NA

# Subset to only look at correct answers
data_NavonT <- data_NavonT %>%
  filter(Correct == "1")

# Ungroup df
data_NavonT <- ungroup(data_NavonT)

# Global SD 1 (consistent trials only)
sd_Global_con <- data_NavonT %>%
  filter((Consistency == "Consistent") & display == "task1") %>%
  summarise(sd(Reaction.Time, na.rm = TRUE))
sd_Global_con <- sd_Global_con[[1]]

# Local SD 1 (consistent trials only)
sd_Local_con <- data_NavonT %>%
  filter((Consistency == "Consistent") & display == "task2") %>%
  summarise(sd(Reaction.Time, na.rm = TRUE))
sd_Local_con <- sd_Local_con[[1]]

# Local SD 2 (inconsistent trials only)
sd_Local_incon <- data_NavonT %>%
  filter((Consistency == "Inconsistent") & display == "task2") %>%
  summarise(sd(Reaction.Time, na.rm = TRUE))
sd_Local_incon <- sd_Local_incon[[1]]
 
# Calculate pooled SD
# https://www.statisticshowto.com/pooled-standard-deviation/ 

# Pooled SD consistent (local and global)
pooled_SD_con <- sqrt((sd_Global_con^2 + sd_Local_con^2)/2)

# Pooled SD local (consistent and inconsistent)
pooled_SD_local <- sqrt((sd_Local_incon^2 + sd_Local_con^2)/2)

# Final form of the Navon data 
  # Select relevant columns and transform the data so columns reflect mean reaction times in each of the four conditions (local/global x consistent/inconsistent)
data_NavonT_final <- data_NavonT %>% 
  select(Participant.Private.ID, Consistency, display, Reaction.Time) %>%
  group_by(Participant.Private.ID, Consistency, display) %>%
  summarise(NavonReactionTimeMean = mean(Reaction.Time, na.rm = TRUE), 
            .groups = "keep") %>%
  pivot_wider(names_from = c(Consistency, display), 
              values_from = NavonReactionTimeMean)

# Global-local precedence index: Standardized mean difference (cohen’s d) in RT between global and local judgments on consistent trials only 
data_NavonT_final <- data_NavonT_final %>%
  mutate(GlobalToLocalPrecedence = ((Consistent_task1 - Consistent_task2) / as.numeric(pooled_SD_con)))

# Global-to-local interference index: Standardized mean difference (cohen’s d) in RT between inconsistent and consistent trials in local condition only
data_NavonT_final <- data_NavonT_final %>%
  mutate(GlobalToLocalInterference = ((Inconsistent_task1 - Consistent_task2) / as.numeric(pooled_SD_local)))

# Exclude nonvalid participants
data_NavonT_final <- merge(data_NavonT_final, data_ConsentValidity, 
                           by = "Participant.Private.ID")

# Validity/languege filter + get rid of participants who stopped mid-task + select relevant columns
data_NavonT_final <- data_NavonT_final %>%
  filter(Validity.quantised %in% c(1, NA, "") & Language %in% c("Sujuva / Ã¤idinkieli", "Keskitaso / keskusteleva","Muu, mikÃ¤?")) %>%  
  filter(Participant.Private.ID != "4907987" & Participant.Private.ID != "4960307" & Participant.Private.ID != "5317567" & Participant.Private.ID != "5372338" & Participant.Private.ID != "5526450" & Participant.Private.ID != "5578226" & Participant.Private.ID != "5626669" & Participant.Private.ID != "5808176") %>%
  select(Participant.Private.ID, Consistent_task1, Consistent_task2, Inconsistent_task1, Inconsistent_task2, GlobalToLocalPrecedence, GlobalToLocalInterference)

# Two participants' reaction times were replaced with NAs due to their accuracy being below 60%; this produced NaNs in the calculations, so the NaNs are here replaced with NAs
data_NavonT_final <- data_NavonT_final %>% 
  filter(Participant.Private.ID != "5608075" & Participant.Private.ID != "5411218")

# Describe
Navon <- c("Consistent_task1", "Consistent_task2", "Inconsistent_task1", "Inconsistent_task2", "GlobalToLocalPrecedence", "GlobalToLocalInterference")
describe(data_NavonT_final[, Navon])
par(mfrow = c(2, 3))
for (i in 1:length(Navon)) {
  hist(data_NavonT_final[, Navon[i]], 
       main = colnames(data_NavonT_final[Navon[i]]), 
       col = sample(colors(), 1), 
       xlab = "")
}

```

### Necker

Exclusion criteria: none for now

Data processing: for each participant, we calculate the average rate of space bar hits (switches the participant experienced) per second. 

```{r Necker Cube}

# Omit irrelevant rows and columns
data_NeckerT <- data_NeckerT %>%
  select(Participant.Private.ID, Spreadsheet:Image) %>%
  filter(display %in% c("Trial 1", "Trial 2"))

# Calculate sum score for how many times space bar was hit in total
data_NeckerT_final <- data_NeckerT %>%
  select(Participant.Private.ID, Response) %>%
  group_by(Participant.Private.ID) %>%
  summarise(NeckerCountTotal = sum(Response == "space", na.rm = TRUE))

# Calculate switches per second
data_NeckerT_final <- data_NeckerT_final %>%
  mutate(NeckerTotalRate = NeckerCountTotal/60)

# Merge to exclude participants
data_NeckerT_final <- merge(data_NeckerT_final, data_ConsentValidity, 
                            by = "Participant.Private.ID")

# Subset by validity and consent info + omit the irrelevant columns
data_NeckerT_final <- data_NeckerT_final %>%
  filter(Validity.quantised %in% c("1", NA, "") & Language %in% c("Sujuva / Ã¤idinkieli", "Keskitaso / keskusteleva", "Muu, mikÃ¤?")) %>%
  filter(Participant.Private.ID != "5385143") %>% 
  select(Participant.Private.ID, NeckerCountTotal, NeckerTotalRate)

# Describe
describe(data_NeckerT_final$NeckerTotalRate)
hist(data_NeckerT_final$NeckerTotalRate, 
       main = "Necker Total Rate", 
       col = sample(colors(), 1), 
       xlab = "")

```

### Unusual Uses Task

Exclusion criteria: None.

Data processing: for each participant, we calculate the average of the fluency sum scores (ratings by two raters) and the flexibility sum scores (ratings by two raters). The fluency/flexibility sum scores capture the number of answers across all trials that were classified as fluent/flexible.
* Fluency is a measure of how many unique unusual uses a ppt can think of for each item. 
* Flexibility score refers to the uniqueness of the functional categories of items, i.e. a participant gets two point for using a shoe as a doorstop and a flowerpot but only one point for a houseplant pot and a bonsai tree pot (same functional use). 

```{r Unusual Uses Task}

# Load data
data_UUT <- read.csv("UUT_scoring_COMBINED_REVISED_Rcsv.csv", sep = ";")

# Rename participant ID column
names(data_UUT)[1] <- "Participant.Private.ID"

# Select relevant columns + calculate the fluency and flexibility sum score for each participant per rater
data_UUT <- data_UUT %>% 
  select(Participant.Private.ID, S.Fluency, S.Flex, K.Fluency, K.Flex) %>%
  group_by(Participant.Private.ID) %>%
  summarise(SFlexibilitySum = sum(S.Flex), SFluencySum = sum(S.Fluency),
            KFlexibilitySum = sum(K.Flex), KFluencySum = sum(K.Fluency))

# Calculate participant scores as a mean of the two raters' scores (one for fluency, one for flexibility)
data_UUT$FlexSum <- rowMeans(cbind(data_UUT$SFlexibilitySum,
                                   data_UUT$KFlexibilitySum), 
                             na.rm = TRUE)
data_UUT$FluencySum <- rowMeans(cbind(data_UUT$SFluencySum, 
                                      data_UUT$KFluencySum), 
                                na.rm = TRUE)

# Check validity + extract relevant columns
data_UUT <- merge(data_UUT, data_ConsentValidity, 
                  by = "Participant.Private.ID")
data_UUT <- data_UUT %>%
  filter(Validity.quantised %in% c("1", NA, "") & Language %in% c("Sujuva / Ã¤idinkieli", "Keskitaso / keskusteleva", "Muu, mikÃ¤?")) %>%
  select(Participant.Private.ID, FlexSum, FluencySum)

# Describe
uut <- c("FlexSum", "FluencySum")
describe(data_UUT[, uut])
par(mfrow = c(2, 1))
for (i in 1:length(uut)) {
  hist(data_UUT[, uut[i]], 
       main = colnames(data_UUT[uut[i]]), 
       col = sample(colors(), 1), 
       xlab = "")
}

```

# Merge task variables

Doesn't yet include the CI task! 

```{r Combine the Task Dataframes}

# Merge dataframes
data_T_total1 <- merge(data_nogoT_final, data_NavonT_final, 
                       by = "Participant.Private.ID", all = TRUE)
data_T_total2 <- merge(data_UUT, data_NeckerT_final, 
                       by = "Participant.Private.ID", all = TRUE)
data_T_total <- merge(data_T_total1, data_T_total2, 
                      by = "Participant.Private.ID", all = TRUE)
rm(data_T_total1, data_T_total2)

# Extract the relevant columns
data_T_sub <- data_T_total %>%
  select(Participant.Private.ID, GNGdprime, GNGbeta, MeanGoRT, SDGoRT, GlobalToLocalPrecedence, GlobalToLocalInterference, NeckerTotalRate, FlexSum, FluencySum)

# Participant ID into factor
data_T_sub$Participant.Private.ID <- as.factor(data_T_sub$Participant.Private.ID)

```


## Analyze Missing Data in Tasks

Participants with large amounts of missing data or suspicious patterns of missing data are removed here. It should be noted that missing data refers to data that was missing from the start *and* data that was replaced with missing values if the participant met one of the exclusion criteria. Done separately for questionnaires and tasks.

```{r Analyze Missing Data in Tasks}

# Task variables
t_var <- c("GNGdprime", "GNGbeta", "MeanGoRT", "SDGoRT", "GlobalToLocalPrecedence", "GlobalToLocalInterference", "NeckerTotalRate", "FlexSum", "FluencySum")

# Add a column for count of missing data per ppt
data_T_sub$t_missingdata <- rowSums(is.na(data_T_sub[, t_var]))
table(data_T_sub$t_missingdata)
plot(sort(data_T_sub$t_missingdata))

# Merge demographic information to investigate missingness
# NOTE: what to do about the age correlation?
data_T_all <- data_Q_total %>%
  select(Participant.Private.ID, gender, Age, Country, Language, Education)
data_T_sub <- merge(data_T_all, data_T_sub, 
                    by = "Participant.Private.ID", all = TRUE)
gg_miss_fct(x = data_T_sub, fct = Age)
cor.test(data_T_sub$Age, data_T_sub$t_missingdata, 
         method = "spearman", exact = FALSE)

# Remove participants with 4 or more missing task variables
data_T_sub <- data_T_sub %>%
  group_by(Participant.Private.ID) %>%
  filter(t_missingdata < 4) %>% 
  select(-t_missingdata)

```

## Check Task Distributions and Correlations

This chunk checks distributions of task variables and correlations between them:
* If distributions are highly skewed (>1), transformations are applied to decrease skewness
* If two variables are highly correlated, one of them is dropped (Eisenberg et al.) (r>.85)

```{r Normality and Correlations of Tasks}

# Normality of variables: visualize with histograms
data_T_sub <- as.data.frame(data_T_sub)
par(mfrow = c(2, 2))
for (i in 1:length(t_var)) {
  hist(data_T_sub[, t_var[i]],
       main = colnames(data_T_sub[t_var[i]]),
       col = sample(colors(), 1),
       xlab = "")
}

# Describe: mean, skew, kurtosis
describe(data_T_sub[, t_var])[c("mean", "skew", "kurtosis")]

# Transforming all variables with absolute skew > 1
data_T_sub$GNGbetalog <- log10(data_T_sub$GNGbeta)
data_T_sub$SDGoRTlog <- log10(data_T_sub$SDGoRT)
data_T_sub$GlobalToLocalInterferencelog <- log10(data_T_sub$GlobalToLocalInterference + 3) # added 3 because log10 cannot handle 0s/negative numbers 
data_T_sub$NeckerTotalRatelog <- log10(data_T_sub$NeckerTotalRate + 0.5) # added 1 because log10 cannot handle 0s
data_T_sub$GlobalToLocalPrecedencelog <- log10(max(data_T_sub$GlobalToLocalPrecedence + 1, na.rm = TRUE) - data_T_sub$GlobalToLocalPrecedence)

# Task variables: transformed
t_var_log <- c("GNGdprime", "GNGbetalog", "MeanGoRT", "SDGoRTlog", "GlobalToLocalPrecedencelog", "GlobalToLocalInterferencelog", "NeckerTotalRatelog", "FlexSum", "FluencySum")

# Describe
describe(data_T_sub[, t_var_log])[c("mean", "skew", "kurtosis")]

# Visualize
par(mfrow = c(2, 2))
for (i in 1:length(t_var_log)) {
  hist(data_T_sub[, t_var_log[i]],
       main = colnames(data_T_sub[t_var_log[i]]),
       col = sample(colors(), 1),
       xlab = "")
}

# Correlation matrix
round(cor(data_T_sub[, t_var_log], method = "pearson", 
          use = "pairwise.complete.obs"), 2)
abs(round(cor(data_T_sub[, t_var_log], method = "pearson", 
              use = "pairwise.complete.obs"), 2)) > .85
corrplot(cor(data_T_sub[, t_var_log], method = "pearson", 
             use = "pairwise.complete.obs"))

# Exclude FluencySum as it correlates highly with FlexSum; only include log-transformed variables where necessary
data_T_sub <- data_T_sub %>% 
  select(Participant.Private.ID, GNGdprime, GNGbetalog, MeanGoRT, SDGoRTlog, GlobalToLocalInterferencelog, GlobalToLocalPrecedencelog, NeckerTotalRatelog, FlexSum)

```

# Format the Task and Questionnaire Dataframes 

```{r Combine Task and Questionnaire Dataframes}

data_all_sub_cleaned <- merge(data_Q_sub, data_T_sub, 
                              by = "Participant.Private.ID", 
                              all = TRUE)

# Ungroup the dataframe
ungroup(data_all_sub_cleaned)

# Select relevant columns
data_all_sub_cleaned <- data_all_sub_cleaned %>%
  select(-Participant.Private.ID)

# Check missing data
sort(colSums(is.na(data_all_sub_cleaned)))
sort(rowSums(is.na(data_all_sub_cleaned)))
# As more participants were excluded based on their task data than based on their questionnaire data, there are participants that have more missing variables than they should based on the thresholds set for tasks and questionnaires (7+3=10). These participants are missing all their task data. 
t_var_log <- t_var_log[-9]
which(rowSums(is.na(data_all_sub_cleaned[, t_var_log])) == length(t_var_log))
View(data_all_sub_cleaned[which(rowSums(is.na(data_all_sub_cleaned[, t_var_log])) == length(t_var_log)), ])
# NOTE: not sure what to do here, I will exclude for now?
data_all_sub_cleaned <- data_all_sub_cleaned[-which(rowSums(is.na(data_all_sub_cleaned[, t_var_log])) == length(t_var_log)), ]

# Rename
data_efa <- data_all_sub_cleaned

# Remove all rows with missing values
data_efa <- na.omit(data_efa)

# Describe
par(mfrow = c(2, 5))
for (i in 1:ncol(data_efa)) {
  hist(data_efa[, i], col = sample(colors(), 1))
}
describe(data_efa)[c("mean", "skew", "kurtosis")]

```

# Data Imputation

```{r Data Imputation}

# Check how much data is missing per row and column
# sort(table(colSums(is.na(data_efa))))
# sort(table(rowSums(is.na(data_efa))))

# % of missing data overall
# sum(is.na(data_efa))/(ncol(data_efa)*nrow(data_efa))*100 # 0.9% of data missing

# NOTE: do we want to impute? 127 participants with no missing data, 139 overall

```

