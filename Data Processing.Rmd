---
title: "Data Processing"
author: "Milla Pihlajamaki and Caitlin Dawson"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: pdf_document
---

# Milla's notes / Things to discuss re exclusion criteria
Eisenberg et al. (2018) used the following exclusion criteria for **tasks**:
* median response time <= 200ms
* > 25% of responses missing
* accuracy <= 60%
* no single response was given > 95% of the time
If any of these criteria were fulfilled, the participant's data for that task were replaced with missing values. If any participant had missing data for 4 or more tasks, their data were omitted completely. **They also mention they had slightly different exclusion criteria for some of the tasks but I cannot find more details on this.** 
The same paper reports no exclusion criteria for self-report variables (questionnaires). They only say that they checked the descriptives (mean, skew, distribution etc.) of each item and the structure of each questionnaire with CFA. **They do not report any exclusion criteria, or excluding any data on items.** Again, not sure if I just didn't know where to find this info.
Finally, once the variables are retrieved, they exclude a variable if it is skewed > 1 after transforming it, or if two variables from the same task/questionnaire correlate > 0.85.
Relevance for us: I think it would be good if we could find the specific exclusion criteria for each task to get an idea of when they made exceptions. **I think it would also be good if we could figure out if they used any exclusion criteria for the questionnaires / if they excluded any questionnaire data based on item-level characteristics.**

# Notes 
* takes data downloaded from Gorilla **blinded, semicolon separated, csv format, short form, versions 22-25**
* cleans and processes data for the study 'Scientific thinking and decision-making in everyday life'

Version history: 
* version 22: Links fixed in information and consent
* version 23: HJ updated the experiment (source deleted from CI task--only 5 sources because of trouble getting a stable URL; Viiskunta source was taken down)
* version 24: HJ updated CI task to include all 6 sources again, one of them as image
* version 25: HJ. Word adult taken off from Study Information and Content

# Load packages

```{r Load Packages, include = FALSE}

library(corrplot)
library(dplyr)
library(ggplot2)
library(lavaan)
library(naniar)
library(psych)
library(psycho)
library(tidyr)

```

# Load Data 

This chunk loads the data and omits experiment-general columns from all files (except for participant private ID).

```{r Load Data, include = FALSE}

# Versions 22-25
versions <- paste0("v", 22:25)
questionnaire_types <- c("2wlk", "79hv", "7qsg", "81k4", "c7cw", "cl1u", "eygv", "go4a", "mz16", "o43u", "otb8", "oyla", "w8n1")
task_types <- c("7mar", "8mu5", "9nll", "n8ns", "tg12", "zryk")
questionnaire_names <- c("epiQ", "heurQ", "openQ", "validQ", "demoQ", "sciattQ", "needcogQ", "scicurQ", "needcloQ", "infoQ", "rpQ", "inthumQ", "big5Q")
task_names <- c("uutT", "NavonT", "NeckerT", "nogoT", "matreasT", "citiT")
names_all <- c(questionnaire_names, task_names)

for (j in 1:length(versions)) {
  for (i in 1:length(questionnaire_types)) {
  assign(paste0("data_", questionnaire_names[i], "_", versions[j]),
         read.csv(paste0("data_exp_55551-", versions[j], "_questionnaire-", questionnaire_types[i], ".csv"), sep = ";")[, -c(1:12, 14:31)])
    }
}

for (j in 1:length(versions)) {
  for (i in 1:length(task_types)) {
    assign(paste0("data_", task_names[i], "_", versions[j]),
           read.csv(paste0("data_exp_55551-", versions[j], "_task-", task_types[i], ".csv"), sep = ";")[, -c(1:12, 14:31)])
  }
}

```

# Create Questionnaire Dataframe

This series of chunks computes the necessary data cleaning to merge the questionnaire dataframes and merges them.

## Merge the Different Versions of Each Questionnaire

This chunk combines the different versions of each questionnaire (leaving us with one df per questionnaire).

```{r Merge the Versions, include = FALSE}

# Concatenate the versions of each questionnaire
data_big5Q <- rbind(data_big5Q_v22, data_big5Q_v23, 
                    data_big5Q_v24, data_big5Q_v25) 
data_citiT <- rbind(data_citiT_v22, data_citiT_v23, 
                    data_citiT_v24, data_citiT_v25) 
data_demoQ <- rbind(data_demoQ_v22, data_demoQ_v23, 
                    data_demoQ_v24, data_demoQ_v25)
data_epiQ <- rbind(data_epiQ_v22, data_epiQ_v23, 
                   data_epiQ_v24, data_epiQ_v25)
data_nogoT <- rbind(data_nogoT_v22, data_nogoT_v23, 
                    data_nogoT_v24, data_nogoT_v25)
data_heurQ <- rbind(data_heurQ_v22, data_heurQ_v23, 
                    data_heurQ_v24, data_heurQ_v25)
data_inthumQ <- rbind(data_inthumQ_v22, data_inthumQ_v23, 
                      data_inthumQ_v24, data_inthumQ_v25)
data_matreasT <- rbind(data_matreasT_v22, data_matreasT_v23, 
                       data_matreasT_v24, data_matreasT_v25)
data_NavonT <- rbind(data_NavonT_v22, data_NavonT_v23, 
                     data_NavonT_v24, data_NavonT_v25)
data_NeckerT <- rbind(data_NeckerT_v22, data_NeckerT_v23, 
                      data_NeckerT_v24, data_NeckerT_v25)
data_needcloQ <- rbind(data_needcloQ_v22, data_needcloQ_v23, 
                       data_needcloQ_v24, data_needcloQ_v25)
data_needcogQ <- rbind(data_needcogQ_v22, data_needcogQ_v23, 
                       data_needcogQ_v24, data_needcogQ_v25)
data_openQ <- rbind(data_openQ_v22, data_openQ_v23, 
                    data_openQ_v24, data_openQ_v25)
data_rpQ <- rbind(data_rpQ_v22, data_rpQ_v23, 
                  data_rpQ_v24, data_rpQ_v25)
data_sciattQ <- rbind(data_sciattQ_v22, data_sciattQ_v23, 
                      data_sciattQ_v24, data_sciattQ_v25)
data_scicurQ <- rbind(data_scicurQ_v22, data_scicurQ_v23, 
                      data_scicurQ_v24, data_scicurQ_v25)
data_infoQ <- rbind(data_infoQ_v22, data_infoQ_v23, 
                    data_infoQ_v24, data_infoQ_v25)
data_uutT <- rbind(data_uutT_v22, data_uutT_v23, 
                   data_uutT_v24, data_uutT_v25)
data_validQ <- rbind(data_validQ_v22, data_validQ_v23, 
                     data_validQ_v24, data_validQ_v25)

# Remove the empty last line of each file
for (i in 1:length(names_all)) {
  assign(paste0("data_", names_all[i]),
         get(paste0("data_", names_all[i]))[which(!is.na(get(paste0("data_", names_all[i]))[, 1])), ])
}

# Remove version files to clean up the environment
for (j in 1:length(versions)) {
  for (i in 1:length(names_all)) {
    rm(list = paste0("data_", names_all[i], "_", versions[j]))
  }
}

``` 


## Merge Questionnaire Dataframes

This chunk merges all questionnaire dataframes, leaving us with one dataframe for all questionnaire data.

```{r Merge Questionnaire Dataframes, include = FALSE}

# Rename two columns before combining all the questionnaires into one df (the column has the same name in each questionnaire file). 
    # END.QUESTIONNAIRE column gives you the response time for that questionnaire. 
    # Randomise.questionnaire.elements. column gives you a logical value indicating whether the questionnaire items were randomized for that questionnaire.

# END.QUESTIONNAIRE
names(data_demoQ)[names(data_demoQ) == "END.QUESTIONNAIRE"] <- "END.QUESTIONNAIRE.DEMOGRAPHICS"
names(data_big5Q)[names(data_big5Q) == "END.QUESTIONNAIRE"] <- "END.QUESTIONNAIRE.BIG5"
names(data_epiQ)[names(data_epiQ) == "END.QUESTIONNAIRE"] <- "END.QUESTIONNAIRE.EPISTEMIC"
names(data_heurQ)[names(data_heurQ) == "END.QUESTIONNAIRE"] <- "END.QUESTIONNAIRE.HEURISTIC"
names(data_inthumQ)[names(data_inthumQ) == "END.QUESTIONNAIRE"] <- "END.QUESTIONNAIRE.INT.HUM"
names(data_needcloQ)[names(data_needcloQ) == "END.QUESTIONNAIRE"] <- "END.QUESTIONNAIRE.NEED.CLO"
names(data_needcogQ)[names(data_needcogQ) == "END.QUESTIONNAIRE"] <- "END.QUESTIONNAIRE.NEED.COG"
names(data_openQ)[names(data_openQ) == "END.QUESTIONNAIRE"] <- "END.QUESTIONNAIRE.OPEN.THINK"
names(data_rpQ)[names(data_rpQ) == "END.QUESTIONNAIRE"] <- "END.QUESTIONNAIRE.RANDOM.PROB"
names(data_sciattQ)[names(data_sciattQ) == "END.QUESTIONNAIRE"] <- "END.QUESTIONNAIRE.SCIENCE.ATT"
names(data_scicurQ)[names(data_scicurQ) == "END.QUESTIONNAIRE"] <- "END.QUESTIONNAIRE.SCIENCE.CUR"
names(data_infoQ)[names(data_infoQ) == "END.QUESTIONNAIRE"] <- "END.QUESTIONNAIRE.INFO"
names(data_validQ)[names(data_validQ) == "END.QUESTIONNAIRE"] <- "END.QUESTIONNAIRE.VALIDITY"

# Randomise.questionnaire.elements.
names(data_demoQ)[names(data_demoQ) == "Randomise.questionnaire.elements."] <- "Randomise.questionnaire.elements..DEMOGRAPHICS"
names(data_big5Q)[names(data_big5Q) == "Randomise.questionnaire.elements."] <- "Randomise.questionnaire.elements..BIG5"
names(data_epiQ)[names(data_epiQ) == "Randomise.questionnaire.elements."] <- "Randomise.questionnaire.elements..EPISTEMIC"
names(data_heurQ)[names(data_heurQ) == "Randomise.questionnaire.elements."] <- "Randomise.questionnaire.elements..HEURISTIC"
names(data_inthumQ)[names(data_inthumQ) == "Randomise.questionnaire.elements."] <- "Randomise.questionnaire.elements..INT.HUM"
names(data_needcloQ)[names(data_needcloQ) == "Randomise.questionnaire.elements."] <- "Randomise.questionnaire.elements..NEED.CLO"
names(data_needcogQ)[names(data_needcogQ) == "Randomise.questionnaire.elements."] <- "Randomise.questionnaire.elements..NEED.COG"
names(data_openQ)[names(data_openQ) == "Randomise.questionnaire.elements."] <- "Randomise.questionnaire.elements..OPEN.THINK"
names(data_rpQ)[names(data_rpQ) == "Randomise.questionnaire.elements."] <- "Randomise.questionnaire.elements..RANDOM.PROB"
names(data_sciattQ)[names(data_sciattQ) == "Randomise.questionnaire.elements."] <- "Randomise.questionnaire.elements..SCIENCE.ATT"
names(data_scicurQ)[names(data_scicurQ) == "Randomise.questionnaire.elements."] <- "Randomise.questionnaire.elements..SCIENCE.CUR"
names(data_infoQ)[names(data_infoQ) == "Randomise.questionnaire.elements."] <- "Randomise.questionnaire.elements..INFO"
names(data_validQ)[names(data_validQ) == "Randomise.questionnaire.elements."] <- "Randomise.questionnaire.elements..VALIDITY"

# Merge by participant ID
# All rows for all dataframes are kept
data1 <- merge(data_demoQ, data_big5Q, 
               by = "Participant.Private.ID", 
               all = TRUE)
data2 <- merge(data_epiQ, data_heurQ, 
               by = "Participant.Private.ID", 
               all = TRUE)
data3 <- merge(data_inthumQ, data_needcloQ, 
               by = "Participant.Private.ID", 
               all = TRUE)
data4 <- merge(data_needcogQ, data_openQ, 
               by = "Participant.Private.ID", 
               all = TRUE)
data5 <- merge(data_rpQ, data_sciattQ, 
               by = "Participant.Private.ID", 
               all = TRUE)
data6 <- merge(data_scicurQ, data_infoQ, 
               by = "Participant.Private.ID", 
               all = TRUE)
data7 <- merge(data1, data2, 
               by = "Participant.Private.ID", 
               all = TRUE)
data8 <- merge(data3, data4, 
               by = "Participant.Private.ID", 
               all = TRUE)
data9 <- merge(data5, data6, 
               by = "Participant.Private.ID", 
               all = TRUE)
data10 <- merge(data7, data8, 
                by = "Participant.Private.ID", 
                all = TRUE)
data11 <- merge(data9, data_validQ, 
                by = "Participant.Private.ID", 
                all = TRUE)
data_Q_total <- merge(data10, data11, 
                      by = "Participant.Private.ID", 
                      all = TRUE)

# Clean the environment
rm(data1, data2, data3, data4, data5, data6, data7, data8, data9, data10, data11)
rm(data_big5Q, data_epiQ, data_heurQ, data_infoQ, data_inthumQ, data_needcloQ, data_needcogQ, data_openQ, data_rpQ, data_sciattQ, data_scicurQ)

```

# Questionnaire Data

## Exclusion Criteria

**ALL QUESTIONNAIRES AND TASKS:** This chunk goes through all questionnaire data, removing **all data for a participant** if they met one or more of the following exclusion criteria:
* stated their data are not valid
* stated their Finnish is not Äidinkieli, Keskusteleva, or Muu, Mikä.

**TIPI - RANDOMNESS/PROBABILITY:** Furthermore, a participant's questionnaire data for a particular questionnaire is replaced with missing values (NAs) if they met the following exclusion criterion *for that questionnaire*:
* their response time for the questionnaire was shorter than the number of items x 1000 milliseconds

**TIPI - RANDOMNESS/PROBABILITY:** For questionnaires that included reverse-coded items, a participant's questionnaire data for that questionnaire is replaced with missing values (NAs) if they met the following exclusion criterion *for that questionnaire*:
* 90% or more of their responses for that questionnaire fell on the same response option

## Questionnaire Data Processing

After excluding data in accordance with the exclusion criteria above, the following steps are taken for each questionnaire:
* reverse-code items (if necessary)
* visualize and describe each item
* check the structure of the questionnaire data with Cronbach's alpha
* calculate mean or sum scores according to the questionnaire instructions
* replace sum score 0s with NAs (if necessary)
* visualize and describe mean or sum scores


### Validity and Language

```{r Validity and Language, include = FALSE}
 
# Save data for nonvalid ppts ("älä huomioi aineistoani") in a separate file prior to removal
data_nonvalid <- data_Q_total %>%
  filter(Validity == "Ã„lÃ¤ huomioi aineistoani. Jokin muu syy esti minua osallistumasta kunnolla." | Validity == "Ã„lÃ¤ huomioi aineistoani. En suurimmaksi osaksi keskittynyt tai lukenut kysymyksiÃ¤ kunnolla.")

# Save data for participants with nonvalid language answers in a separate file. One participant who said "muu" but said they were fluent but not native level was included.
data_Q_lang_omit <- data_Q_total %>%
  filter(Language != "Sujuva / Ã¤idinkieli" & Language != "Keskitaso / keskusteleva" & Language != "Muu, mikÃ¤?")

# Remove nonvalid participants
data_Q_total <- data_Q_total %>%
  filter(Validity %in% c("Aineistoani voi kÃ¤yttÃ¤Ã¤.", "Muu, mikÃ¤? ", NA, "") & Language %in% c("Sujuva / Ã¤idinkieli", "Keskitaso / keskusteleva", "Muu, mikÃ¤?"))

```

### TIPI (Big Five)

Reliability are not calculated for TIPI, see Gosling's website for explanation. http://gosling.psy.utexas.edu/scales-weve-developed/ten-item-personality-measure-tipi/ 

```{r TIPI, include = FALSE}

# Big Five
b5 <- c("agreeablenessNormal", "emotionalStabilityNormal", "extroversionNormal", "conscientiousnessNormal", "opennessNormal", "agreeablenessReverse", "emotionalStabilityReverse", "extroversionReverse", "conscientiousnessReverse", "opennessReverse")
b5_rev <- c("agreeablenessReverse", "emotionalStabilityReverse", "extroversionReverse", "conscientiousnessReverse", "opennessReverse")

# Exclusion criteria
  # >= 90% of responses fall on the same response option
  # <= 10000ms response time 
data_Q_total <- data_Q_total %>% 
  mutate(B5_max = max(table(c(agreeablenessNormal, agreeablenessReverse, conscientiousnessNormal, conscientiousnessReverse, extroversionNormal, extroversionReverse, emotionalStabilityNormal, emotionalStabilityReverse, opennessNormal, opennessReverse)), na.rm = TRUE) /
           sum(table(c(agreeablenessNormal, agreeablenessReverse, conscientiousnessNormal, conscientiousnessReverse, extroversionNormal, extroversionReverse, emotionalStabilityNormal, emotionalStabilityReverse, opennessNormal, opennessReverse)), na.rm = TRUE))

data_Q_total <- data_Q_total %>% 
  mutate(B5_omit = case_when(B5_max >= .90 | END.QUESTIONNAIRE.BIG5 <= 1000*length(b5) ~ 1,
                             B5_max < .90 & END.QUESTIONNAIRE.BIG5 > 1000*length(b5) ~ 0))
 
# Omit B5 data based on the B5_omit column
data_Q_total[which(data_Q_total$B5_omit == 1), b5] <- NA

# Reverse-code
for (i in 1:length(b5_rev)) {
  data_Q_total[, b5_rev[i]] <- reverse.code(keys = c(-1), 
                                            items = data_Q_total[, b5_rev[i]],
                                            mini = c(1), 
                                            maxi = c(7)) 
}

# Describe: item-level
par(mfrow = c(2, 5))
for (i in 1:length(b5)) {
  hist(as.numeric(data_Q_total[, b5[i]]),
       main = colnames(data_Q_total[b5[i]]),
       col = sample(colors(), 1),
       breaks = seq(0, 7, 1),
       xlab = ""
       )
}
describe(data_Q_total[, b5])

# Calculate means
data_Q_total$AMean <- rowMeans(cbind(data_Q_total$agreeablenessNormal, 
                                     data_Q_total$agreeablenessReverse), 
                               na.rm = TRUE)
data_Q_total$CMean <- rowMeans(cbind(data_Q_total$conscientiousnessNormal,                                     data_Q_total$conscientiousnessReverse),
                               na.rm = TRUE)
data_Q_total$EMean <- rowMeans(cbind(data_Q_total$extroversionNormal,
                                     data_Q_total$extroversionReverse), 
                               na.rm = TRUE)
data_Q_total$ESMean <- rowMeans(cbind(data_Q_total$emotionalStabilityNormal,
                                      data_Q_total$emostabilityReverse), 
                                na.rm = TRUE)
data_Q_total$OMean <- rowMeans(cbind(data_Q_total$opennessNormal,
                                     data_Q_total$opennessReverse), 
                               na.rm = TRUE)

# Describe: mean score level
b5_mean <- c("AMean", "OMean", "EMean", "CMean", "ESMean")
par(mfrow = c(2, 3))
for (i in 1:length(b5_mean)) {
  hist(data_Q_total[, b5_mean[i]],
       main = colnames(data_Q_total[b5_mean[i]]),
       col = sample(colors(), 1),
       breaks = seq(0, 7, 1),
       xlab = ""
       )
}
describe(data_Q_total[, b5_mean])

```

### Epistemic Curiosity

```{r Epistemic Curiosity, include = FALSE}

# Epistemic Curiosity
epicur <- c(paste0("Icuriosity", 1:5), paste0("Dcuriosity", 1:5))

# Exclusion criteria
  # <= 10000ms response time 
data_Q_total[which(data_Q_total$END.QUESTIONNAIRE.EPISTEMIC <= 1000*length(epicur)), epicur] <- NA

# Describe: item-level
par(mfrow = c(2, 5))
for (i in 1:length(epicur)) {
  hist(data_Q_total[, epicur[i]],
       main = colnames(data_Q_total[epicur[i]]),
       col = sample(colors(), 1),
       breaks = seq(0, 5, 1),
       xlab = ""
       )
}
describe(data_Q_total[, epicur])

# Cronbach's alphas
psych::alpha(data_Q_total[, epicur[1:5]])
psych::alpha(data_Q_total[, epicur[6:10]])

# Calculate sum scores
data_Q_total$ICuriositySum <- rowSums(cbind(data_Q_total$Icuriosity1, data_Q_total$Icuriosity2, data_Q_total$Icuriosity3, data_Q_total$Icuriosity4, data_Q_total$Icuriosity5), na.rm = TRUE)
data_Q_total$DCuriositySum <- rowSums(cbind(data_Q_total$Dcuriosity1, data_Q_total$Dcuriosity2, data_Q_total$Dcuriosity3, data_Q_total$Dcuriosity4, data_Q_total$Dcuriosity5), na.rm = TRUE)

# Replace sum score 0s with NAs
data_Q_total$ICuriositySum[which(data_Q_total$ICuriositySum == 0)] <- NA
data_Q_total$DCuriositySum[which(data_Q_total$DCuriositySum == 0)] <- NA

# Describe: sum score level
epicur_sum <- c("ICuriositySum", "DCuriositySum")
par(mfrow = c(1, 2))
for (i in 1:length(epicur_sum)) {
  hist(data_Q_total[, epicur_sum[i]],
       main = colnames(data_Q_total[epicur_sum[i]]),
       col = sample(colors(), 1),
       xlab = ""
       )
}
describe(data_Q_total[, c(epicur_sum)])

```

### Intellectual Humility

```{r Intellectual Humility, include = FALSE}

# Intellectual Humility
IH <- c(paste0("IH", 1:5, "rev1"), paste0("IH", 6:10, "norm2"), 
        paste0("IH", 11:16, "norm3"), paste0("IH", 17:22, "rev4"))
IH_rev <- c(paste0("IH", 1:5, "rev1"), paste0("IH", 17:22, "rev4"))

# Exclusion criteria
  # >=90% of responses on the same option
  # <=22000ms response time 
data_Q_total <- data_Q_total %>% 
  mutate(IH_max = max(table(c(IH1rev1, IH2rev1, IH3rev1, IH4rev1, IH5rev1, IH6norm2, IH7norm2, IH8norm2, IH9norm2, IH10norm2, IH11norm3, IH12norm3, IH13norm3, IH14norm3, IH15norm3, IH16norm3, IH17rev4, IH18rev4, IH19rev4, IH20rev4, IH21rev4, IH22rev4)), na.rm = TRUE) /
           sum(table(c(IH1rev1, IH2rev1, IH3rev1, IH4rev1, IH5rev1, IH6norm2, IH7norm2, IH8norm2, IH9norm2, IH10norm2, IH11norm3, IH12norm3, IH13norm3, IH14norm3, IH15norm3, IH16norm3, IH17rev4, IH18rev4, IH19rev4, IH20rev4, IH21rev4, IH22rev4)), na.rm = TRUE))

data_Q_total <- data_Q_total %>% 
  mutate(IH_omit = case_when(IH_max >= .90 | END.QUESTIONNAIRE.INT.HUM <= 1000*length(IH) ~ 1,
                             IH_max < .90 & END.QUESTIONNAIRE.INT.HUM > 1000*length(IH) ~ 0))
 
# Omit IH data based on the IH_omit column
data_Q_total[which(data_Q_total$IH_omit == 1), IH] <- NA

# Reverse-code
for (i in 1:length(IH_rev)) {
  data_Q_total[, IH_rev[i]] <- reverse.code(keys = c(-1), 
                                            items = data_Q_total[, IH_rev[i]],
                                            mini = c(1), 
                                            maxi = c(4))
}

# Describe: item-level
par(mfrow = c(2, 3))
for (i in 1:length(IH)) {
  hist(data_Q_total[, IH[i]],
       main = colnames(data_Q_total[IH[i]]),
       col = sample(colors(), 1),
       breaks = seq(0, 5, 1),
       xlab = ""
       )
}
describe(data_Q_total[, IH])

# Cronbach's alphas
psych::alpha(data_Q_total[, IH[1:5]])
psych::alpha(data_Q_total[, IH[6:10]])
psych::alpha(data_Q_total[, IH[11:16]])
psych::alpha(data_Q_total[, IH[17:22]])

# Intellectual Humility Scoring
# https://seaver.pepperdine.edu/social-science/content/comprehensive-intellectual-humility.pdf -> sum scores

# Calculate sum scores
data_Q_total$IH1Sum <- rowSums(cbind(data_Q_total$IH1rev1, data_Q_total$IH2rev1, data_Q_total$IH3rev1, data_Q_total$IH4rev1, data_Q_total$IH5rev1), na.rm = TRUE)
data_Q_total$IH2Sum <- rowSums(cbind(data_Q_total$IH6norm2, data_Q_total$IH7norm2, data_Q_total$IH8norm2, data_Q_total$IH9norm2, data_Q_total$IH10norm2), na.rm = TRUE)
data_Q_total$IH3Sum <- rowSums(cbind(data_Q_total$IH11norm3, data_Q_total$IH12norm3, data_Q_total$IH13norm3, data_Q_total$IH14norm3, data_Q_total$IH15norm3), na.rm = TRUE)
data_Q_total$IH4Sum <- rowSums(cbind(data_Q_total$IH17rev4, data_Q_total$IH18rev4, data_Q_total$IH19rev4, data_Q_total$IH20rev4, data_Q_total$IH21rev4), na.rm = TRUE)

# Replace sum score 0s with NAs
data_Q_total$IH1Sum[which(data_Q_total$IH1Sum == 0)] <- NA
data_Q_total$IH2Sum[which(data_Q_total$IH2Sum == 0)] <- NA
data_Q_total$IH3Sum[which(data_Q_total$IH3Sum == 0)] <- NA
data_Q_total$IH4Sum[which(data_Q_total$IH4Sum == 0)] <- NA

# Describe: sum score level
IH_sum <- c("IH1Sum", "IH2Sum", "IH3Sum", "IH4Sum")
par(mfrow = c(2, 2))
for (i in 1:length(IH_sum)) {
  hist(data_Q_total[, IH_sum[i]],
       main = colnames(data_Q_total[IH_sum[i]]),
       col = sample(colors(), 1),
       xlab = ""
       )
}
describe(data_Q_total[, IH_sum])

```

### Need for closure

```{r Need for Closure, include = FALSE}

# Need for Closure
need_clo <- paste0("closure", 1:15)

# Exclusion criteria
  # <=15000ms response time
data_Q_total[which(data_Q_total$END.QUESTIONNAIRE.NEED.CLO <= 1000*length(need_clo)), need_clo] <- NA
# NOTE: case_when(Participant.Private.ID == "ppt ID that needs to be rejected for this task")

# Describe: item-level
par(mfrow = c(3, 5))
for (i in 1:length(need_clo)) {
  hist(data_Q_total[, need_clo[i]],
       main = colnames(data_Q_total[need_clo[i]]),
       breaks = seq(0, 6, 1),
       col = sample(colors(), 1),
       xlab = ""
       )
}
describe(data_Q_total[, need_clo])

# Cronbach's alphas
psych::alpha(data_Q_total[, need_clo])

# Need for Closure Scoring
# https://www.midss.org/sites/default/files/need_for_closure_scale.pdf
# Not entirely sure if this is the correct file but it says to sum so I'll do that here

# Calculate sum score
data_Q_total$CloSum <- rowSums(cbind(data_Q_total$closure1, data_Q_total$closure2, data_Q_total$closure3, data_Q_total$closure4, data_Q_total$closure5, data_Q_total$closure6, data_Q_total$closure7, data_Q_total$closure8, data_Q_total$closure9, data_Q_total$closure10, data_Q_total$closure11, data_Q_total$closure12, data_Q_total$closure13, data_Q_total$closure14, data_Q_total$closure15), na.rm = TRUE)

# Replace sum score 0s with NAs
data_Q_total$CloSum[which(data_Q_total$CloSum == 0)] <- NA

# Describe: sum score level
describe(data_Q_total$CloSum)
hist(data_Q_total$CloSum, 
     main = "Need for Closure", 
     col = rainbow(14), 
     ylim = c(0, 12), 
     breaks = seq(0, 80, 1), 
     xlab = "")

```

### Need for cognition

```{r Need for Cognition, include = FALSE}

# Need for Cognition
need_cog <- c(paste0("cognition", c(1:2, 6, 10:11, 13:15, 18)), 
              paste0("cognition", c(3:5, 7:9, 12, 16:17), ".rev"))
need_cog_rev <- paste0("cognition", c(3:5, 7:9, 12, 16:17), ".rev")

# Exclusion criteria
  # >=90% of responses fall on the same response option
  # <=18000ms response time
data_Q_total <- data_Q_total %>% 
  mutate(Cog_max = max(table(c(cognition1, cognition2, cognition3.rev, cognition4.rev, cognition5.rev, cognition6, cognition7.rev, cognition8.rev, cognition9.rev, cognition10, cognition11, cognition12.rev, cognition13, cognition14, cognition15, cognition16.rev, cognition17.rev, cognition18)), na.rm = TRUE) /
           sum(table(c(cognition1, cognition2, cognition3.rev, cognition4.rev, cognition5.rev, cognition6, cognition7.rev, cognition8.rev, cognition9.rev, cognition10, cognition11, cognition12.rev, cognition13, cognition14, cognition15, cognition16.rev, cognition17.rev, cognition18)), na.rm = TRUE))

data_Q_total <- data_Q_total %>% 
  mutate(Cog_omit = case_when(Cog_max >= .90 | END.QUESTIONNAIRE.NEED.COG <= 1000*length(need_cog) ~ 1,
                             Cog_max < .90 & END.QUESTIONNAIRE.NEED.COG > 1000*length(need_cog) ~ 0))
 
# Omit Cog data based on the Cog_omit column
data_Q_total[which(data_Q_total$Cog_omit == 1), need_cog] <- NA

# Reverse-code
for (i in 1:length(need_cog_rev)) {
  data_Q_total[, need_cog_rev[i]] <- reverse.code(keys = c(-1),
                                                  items = data_Q_total[, need_cog_rev[i]],
                                                  mini = c(1),
                                                  maxi = c(6))
}

# Describe: item-level
par(mfrow = c(3, 6))
for (i in 1:length(need_cog)) {
  hist(data_Q_total[, need_cog[i]],
       main = colnames(data_Q_total[need_cog[i]]),
       breaks = seq(0, 6, 1),
       col = sample(colors(), 1),
       xlab = ""
       )
}
describe(data_Q_total[, need_cog])

# Cronbach's alphas
psych::alpha(data_Q_total[, need_cog])

# Need for Cognition Scoring
# https://centerofinquiry.org/uncategorized/need-for-cognition-scale-wabash-national-study/
# According to this source you should calculate the sum score for this scale

# Calculate sum score
data_Q_total$CogSum <- rowSums(cbind(data_Q_total$cognition1, data_Q_total$cognition2, data_Q_total$cognition3.rev, data_Q_total$cognition4.rev, data_Q_total$cognition5.rev, data_Q_total$cognition6, data_Q_total$cognition7.rev, data_Q_total$cognition8.rev, data_Q_total$cognition9.rev, data_Q_total$cognition10, data_Q_total$cognition11, data_Q_total$cognition12.rev, data_Q_total$cognition13, data_Q_total$cognition14, data_Q_total$cognition15, data_Q_total$cognition16.rev, data_Q_total$cognition17.rev, data_Q_total$cognition18), na.rm = TRUE)

# Replace sum score 0s with NAs
data_Q_total$CogSum[which(data_Q_total$CogSum == 0)] <- NA

# Describe: sum-score level
describe(data_Q_total$CogSum)
hist(data_Q_total$CogSum,
     main = "Need for Cognition", 
     col = rainbow(14), 
     xlab = "")

```

### Actively openminded thinking

```{r Actively Openminded Thinking, include = FALSE}

# Actively Openminded Thinking
aot <- c(paste0("openReverse", 1:4), paste0("openNormal", 1:3))
aot_rev <- paste0("openReverse", 1:4)

# Exclusion criteria
  # >=90% of responses on the same response option
  # <=7000ms response time
data_Q_total <- data_Q_total %>% 
  mutate(AOT_max = max(table(c(openReverse1, openReverse2, openReverse3, openReverse4, openNormal1, openNormal2, openNormal3)), na.rm = TRUE) /
           sum(table(c(openReverse1, openReverse2, openReverse3, openReverse4, openNormal1, openNormal2, openNormal3)), na.rm = TRUE))

data_Q_total <- data_Q_total %>% 
  mutate(AOT_omit = case_when(AOT_max >= .90 | END.QUESTIONNAIRE.OPEN.THINK <= 1000*length(aot) ~ 1, 
                             AOT_max < .90 & END.QUESTIONNAIRE.OPEN.THINK > 1000*length(aot) ~ 0))
 
# Omit AOT data based on the AOT_omit column
data_Q_total[which(data_Q_total$AOT_omit == 1), aot] <- NA

# Reverse-code
for (i in 1:length(aot_rev)) {
  data_Q_total[, aot_rev[i]] <- reverse.code(keys = c(-1), 
                                             items = data_Q_total[, aot_rev[i]], 
                                             mini = c(1), 
                                             maxi = c(7))
}

# Describe: item-level
par(mfrow = c(2, 4))
for (i in 1:length(aot)) {
  hist(data_Q_total[, aot[i]], 
       main = colnames(data_Q_total[aot[i]]), 
       col = sample(colors(), 1), 
       breaks = seq(0, 7, 1), 
       xlab = "")
}
describe(data_Q_total[, aot])

# Cronbach's alpha
psych::alpha(data_Q_total[, aot])

# Actively Openminded Thinking Scoring
# https://www.sciencedirect.com/science/article/pii/S1871187119303700
# Again, this article says to do a sum score

# Calculate sum score
data_Q_total$AOTSum <- rowSums(cbind(data_Q_total$openNormal1, data_Q_total$openNormal2, data_Q_total$openNormal3, data_Q_total$openReverse1, data_Q_total$openReverse2, data_Q_total$openReverse3, data_Q_total$openReverse4), na.rm = TRUE)

# Replace sum score 0s with NAs
data_Q_total$AOTSum[which(data_Q_total$AOTSum == 0)] <- NA

# Describe: sum-score level
describe(data_Q_total$AOTSum)
hist(data_Q_total$AOTSum,
     main = "Actively Openminded Thinking", 
     col = rainbow(14), 
     xlab = "")

```

### Science Attitudes Questionnaire

This section has 12 questions with Likert responses that represent a (possible) 3 factor model based on the sources of the questions, from an early version of the Science Capital Scale from the FINSCI population survey study. The first 8 questions originally come from Archer, 2015, and the last 4 are modified from the Trust in Science and Scientists Scale (Nadelson et al., 2014). 

```{r Science Attitudes, include = FALSE}

# Science Attitudes
sci_att <- c(paste0("sa.1", letters[1:8]), paste0("sa.2", letters[1:4]))
sci_att_rev <- c("sa.1b", "sa.1d", "sa.2a", "sa.2b", "sa.2d")

# Exclusion criteria
  # >=90% of responses fall on the same response option
  # <=15000ms response time
data_Q_total <- data_Q_total %>% 
  mutate(sciatt_max = max(table(c(sa.1a, sa.1b, sa.1c, sa.1d, sa.1e, sa.1f, sa.1g, sa.1h, sa.2a, sa.2b, sa.2c, sa.2d)), na.rm = TRUE) /
           sum(table(c(sa.1a, sa.1b, sa.1c, sa.1d, sa.1e, sa.1f, sa.1g, sa.1h, sa.2a, sa.2b, sa.2c, sa.2d)), na.rm = TRUE))

data_Q_total <- data_Q_total %>% 
  mutate(sciatt_omit = case_when(sciatt_max >= .90 | END.QUESTIONNAIRE.SCIENCE.ATT <= 1000*length(sci_att) ~ 1, 
                             sciatt_max < .90 & END.QUESTIONNAIRE.SCIENCE.ATT> 1000*length(sci_att) ~ 0))
 
# Omit science attitude data based on sciatt_omit column
data_Q_total[which(data_Q_total$sciatt_omit == 1), sci_att] <- NA

# Reverse-code
for (i in 1:length(sci_att_rev)) {
  data_Q_total[, sci_att_rev[i]] <- reverse.code(keys = c(-1), 
                                             items = data_Q_total[, sci_att_rev[i]], 
                                             mini = c(1), 
                                             maxi = c(5))
}

# Describe: item-level
par(mfrow = c(2, 2))
for (i in 1:length(sci_att)) {
  hist(data_Q_total[, sci_att[i]], 
       main = colnames(data_Q_total[sci_att[i]]), 
       col = sample(colors(), 1), 
       breaks = seq(0, 7, 1), 
       xlab = "")
}
describe(data_Q_total[, sci_att])

# Cronbach's alphas
psych::alpha(data_Q_total[, sci_att])

# Calculate sum score
data_Q_total$sci_id <- rowSums(cbind(data_Q_total$sa.1a, data_Q_total$sa.1b.rev, data_Q_total$sa.1d.rev, data_Q_total$sa.1g), na.rm = TRUE)
data_Q_total$sci_impo <- rowSums(cbind(data_Q_total$sa.1c, data_Q_total$sa.1e, data_Q_total$sa.1h, data_Q_total$sa.1f), na.rm = TRUE)
data_Q_total$sci_tru <- rowSums(cbind(data_Q_total$sa.2a.rev, data_Q_total$sa.2b.rev, data_Q_total$sa.2c, data_Q_total$sa.2d.rev), na.rm = TRUE)

# Replace total score zeroes with NA
data_Q_total$sci_id[which(data_Q_total$sci_id == 0)] <- NA
data_Q_total$sci_impo[which(data_Q_total$sci_impo == 0)] <- NA
data_Q_total$sci_tru[which(data_Q_total$sci_tru == 0)] <- NA

# Describe: sum-score level
sci_att_sum <- c("sci_id", "sci_impo", "sci_tru")
describe(data_Q_total[, sci_att_sum])
par(mfrow = c(3, 1))
for (i in 1:length(sci_att_sum)) {
  hist(data_Q_total[, sci_att_sum[i]], 
       main = colnames(data_Q_total[sci_att_sum[i]]), 
       col = sample(colors(), 1), 
       xlab = "")
}

```

### Science Curiosity

This is a 4-item scale, scored as a single sum score from the quantised responses. No reverse scoring. Modified from Landrum et al., 2016 and Motta et al., 2019

```{r Science Curiosity, include = FALSE}

# Science Curiosity 
sci_cur <- paste0("sc.", 1:4, ".quantised")

# Exclusion criteria
  # >=4000ms response time
data_Q_total <- data_Q_total %>%
 mutate(Sci.cur_omit = case_when(END.QUESTIONNAIRE.SCIENCE.CUR <= 4000 ~ 1, 
                 END.QUESTIONNAIRE.SCIENCE.CUR > 4000 ~ 0))

# Omit SC data based on the Sci.cur_omit column
data_Q_total[which(data_Q_total$Sci.cur_omit == 1), sci_cur] <- NA

# Describe: item-level
par(mfrow = c(2, 2))
for (i in 1:length(sci_cur)) {
  hist(data_Q_total[, sci_cur[i]], 
       main = colnames(data_Q_total[sci_cur[i]]), 
       col = sample(colors(), 1), 
       breaks = seq(0, 7, 1), 
       xlab = "")
}
describe(data_Q_total[, sci_cur])

# Cronbach's alpha
psych::alpha(data_Q_total[, sci_cur])

# Calculate sum score
data_Q_total$sci_cur <- rowSums(cbind(data_Q_total$sc.1.quantised, data_Q_total$sc.2.quantised, data_Q_total$sc.3.quantised, data_Q_total$sc.4.quantised), na.rm = TRUE)

# Replace sum score 0s with NAs
data_Q_total$sci_cur[which(data_Q_total$sci_cur == 0)] <- NA

# Describe: sum-score level
describe(data_Q_total$sci_cur)
hist(data_Q_total$sci_cur, 
     col = sample(colors(), 1), 
     xlab = "")

```

### Heuristic Reasoning

```{r Heuristic Reasoning, include = FALSE}

# Exclusion criteria
  # <=6000ms response time
data_Q_total <- data_Q_total %>%
 mutate(HR_omit = case_when(END.QUESTIONNAIRE.HEURISTIC <= 6000 ~ 1, 
                            END.QUESTIONNAIRE.HEURISTIC > 6000 ~ 0))

# Q1
data_Q_total$hr.1 <- as.factor(data_Q_total$hr.1)
levels(data_Q_total$hr.1) <- list("N" = "Kumpikin on yhtÃ¤ todennÃ¤kÃ¶istÃ¤", "H-R" = "Kuudes lapsi on tyttÃ¶", "Neither" = "Kuudes lapsi on poika")

# Q2
data_Q_total$hr.2 <- as.factor(data_Q_total$hr.2)
levels(data_Q_total$hr.2) <- list("N" = "Kummatkin sarjat ovat yhtÃ¤ todennÃ¤kÃ¶isiÃ¤", "H-R" = "THHTHT", "Neither" = "HTHTHT")

# Q3
# 3.1
data_Q_total$hr.3.1 <- as.factor(data_Q_total$hr.3.1)
levels(data_Q_total$hr.3.1) <- list("b" = "Herra F. sai sydÃ¤nkohtauksen", "a" = "Herra F. on yli 55-vuotias ja on saanut sydÃ¤nkohtauksen", "c" = "Herra F.:llÃ¤ on suuri perhe")
# 3.2
data_Q_total$hr.3.2 <- as.factor(data_Q_total$hr.3.2)
levels(data_Q_total$hr.3.2) <- list("b" = "Herra F. sai sydÃ¤nkohtauksen", "a" = "Herra F. on yli 55-vuotias ja on saanut sydÃ¤nkohtauksen", "c" = "Herra F.:llÃ¤ on suuri perhe")
# 3.3
data_Q_total$hr.3.3 <- as.factor(data_Q_total$hr.3.3)
levels(data_Q_total$hr.3.3) <- list("b" = "Herra F. sai sydÃ¤nkohtauksen", "a" = "Herra F. on yli 55-vuotias ja on saanut sydÃ¤nkohtauksen", "c" = "Herra F.:llÃ¤ on suuri perhe")
# Create the variables for the combinations of answers
# abc / acb / cab = N; bac / bca /cba = H-R
data_Q_total <- data_Q_total %>%
  mutate(hr.3.updated = case_when(
  ((hr.3.1 == "b" & (hr.3.2 == "a"|hr.3.3 == "a")) | hr.3.2 == "b" & hr.3.3 == "a") ~ "H-R", 
  ((hr.3.1 == "a" & (hr.3.2 == "b"|hr.3.3 == "b")) | (hr.3.2 == "a" & hr.3.3 == "b")) ~ "N"
 ))
data_Q_total$hr.3.updated <- factor(data_Q_total$hr.3.updated)

# Q4
data_Q_total$hr.4 <- as.factor(data_Q_total$hr.4)
levels(data_Q_total$hr.4) <- list("H-E" = "Kumpikin on yhtÃ¤ tehokas", "N" = "Kognitiivis-behavioraalista terapiaa", "Neither" = "LÃ¤Ã¤kehoitoa")
# Kognitiivis-behavioraalista terapiaa = N; Yhta tehokas = H-E

# Q5
data_Q_total$hr.5 <- as.factor(data_Q_total$hr.5)
levels(data_Q_total$hr.5) <- list("N" = "Huomenna luultavasti sataa", "H-E" = "On mahdotonta sanoa sataako huomenna vai ei", "Neither" = "Huomenna sataa")
# Huomenna luultavasti sataa = N; On mahdotonta sanoa sataako huomenna vai ei = H-E

# Q6
data_Q_total$hr.6 <- as.factor(data_Q_total$hr.6)
levels(data_Q_total$hr.6) <- list("N" = "TyttÃ¶", "H-E" = "Kumpikin on yhtÃ¤ todennÃ¤kÃ¶istÃ¤", "Neither" = "Poika")
# Tytto = N; Kumpikin on yht? todenn?k?ist? = H-E

# Calculate sums
data_Q_total <- data_Q_total %>%
  mutate(
    total_N = apply(., 1, function(x) length(which(x == "N"))), 
    total_HR = apply(., 1, function(x) length(which(x == "H-R"))), 
    total_HE = apply(., 1, function(x) length(which(x == "H-E"))), 
    total_Neither = apply(., 1, function(x) length(which(x == "Neither")))
 )

# Omit heuristic reasoning data based on the HR_omit column
heur <- c("total_N", "total_HR", "total_HE", "total_Neither")
data_Q_total[which(data_Q_total$HR_omit == 1), heur] <- NA

# Omit participants with 0s in all the sum columns
data_Q_total[which(data_Q_total$total_N == 0 & data_Q_total$total_HR == 0 & data_Q_total$total_HE == 0 & data_Q_total$total_Neither == 0), heur] <- NA

# Describe: sum score level
describe(data_Q_total[, heur])
par(mfrow = c(2, 2))
for (i in 1:length(heur)) {
  hist(data_Q_total[, heur[i]], 
       main = colnames(data_Q_total[heur[i]]), 
       col = sample(colors(), 1), 
       xlab = "")
}

# Create binary heuristic scores (0=0, 1=everything else)
data_Q_total <- data_Q_total %>% 
  mutate(HEscore = case_when(total_HE == 0 ~ 0,
                             total_HE != 0 ~ 1),
         HRscore = case_when(total_HR == 0 ~ 0,
                             total_HR != 0 ~ 1))

# Add a measure of total heuristic response, where 1 = at least one mistake in one heuristic category, 2 = at least 1 heuristic mistake in both heuristic categories, 0 = all normative responses
data_Q_total <- data_Q_total %>%
  group_by(Participant.Private.ID) %>% 
  mutate(HEHRscore = HEscore + HRscore)

```

### Randomness and Probability

New columns are created for each item, specifying whether the participant got the question correct or not (0=incorrect, 1=correct). A sum score is calculated for all the items.

```{r Randomness and Probability, include = FALSE}

# Exclusion criteria
  # <=7000ms reaction time
data_Q_total <- data_Q_total %>%
  mutate(RP_omit = case_when(END.QUESTIONNAIRE.RANDOM.PROB <= 4000 ~ 1, 
                             END.QUESTIONNAIRE.RANDOM.PROB > 4000 ~ 0))

# Q1: Sairaala B is the correct answer
# Create a new column
data_Q_total$rp.1 <- as.factor(data_Q_total$rp.1)
data_Q_total <- data_Q_total %>%
  mutate(rp.1.int = case_when(rp.1 == "Sairaalassa B (jossa syntyy 10 lasta pÃ¤ivÃ¤ssÃ¤)." ~ 1, 
                              rp.1 != "Sairaalassa B (jossa syntyy 10 lasta pÃ¤ivÃ¤ssÃ¤)." ~ 0)) 

# Q2: Pyydys1 AND Pyydys 2 is the correct combination
data_Q_total$rp.2.1 <- as.factor(data_Q_total$rp.2.1)
data_Q_total$rp.2.2 <- as.factor(data_Q_total$rp.2.2)
data_Q_total$rp.2.3 <- as.factor(data_Q_total$rp.2.3)
data_Q_total$rp.2.4 <- as.factor(data_Q_total$rp.2.4)
data_Q_total$rp.2.5 <- as.factor(data_Q_total$rp.2.5)
data_Q_total$rp.2.6 <- as.factor(data_Q_total$rp.2.6)
data_Q_total$rp.2.7 <- as.factor(data_Q_total$rp.2.7)
data_Q_total$rp.2.8 <- as.factor(data_Q_total$rp.2.8)
# Create a new column
data_Q_total <- data_Q_total %>%
  mutate(rp.2.int = case_when((rp.2.1 == "Pyydys 1" & rp.2.2 == "Pyydys 2" & rp.2.3 == "" & rp.2.4 == "" & rp.2.5 == "" & rp.2.6 == "" & rp.2.7 == "" & rp.2.8 == "") ~ 1)) %>% 
  mutate(rp.2.int = case_when(rp.2.int == "1" ~ 1,
                              is.na(rp.2.int) ~ 0))

# Q3: Kanava 1; 2 tai 3 is the correct answer
# Create a new column
data_Q_total <- data_Q_total %>%
  mutate(rp.3.int = case_when(rp.3.quantised == "4" ~ 1, 
                              rp.3.quantised != "4" ~ 0))

# Q4: Ruudukot A; B ja C is the correct answer
# Create a new column
data_Q_total <- data_Q_total %>%
  mutate(rp.4.int = case_when(rp.4.quantised == "5" ~ 1, 
                              rp.4.quantised != "5" ~ 0))

# Calculate the sum score for the randomness/probability questions
data_Q_total <- data_Q_total %>%
  mutate(RPSum = rp.1.int + rp.2.int + rp.3.int + rp.4.int)

# Omit Randomness-Probability data based on the RP_omit column
data_Q_total$RPSum[which(data_Q_total$RP_omit == 1)] <- NA
data_Q_total$RPSum[which(is.na(data_Q_total$RP_omit))] <- NA 

# Describe
describe(data_Q_total$RPSum)
hist(data_Q_total$RPSum,
     main = "RPSum",
     col = sample(colors(), 1),
     xlab = "")

```

### Matrix Reasoning

For the matrix reasoning task (treated as a questionnaire by Gorilla so included in this section), a participant's data for that task is replaced with missing values (NAs) if they met any of the following exclusion criteria:
* medium reaction time equal to or smaller than 500ms
* stated they had technical problems or did not understand the task

```{r Matrix Reasoning, include = FALSE}

# Omit columns that are not relevant (experiment-general columns) + rows that are not relevant (e.g., practice trials)
data_matreasT <- data_matreasT %>%
  select(Participant.Private.ID, Spreadsheet:ANSWER) %>% 
  filter(display %in% c("TehtÃ¤vÃ¤_6", "TehtÃ¤vÃ¤_8"))

# Omit participants who commented that they had technical issues with this task or did not understand the task
data_matreasT <- data_matreasT %>%
  filter(Participant.Private.ID != "5555882" & Participant.Private.ID != "5608075" & Participant.Private.ID != "4887607")

# Exclusion Criteria
  # median reaction time <= 500ms
data_matreasT <- data_matreasT %>%
  group_by(Participant.Private.ID) %>%
  mutate(medianRT = median(Reaction.Time, na.rm = TRUE)) %>%
  mutate(Matrix_omit = case_when((medianRT > 500) ~ 0, 
                                 (medianRT <= 500) ~ 1))
# Create a separate df for subsetting
data_matrix_omit <- data_matreasT %>%
  group_by(Participant.Private.ID) %>%
  summarise(Matrix_omit = mean(Matrix_omit)) %>%
  select(Participant.Private.ID, Matrix_omit) 
# Extract the relevant information
data_matreasT <- data_matreasT %>%
  group_by(Participant.Private.ID) %>%
  summarise(MatrixCorrectCount = sum(Correct, na.rm = TRUE)) %>%
  select(Participant.Private.ID, MatrixCorrectCount)
# Omit data based on matrix_omit
data_matreasT_final <- merge(data_matreasT, data_matrix_omit, 
                             by = "Participant.Private.ID", 
                             all = TRUE)
data_matreasT_final[which(data_matreasT_final$Matrix_omit == 1), "MatrixCorrectCount"] <- NA
data_matreasT_final <- data_matreasT_final %>%
  select(Participant.Private.ID, MatrixCorrectCount)

# Exclude participants with invalid responses
data_ValidityFlag <- data_validQ %>%
  select(Participant.Private.ID, Validity.quantised)
data_Language <- data_demoQ %>%
  select(Participant.Private.ID, Language)
data_matrixValidity <- merge(data_Language, data_ValidityFlag, 
                             by = "Participant.Private.ID", 
                             all = TRUE)
data_matreasT_final <- merge(data_matreasT_final, data_matrixValidity, 
                             by = "Participant.Private.ID", 
                             all = TRUE)

# Create separate df for nonvalid ppts
data_nonvalid_matrix <- data_matreasT_final %>%
  filter(Validity.quantised %in% c(2, 3, 4))
data_Q_lang_omit_matrix <- data_matreasT_final %>%
  filter(Language != "Sujuva / Ã¤idinkieli" & Language != "Keskitaso / keskusteleva" & Language != "Muu, mikÃ¤?")
data_matreasT_final <- data_matreasT_final %>%
  filter(Validity.quantised %in% c("1", NA, "") & Language %in% c("Sujuva / Ã¤idinkieli", "Keskitaso / keskusteleva", "Muu, mikÃ¤?"))

# Delete the irrelevant columns
data_matreasT_final <- data_matreasT_final %>%
  select(Participant.Private.ID, MatrixCorrectCount)

# Describe
describe(data_matreasT_final$MatrixCorrectCount)
hist(data_matreasT_final$MatrixCorrectCount,
     main = "Matrix Score", 
     col = rainbow(14), 
     xlab = "")

```

## Merge Questionnaire Variables

```{r Merge questionnaire variables, include = FALSE}

# Merge the important columns
data_Q_sub <- data_Q_total %>%
  select(Participant.Private.ID, gender, Age, Country, Language, Education, AMean, CMean, EMean, ESMean, OMean, ICuriositySum, DCuriositySum, IH1Sum, IH2Sum, IH3Sum, IH4Sum, CloSum, CogSum, AOTSum, HEscore, HRscore, RPSum, sci_cur, sci_tru, sci_impo, sci_id)

# Add matrix reasoning data to the dataframe
data_Q_sub <- merge(data_Q_sub, data_matreasT_final, 
                    by = "Participant.Private.ID", 
                    all = TRUE)

# Get rid of NaNs or string NAs introduced by calculations
is.nan.data.frame <- function(x)
  do.call(cbind, lapply(x, is.nan))
data_Q_sub[is.nan(data_Q_sub)] <- NA

# Numeric variables into numeric
data_Q_sub$MatrixCorrectCount <- as.numeric(data_Q_sub$MatrixCorrectCount)
data_Q_sub$Age <- as.numeric(data_Q_sub$Age)

```

## Analyze Missing Data in Questionnaires

Participants with large amounts of missing data or suspicious patterns of missing data are removed here. It should be noted that missing data refers to data that was missing from the start *and* data that was replaced with missing values if the participant met one of the exclusion criteria for (a) questionnaire(s).

```{r Analyze Missing Data in Questionnaires, include = FALSE}

# Add a column for count of missing data per ppt
data_Q_sub <- data_Q_sub %>%
  mutate(missingdata = rowSums(is.na(data_Q_sub)))

# Plot number of missing values
ggplot(data_Q_sub, aes(x = as.factor(Participant.Private.ID), y = missingdata)) + 
   geom_boxplot(fill = "slateblue", alpha = 0.2) + 
   xlab("Missing variables count")

# Plot to look for an elbow value
MissingData <- sort(data_Q_sub$missingdata, decreasing = FALSE)
plot(MissingData)

# Correlation between age and amount of missing values
gg_miss_fct(x = data_Q_sub, fct = Age)
cor.test(data_Q_sub$Age, data_Q_sub$missingdata, method = "spearman", exact = FALSE)

# Visualize missing data by demographics
gg_miss_fct(x = data_Q_sub, fct = Language)
gg_miss_fct(x = data_Q_sub, fct = Country)
gg_miss_fct(x = data_Q_sub, fct = gender)
gg_miss_fct(x = data_Q_sub, fct = Education)

# Remove ppts with 16 or fewer variables missing out of 22 (30%)
data_Q_sub <- data_Q_sub %>%
  filter(missingdata <= 16)

```

## Check Questionnaire Distributions and Correlations

This chunk checks distributions of questionnaire variables and correlations between them:
* If distributions are highly skewed, transformations are applied to decrease skewness
* If two variables are highly correlated, one of them is dropped (Eisenberg et al.)

```{r Normality and Correlations of Questionnaires, include = FALSE}

# Normality of variables: visualize with histograms
vars <- c("AMean", "CMean", "EMean", "OMean", "ESMean", "ICuriositySum", "DCuriositySum", "IH1Sum", "IH2Sum", "IH3Sum", "IH4Sum", "CloSum", "CogSum", "AOTSum", "RPSum", "sci_cur", "sci_tru", "sci_impo", "sci_id", "MatrixCorrectCount")
par(mfrow = c(2, 3))
for (i in 1:length(vars)) {
  hist(data_Q_sub[, vars[i]],
       main = colnames(data_Q_sub[vars[i]]),
       col = sample(colors(), 1),
       xlab = "")
}

# Describe: mean, skew, kurtosis
describe(data_Q_sub[, vars])[c("mean", "skew", "kurtosis")]

# Correlation matrix
round(cor(data_Q_sub[, vars], method = "pearson", 
          use = "pairwise.complete.obs"), 2)
abs(round(cor(data_Q_sub[, vars], method = "pearson", 
              use = "pairwise.complete.obs"), 2)) > .85
corrplot(cor(data_Q_sub[, vars], method = "pearson", 
             use = "pairwise.complete.obs"))

# Subset data: exclude demographic variables
data_Q_sub <- data_Q_sub %>%
 select(Participant.Private.ID, AMean:MatrixCorrectCount)

``` 

# Task Data

## Exclusion Criteria

**ALL QUESTIONNAIRES AND TASKS:** This chunk goes through all task data, removing **all data for a participant** if they met one or more of the following exclusion criteria:
* stated their data are not valid
* stated their Finnish is not Äidinkieli or Keskusteleva

**TASKS**: A participant's data for a questionnaire was replaced with missing values (NAs) if they met the following exclusion criterion:
* stopped during that task

**For further exclusion criteria for the tasks, please see each chunk.**

## Task Data Processing

See each chunk.

### Go No Go

Trial-level exclusion criteria:
* The reaction time for each row is replaced with missing values (NAs) if it is equal to or higher than 1025ms (screen time 1000ms + 16.67ms refresh rate + ~8ms latency between computer and mouse -> reaction times higher than this are likely not valid) *or* if it is equal to or lower than 150ms (a human needs time to react)

Participant-level exclusion criteria:
* A participant's data for the Go No-Go task is replaced with missing values (NAs) if their median reaction time was equal to or lower than 200ms *or* if their accuracy (% of correct trials out of all trials) was equal to or lower than 60%.

Data processing: for each participant, we calculate the sum of hits/misses/FAs/CRs as well as dprime, beta, c, aprime, and bppd (see the dprime() function help file). These are described and visualized.

```{r Go No-Go, include = FALSE}

# Omit experiment-general columns & practice (etc.) rows
data_nogoT <- data_nogoT %>%
  select(Participant.Private.ID, Spreadsheet:Answer) %>%
  filter(display == "Trials")

# Exclusion Criteria: Trial-Level
# NOTE: Caitlin had used different ones but I think this might be better?
  # Reaction time upper boundary: In the Go No-Go task, the screens advance automatically after 1000ms (therefore, a no-go response should be recorded as 1000ms). According to Gorilla, there is also a 16.67ms refresh rate and ~8ms latency between a computer and a mouse. This means that there might be no-go responses up to ~1025ms. This is used as a threshold.
  # Reaction time lower boundary: the brain needs time to react (Jaana's 2017 paper) -> 150ms
data_nogoT$Reaction.Time[which(data_nogoT$Reaction.Time >= 1025 | data_nogoT$Reaction.Time <= 150)] <- NA

# Exclusion Criteria: Participant-Level
  # <=200ms median RT
  # <=60% accuracy
data_nogoT <- data_nogoT %>%
  group_by(Participant.Private.ID) %>% 
  mutate(medianRT = median(Reaction.Time, na.rm = TRUE), 
         Accuracy = sum(Correct, na.rm = TRUE) / (sum(Correct, na.rm = TRUE) + sum(Incorrect, na.rm = TRUE))) %>% 
  mutate(go.nogo_omit = case_when((medianRT > 200 & Accuracy > .60) ~ 0, 
                                  (medianRT <= 200 | Accuracy <= .60) ~ 1)) 
 
# Omit rows based on exclusion criteria
data_nogoT$Response[which(data_nogoT$go.nogo_omit == 1)] <- NA

# Create Signal Detection Theory categories for each row
data_nogoT <- data_nogoT %>%
  filter(display == "Trials") %>%
  mutate(SDT = case_when((Array %in% c("H.png",  "T.png") & Response == "Go") ~ "Hit", 
                         (Array %in% c("H.png", "T.png") & Response == "No Go") ~ "Miss", 
                         (Array == "N.png" & Response == "Go") ~ "False Alarm", 
                         (Array == "N.png" & Response == "No Go") ~ "Correct Rejection")) %>% 
    mutate(Hit = case_when(SDT == "Hit" ~ 1, 
                           SDT != "Hit" ~ 0), 
           Miss = case_when(SDT == "Miss" ~ 1, 
                            SDT != "Miss" ~ 0), 
           FA = case_when(SDT == "False Alarm" ~ 1, 
                          SDT != "False Alarm" ~ 0), 
           CR = case_when(SDT == "Correct Rejection" ~ 1, 
                          SDT != "Correct Rejection" ~ 0))

# Calculate the mean and SD of hits and FA (trials with a go response)
data_go_RT <- data_nogoT %>% 
  filter(SDT %in% c("Hit", "FA")) %>% 
  group_by(Participant.Private.ID) %>% 
  summarise(MeanGoRT = mean(Reaction.Time, na.rm = TRUE),
            SDGoRT = sd(Reaction.Time, na.rm = TRUE))
# NOTE: Caitlin had removed participants based on these but I think the distributions look ok?

# Compute D-Prime and Bias for each participant
# https://www.rdocumentation.org/packages/psycho/versions/0.6.1/topics/dprime
# http://wise.cgu.edu/wise-tutorials/tutorial-signal-detection-theory/signal-detection-d-defined-2/
data_dprime <- data_nogoT %>%
  group_by(Participant.Private.ID) %>%
  summarise(dp = dprime(n_hit = sum(Hit, na.rm = TRUE), 
                        n_fa = sum(FA, na.rm = TRUE), 
                        n_miss = sum(Miss, na.rm = TRUE), 
                        n_cr = sum(CR, na.rm = TRUE),
                        n_targets = 300, 
                        n_distractors = 100))

# Add a row to specify what the five different values given mean
data_dprime$value <- rep_len(c("GNGdprime", "GNGbeta", "GNGaprime", "GNGbppd", "GNGc"),
                             length.out = nrow(data_dprime))

# Into wide format
data_dprime <- data_dprime %>%
  pivot_wider(names_from = value, values_from = dp)
data_GNGdprime <- as.data.frame(data_dprime)

# Create a sum score of each response category for each participant
data_nogoT_final <- data_nogoT %>%
  select(Participant.Private.ID, Hit, Miss, FA, CR) %>%
  group_by(Participant.Private.ID) %>%
  summarise(GNGHitSum = sum(Hit, na.rm = TRUE), 
            GNGMissSum = sum(Miss, na.rm = TRUE), 
            GNGFASum = sum(FA, na.rm = TRUE), 
            GNGCRSum = sum(CR, na.rm = TRUE))

# Combine the dataframes (keep all rows)
data_nogoT_final <- merge(data_nogoT_final, data_GNGdprime, 
                          by = "Participant.Private.ID", all = TRUE)
data_nogoT_final <- merge(data_nogoT_final, data_go_RT, 
                          by = "Participant.Private.ID", all = TRUE)

# Exclude participants with invalid responses
data_ConsentValidity <- data_Q_total %>%
  select(Participant.Private.ID, Language, Validity.quantised)
 
# Final dataset
data_nogoT_final <- merge(data_nogoT_final, data_ConsentValidity,
                          by = "Participant.Private.ID")

# Exclude participants according to validity + language criteria; exclude the participants who stopped mid-task; exclude participants whose sum score for all hit/miss/cr/fa are 0s
data_nogoT_final <- data_nogoT_final %>% 
  filter(Validity.quantised %in% c(1, NA, "") & Language %in% c("Sujuva / Ã¤idinkieli", "Keskitaso / keskusteleva","Muu, mikÃ¤?")) %>% 
  filter(!Participant.Private.ID %in% c("5201242", "4886650", "5117494", "5282634", "5500754", "5552405", "5635997")) %>% 
  filter(!(GNGHitSum == 0 & GNGMissSum == 0 & GNGFASum == 0 & GNGCRSum == 0))

# Final set of columns
data_nogoT_final <- data_nogoT_final %>%
  select(Participant.Private.ID, GNGHitSum, GNGMissSum, GNGFASum, GNGCRSum, MeanGoRT, SDGoRT, GNGdprime, GNGbeta, GNGaprime, GNGbppd, GNGc) 

# Change columns into numeric
nogo <- c("GNGHitSum", "GNGMissSum", "GNGFASum", "GNGCRSum", "MeanGoRT", "SDGoRT", "GNGdprime", "GNGbeta", "GNGaprime", "GNGbppd", "GNGc")
for (i in 1:length(nogo)) {
  data_nogoT_final[, nogo[i]] <- as.numeric(data_nogoT_final[, nogo[i]])
}

# Describe
describe(data_nogoT_final[, nogo])
par(mfrow = c(2, 3))
for (i in 1:length(nogo)) {
  hist(data_nogoT_final[, nogo[i]], 
       main = colnames(data_nogoT_final[nogo[i]]), 
       col = sample(colors(), 1), 
       xlab = "")
}

```

NOTE: double check go no-go, lots of things to discuss

### Navon

Trial-level exclusion criteria:
* The reaction time for each row is replaced with missing values (NAs) if it is equal to or higher than 2000ms
NOTE: should we also have a lower boundary as in go no-go?

Participant-level exclusion criteria:
* A participant's data for the Navon task is replaced with missing values (NAs) if their median reaction time was equal to or lower than 50ms *or* if their accuracy (% of correct trials out of all trials) was equal to or lower than 60% *or* if the ratio of the most frequent response to all responses is equal to or higher than .95.
NOTE: why 50ms and not 200ms?

Data processing: for each participant, we calculate the global-local precedence index (bias toward a global processing level), and global-to-local interference index (positive values indicate the extent to which the bias toward global stimuli interferes with processing local information).
* Global-local precedence index: Standardized mean difference (cohen’s d) in RT between global and local judgments on consistent trials only 
* Global-to-local interference index: Standardized mean difference (cohen’s d) in RT between inconsistent and consistent trials in local condition only 
These are described and visualized.

```{r Navon Task, include = FALSE}

# Omit experiment-general columns & practice (etc.) rows
data_NavonT <- data_NavonT %>%
  select(Participant.Private.ID, Spreadsheet:Image) %>%
  filter(display %in% c("task1", "task2")) %>% 
  filter(Screen.Name == "Screen 3")

# Create a column for consistency
data_NavonT <- data_NavonT %>%
  mutate(Consistency = case_when((Image == "bigHsmallH.png" | Image == "bigSsmallS.png") ~ "Consistent", 
                                 (Image == "bigHsmallS.png" | Image == "bigSsmallH.png") ~ "Inconsistent"))
data_NavonT$Consistency <- as.factor(data_NavonT$Consistency)
data_NavonT$display <- as.factor(data_NavonT$display)

# Accuracy for each task and consistency
Accuracy_Navon <- data_NavonT %>%
  group_by(Consistency, display) %>%
  summarise(Accuracy_cond = sum(Correct, na.rm = TRUE) / (sum(Correct, na.rm = TRUE) + sum(Incorrect, na.rm = TRUE)))

# Exclusion Criteria: Trial-Level
# NOTE: Caitlin had used different ones (interquartile range or trials that are outside 2.5 SDs of the mean per participant); we should discuss this; WHY 50ms median RT and not 200ms like in gonogo?
  # Based on the histogram, I picked an arbitrary cut-off point of 2000ms to maintain a less skewed distribution but also keep most of the data.
data_NavonT$Reaction.Time[which(data_NavonT$Reaction.Time >= 2000)] <- NA

# Exclusion Criteria: Participant-Level
  # <= 50ms median RT
  # <= 60% accuracy
  # ratio of most frequent answer to all answers >= .95
data_NavonT <- data_NavonT %>%
  group_by(Participant.Private.ID) %>%
  mutate(MedianRT = median(Reaction.Time, na.rm = TRUE), 
         Accuracy = sum(Correct, na.rm = TRUE) / (sum(Correct, na.rm = TRUE) + sum(Incorrect, na.rm = TRUE)), 
         Ratio = max(table(Response), na.rm = TRUE) / length(Response)) %>%
  mutate(Navon_omit = case_when(MedianRT > 50 & MedianRT < 1000 & Accuracy > .60 & Ratio < .95 ~ 0, 
                MedianRT <= 50 | MedianRT >= 1000 | Accuracy <= .60 | Ratio >= .95 ~ 1))

# Omit data based on Navon_omit
data_NavonT$Reaction.Time[which(data_NavonT$Navon_omit == 1)] <- NA

# Subset to only look at correct answers
data_NavonT <- data_NavonT %>%
  filter(Correct == "1")

# Ungroup df
data_NavonT <- ungroup(data_NavonT)

# Global SD 1 (consistent trials only)
sd_Global_con <- data_NavonT %>%
  filter((Consistency == "Consistent") & display == "task1") %>%
  summarise(sd(Reaction.Time, na.rm = TRUE))
sd_Global_con <- sd_Global_con[[1]]

# Local SD 1 (consistent trials only)
sd_Local_con <- data_NavonT %>%
  filter((Consistency == "Consistent") & display == "task2") %>%
  summarise(sd(Reaction.Time, na.rm = TRUE))
sd_Local_con <- sd_Local_con[[1]]

# Local SD 2 (inconsistent trials only)
sd_Local_incon <- data_NavonT %>%
  filter((Consistency == "Inconsistent") & display == "task2") %>%
  summarise(sd(Reaction.Time, na.rm = TRUE))
sd_Local_incon <- sd_Local_incon[[1]]
 
# Calculate pooled SD
# https://www.statisticshowto.com/pooled-standard-deviation/ 

# Pooled SD consistent (local and global)
pooled_SD_con <- sqrt((sd_Global_con^2 + sd_Local_con^2)/2)

# Pooled SD local (consistent and inconsistent)
pooled_SD_local <- sqrt((sd_Local_incon^2 + sd_Local_con^2)/2)

# Final form of the Navon data 
  # Select relevant columns and transform the data so columns reflect mean reaction times in each of the four conditions (local/global x consistent/inconsistent)
data_NavonT_final <- data_NavonT %>% 
  select(Participant.Private.ID, Consistency, display, Reaction.Time) %>%
  group_by(Participant.Private.ID, Consistency, display) %>%
  summarise(NavonReactionTimeMean = mean(Reaction.Time, na.rm = TRUE), 
            .groups = "keep") %>%
  pivot_wider(names_from = c(Consistency, display), 
              values_from = NavonReactionTimeMean)

# Global-local precedence index: Standardized mean difference (cohen’s d) in RT between global and local judgments on consistent trials only 
data_NavonT_final <- data_NavonT_final %>%
  mutate(GlobalToLocalPrecedence = ((Consistent_task1 - Consistent_task2) / as.numeric(pooled_SD_con)))

# Global-to-local interference index: Standardized mean difference (cohen’s d) in RT between inconsistent and consistent trials in local condition only
data_NavonT_final <- data_NavonT_final %>%
  mutate(GlobalToLocalInterference = ((Inconsistent_task1 - Consistent_task2) / as.numeric(pooled_SD_local)))

# Exclude nonvalid participants
data_NavonT_final <- merge(data_NavonT_final, data_ConsentValidity, 
                           by = "Participant.Private.ID")

# Validity/languege filter + get rid of participants who stopped mid-task + select relevant columns
data_NavonT_final <- data_NavonT_final %>%
  filter(Validity.quantised %in% c(1, NA, "") & Language %in% c("Sujuva / Ã¤idinkieli", "Keskitaso / keskusteleva","Muu, mikÃ¤?")) %>%  
  filter(Participant.Private.ID != "4907987" & Participant.Private.ID != "4960307" & Participant.Private.ID != "5317567" & Participant.Private.ID != "5372338" & Participant.Private.ID != "5526450" & Participant.Private.ID != "5578226" & Participant.Private.ID != "5626669" & Participant.Private.ID != "5808176") %>%
  select(Participant.Private.ID, Consistent_task1, Consistent_task2, Inconsistent_task1, Inconsistent_task2, GlobalToLocalPrecedence, GlobalToLocalInterference)

# NOTE: for some reason there are two NaN rows, removing them here but we should look into why this is?
  # one of them was omitted (all RTs replaced with NA), one did not have recorded reaction times in the original data?
data_NavonT_final <- data_NavonT_final %>% 
  filter(Participant.Private.ID != "5608075" & Participant.Private.ID != "5411218")

# Describe
Navon <- c("Consistent_task1", "Consistent_task2", "Inconsistent_task1", "Inconsistent_task2", "GlobalToLocalPrecedence", "GlobalToLocalInterference")
describe(data_NavonT_final[, Navon])
par(mfrow = c(2, 3))
for (i in 1:length(Navon)) {
  hist(data_NavonT_final[, Navon[i]], 
       main = colnames(data_NavonT_final[Navon[i]]), 
       col = sample(colors(), 1), 
       xlab = "")
}

```
NOTE: also discuss navon

### Necker

Exclusion criteria: none for now
NOTE: could consider high outliers (right tail of the distribution) or participants who reported no switching

Data processing: for each participant, we calculate the average rate of space bar hits (switches the participant experienced) per second. 

```{r Necker Cube, include = FALSE}

# Omit irrelevant rows and columns
data_NeckerT <- data_NeckerT %>%
  select(Participant.Private.ID, Spreadsheet:Image) %>%
  filter(display %in% c("Trial 1", "Trial 2"))

# Calculate sum score for how many times space bar was hit in total
data_NeckerT_final <- data_NeckerT %>%
  select(Participant.Private.ID, Response) %>%
  group_by(Participant.Private.ID) %>%
  summarise(NeckerCountTotal = sum(Response == "space", na.rm = TRUE))

# Calculate switches per second
data_NeckerT_final <- data_NeckerT_final %>%
  mutate(NeckerTotalRate = NeckerCountTotal/60)

# Merge to exclude participants
data_NeckerT_final <- merge(data_NeckerT_final, data_ConsentValidity, 
                            by = "Participant.Private.ID")

# Subset by validity and consent info + omit the irrelevant columns
data_NeckerT_final <- data_NeckerT_final %>%
  filter(Validity.quantised %in% c("1", NA, "") & Language %in% c("Sujuva / Ã¤idinkieli", "Keskitaso / keskusteleva", "Muu, mikÃ¤?")) %>%
  filter(Participant.Private.ID != "5385143") %>% 
  select(Participant.Private.ID, NeckerCountTotal, NeckerTotalRate)

# Describe
describe(data_NeckerT_final$NeckerTotalRate)
hist(data_NeckerT_final$NeckerTotalRate, 
       main = "Necker Total Rate", 
       col = sample(colors(), 1), 
       xlab = "")

```

### Unusual Uses Task

Exclusion criteria: None for now.
NOTE: should we have something?

Data processing: for each participant, we calculate the average of the fluency sum scores (ratings by two raters) and the flexibility sum scores (ratings by two raters). The fluency/flexibility sum scores capture the number of answers across all trials that were classified as fluent/flexible.
* Fluency is a measure of how many unique unusual uses a ppt can think of for each item. 
* Flexibility score refers to the uniqueness of the functional categories of items, i.e. a participant gets two point for using a shoe as a doorstop and a flowerpot but only one point for a houseplant pot and a bonsai tree pot (same functional use). 

```{r Unusual Uses Task, include = FALSE}

# Load data
data_UUT <- read.csv("UUT_scoring_COMBINED_REVISED_Rcsv.csv", sep = ";")

# Rename participant ID column
names(data_UUT)[1] <- "Participant.Private.ID"

# Select relevant columns + calculate the fluency and flexibility sum score for each participant per rater
data_UUT <- data_UUT %>% 
  select(Participant.Private.ID, S.Fluency, S.Flex, K.Fluency, K.Flex) %>%
  group_by(Participant.Private.ID) %>%
  summarise(SFlexibilitySum = sum(S.Flex), SFluencySum = sum(S.Fluency),
            KFlexibilitySum = sum(K.Flex), KFluencySum = sum(K.Fluency))

# Calculate participant scores as a mean of the two raters' scores (one for fluency, one for flexibility)
data_UUT$FlexSum <- rowMeans(cbind(data_UUT$SFlexibilitySum,
                                   data_UUT$KFlexibilitySum), 
                             na.rm = TRUE)
data_UUT$FluencySum <- rowMeans(cbind(data_UUT$SFluencySum, 
                                      data_UUT$KFluencySum), 
                                na.rm = TRUE)

# Check validity + extract relevant columns
data_UUT <- merge(data_UUT, data_ConsentValidity, 
                  by = "Participant.Private.ID")
data_UUT <- data_UUT %>%
  filter(Validity.quantised %in% c("1", NA, "") & Language %in% c("Sujuva / Ã¤idinkieli", "Keskitaso / keskusteleva", "Muu, mikÃ¤?")) %>%
  select(Participant.Private.ID, FlexSum, FluencySum)

# Describe
uut <- c("FlexSum", "FluencySum")
describe(data_UUT[, uut])
par(mfrow = c(2, 1))
for (i in 1:length(uut)) {
  hist(data_UUT[, uut[i]], 
       main = colnames(data_UUT[uut[i]]), 
       col = sample(colors(), 1), 
       xlab = "")
}

```

# Merge task variables

Doesn't yet include the CI task! 

```{r Combine the Task Dataframes, include = FALSE}

# Merge dataframes
data_T_total1 <- merge(data_nogoT_final, data_NavonT_final, 
                       by = "Participant.Private.ID", all = TRUE)
data_T_total2 <- merge(data_UUT, data_NeckerT_final, 
                       by = "Participant.Private.ID", all = TRUE)
data_T_total <- merge(data_T_total1, data_T_total2, 
                      by = "Participant.Private.ID", all = TRUE)
rm(data_T_total1, data_T_total2)

# Extract the relevant columns
data_T_sub <- data_T_total %>%
  select(Participant.Private.ID, GNGdprime, GNGbeta, MeanGoRT, SDGoRT, GlobalToLocalPrecedence, GlobalToLocalInterference, NeckerTotalRate, FlexSum, FluencySum)

# Participant ID into factor
data_T_sub$Participant.Private.ID <- as.factor(data_T_sub$Participant.Private.ID)

```


## Analyze Missing Data in Tasks

Participants with large amounts of missing data or suspicious patterns of missing data are removed here. It should be noted that missing data refers to data that was missing from the start *and* data that was replaced with missing values if the participant met one of the exclusion criteria. Done separately for questionnaires and tasks.

```{r Analyze Missing Data in Tasks}

# Add a column for count of missing data per ppt
data_T_sub <- data_T_sub %>%
  mutate(missingtasks = rowSums(is.na(data_T_sub)))
table(data_T_sub$missingtasks)
plot(sort(data_T_sub$missingtasks))

# NOTE: which cut off? 4 was used before but not sure as there is no elbow here

# Merge demographic information to investigate missingness
# NOTE: what to do about the age correlation?
data_T_all <- data_Q_total %>%
  select(Participant.Private.ID, gender, Age, Country, Language, Education)
data_T_sub <- merge(data_T_all, data_T_sub, 
                    by = "Participant.Private.ID", all = TRUE)
gg_miss_fct(x = data_T_sub, fct = Age)
cor.test(data_T_sub$Age, data_T_sub$missing, 
         method = "spearman", exact = FALSE)

# Remove participants with 5 or more missing task variables
data_T_sub <- data_T_sub %>%
  group_by(Participant.Private.ID) %>%
  filter(missingtasks < 4)

# Omit irrelevant columns
data_T_sub <- data_T_sub %>%
  select(-missingtasks)

```

## Check Task Distributions and Correlations

This chunk checks distributions of task variables and correlations between them:
* If distributions are highly skewed, transformations are applied to decrease skewness
* If two variables are highly correlated, one of them is dropped (Eisenberg et al.)

```{r Normality and Correlations of Tasks, include = FALSE}

# Normality of variables: visualize with histograms
data_T_sub <- as.data.frame(data_T_sub)
vars_T <- c("GNGdprime", "GNGbeta", "MeanGoRT", "SDGoRT", "GlobalToLocalPrecedence", "GlobalToLocalInterference", "NeckerTotalRate", "FlexSum", "FluencySum")
par(mfrow = c(2, 2))
for (i in 1:length(vars_T)) {
  hist(data_T_sub[, vars_T[i]],
       main = colnames(data_T_sub[vars_T[i]]),
       col = sample(colors(), 1),
       xlab = "")
}

# Describe: mean, skew, kurtosis
describe(data_T_sub[, vars_T])[c("mean", "skew", "kurtosis")]

# Correlation matrix
round(cor(data_T_sub[, vars_T], method = "pearson", 
          use = "pairwise.complete.obs"), 2)
abs(round(cor(data_T_sub[, vars_T], method = "pearson", 
              use = "pairwise.complete.obs"), 2)) > .85
corrplot(cor(data_T_sub[, vars_T], method = "pearson", 
             use = "pairwise.complete.obs"))

# NOTE: we should discuss this, only transforming GNGbeta for now
data_T_sub$GNGbetalog <- log10(data_T_sub$GNGbeta)

# NOTE: we should also discuss which variables to remove; here removing GlobalToLocalInterference and FluencySum  
data_T_sub <- data_T_sub %>%
  select(Participant.Private.ID, GNGdprime, GNGbetalog, MeanGoRT, SDGoRT, GlobalToLocalPrecedence, NeckerTotalRate, FlexSum)

```

# Format the Task and Questionnaire Dataframes 

```{r Combine Task and Questionnaire Dataframes, include = FALSE}

data_all_sub_cleaned <- merge(data_Q_sub, data_T_sub, 
                              by = "Participant.Private.ID", 
                              all = TRUE)

# NOTE: should we check missingness again?
missing_data <- data_all_sub_cleaned %>%
  mutate(missing = rowSums(is.na(data_all_sub_cleaned))) %>% 
  select(Participant.Private.ID, missing)

# Ungroup the dataframe
ungroup(data_all_sub_cleaned)
data_all_sub_cleaned <- data_all_sub_cleaned %>%
  select(AMean:FlexSum)

```

# Further Subsetting?

```{r Possible Subsetting?}

# Binary variables
binary <- c("HEscore", "HRscore")

# Go No-Go variables
gng <- c("GNGdprime", "GNGbetalog", "MeanGoRT", "SDGoRT")

# NOTE: do we want to get rid of them for analysis? binary might be an issue still but gng are ok in terms of correlations
 
```

# Citizen's Initiative
Potentially problematic ppts for CI: 
ri0gfe1h	5767733	skipped reading at least one article
vu1mvwkd	5691188	didn't focus on the first CI text
8zz2teyh	5629594	is dyslexic
tjh0tjq3	4887029	CI task invalid


```{r}

# will add the code when it works on its own 


```




