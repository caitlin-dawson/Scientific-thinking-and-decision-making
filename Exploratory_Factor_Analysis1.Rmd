---
title: "Exploratory Factor Analysis"
author: "Milla Pihlajamaki and Caitlin Dawson"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_notebook
---

Need to do EFA in lavaan, Caitlin had linked a tutorial + had some code; we need to do this to extract the factor scores
-> change all FA to lavaan instead of psych
-> extract factor scores + visualization

```{r Load Packages}

# library(ISLR) 
# library(glmnet) 
# library(dplyr) 
# library(tidyr)
library(tidyverse)
library(corrplot)
# library(caret)
library(glmnet)
library(semPlot)
# library(tidyverse)
# library(tidyselect)
library(psych)
library(lavaan)
# library(dplyr)
# library(ggplot2)
# library(psychTools)
# library(GPArotation)
# library(devtools)
# library(apaTables)
# library(semPlot)
# library(sjPlot)
# library(glasso)

# Load data
data_efa <- read.csv("data_efa.csv")

# Create task- and survey-only sets
t_var_log <- c("GNGdprime", "GNGbetalog", "MeanGoRT", "SDGoRTlog", "GlobalToLocalPrecedencelog", "GlobalToLocalInterferencelog", "NeckerTotalRatelog", "FlexSum")
q_var <- c("AMean", "CMean", "EMean", "ESMean", "OMean", "ICuriositySum", "DCuriositySum", "IH1Sum", "IH2Sum", "IH3Sum", "IH4Sum", "CloSum", "CogSum", "AOTSum", "HEscore", "HRscore", "RPSum", "sci_cur", "sci_tru", "sci_impo", "sci_id", "MatrixCorrectCount")
survey_data <- data_efa[, q_var]
task_data <- data_efa[, t_var_log]

```


# Cross-method prediction

10-fold cross validated ridge regression to predict survey and task variables *within* and *between* measure types following Eisenberg et al. Intended to help decide whether to factor tasks and surveys together or separately, and also to see if some variables should be left out. 

Taking each DV one by one and trying to predict it using its own measurement category (leaving it out) and by the other measurement category. Each DV is the target; the predictors are the other variables in the category. 

4 distributions of predictions: tasks-by-tasks, tasks-by-surveys, surveys-by-tasks, and surveys-by-surveys -> extract the R^2 values and create four distributions

"Prediction success was assessed by 10-fold cross-validated ridge regression using the RidgeCV function from scikit-learn with default parameters (10). Almost all DVs were able to be predicted to some degree by their respective measurement category (mean task-by-tasks R2 mean survey-by-surveys R2  = .45). In contrast, cross-measurement prediction failed: surveys were unable to predict task DVs and vice versa (mean task-by-surveys R2 = -.13, mean survey-by-tasks R2 = -.29). Note that R2 values below 0 are possible when employing  cross-validation and indicate no discoverable linear relationship. The entire distribution of R2  values for each of these predictions is shown is Figure S2b."

Ridge regression differs from lasso regression in that it can reduce coefficients down to near zero but always includes all predictors. Lasso regression can be used to reduce dimensionality as coefficients can be zero. Elastic net regression combines the two. 

Ridge regression works better than regular linear regression when you have many variables in a model and also collinearity. It is a form of penalized regression that adds a constraint. The constraint constant is defined by *lambda*, which shrinks coefficients. A lambda of 0 is a regular linear regression, and a too large lambda will shrink the coefficients too much to make a usable model. We can identify the best value of lambda by using cross-validation. 

Instead of splitting data into a training and test set, cross-validation trains and tests the model on subsets of the full dataset, creating a distribution. Here we just use it to compare various values of lambda in order to choose the best one for each prediction model. 

The aim of this section is to create a model for predicting each DV in the 4 distributions described above. Ridge regression is used because of the type of dataset and because we want to retain all variables when checking the cross- and between-measure type predictions. 

The question here is, can X variable be predicted equally well by surveys vs. tasks? After we get these R2 for each model (there will be 4 models per variable), we can take the mean R2 for each of the 4 distributions and make a plot as in Eisenberg to examine whether the distributions are similar. This helps us decide whether task and survey variables should be clustered or factored together.  

```{r Ridge Regression (Cross-Method Prediction)}

# Survey x survey 
# NOTE: not sure if the binary items are an issue? not sure if this is the case when they are predictors but I would assume when they are being predicted? I will omit them here just in case
q_var_sub <- q_var[-c(which(q_var == "HEscore"), which(q_var == "HRscore"))]
survey_data_sub <- data_efa[, q_var_sub]

predicted_SS <- c()
predicted_type_SS <- rep("survey", length(q_var_sub))
predictors_SS <- rep("surveys", length(q_var_sub))
R2_SS <- c()

for (i in 1:length(q_var_sub)) {
  
  # Save x and y
  x_var <- as.matrix(survey_data_sub[, -which(colnames(survey_data_sub) == q_var_sub[i])])
  y_var <- survey_data_sub[, q_var_sub[i]]
  predicted_SS <- c(predicted_SS, q_var_sub[i])
  
  # Fit the ridge regression model
  model <- glmnet(x = x_var, y = y_var, alpha = 0)
  
  # Perform k-fold cross-validation to find optimal lambda value
  cv_model <- cv.glmnet(x = x_var, y = y_var, alpha = 0)
  
  # Find optimal lambda
  best_lambda <- cv_model$lambda.min
  
  # Find coefficients for the best model
  best_model <- glmnet(x = x_var, y = y_var, alpha = 0, lambda = best_lambda)
  
  # Use fitted best model to make predictions (NOTE: not 100% sure about this)
  y_predicted <- predict(object = best_model, s = best_lambda, newx = x_var)
  
  # Find SST and SSE
  sst <- sum((y_var - mean(y_var))^2)
  sse <- sum((y_predicted - y_var)^2)
  
  # Find R-squared of the model on the training data
  rsq <- 1 - sse/sst
  R2_SS <- c(R2_SS, rsq)
  
}

SxS <- data.frame(predicted = predicted_SS, predictors = predictors_SS, 
                  predicted_type = predicted_type_SS, R2 = R2_SS)

# Task x task
predicted_TT <- c()
predicted_type_TT <- rep("task", length(t_var_log))
predictors_TT <- rep("tasks", length(t_var_log))
R2_TT <- c()
for (i in 1:length(t_var_log)) {
  x_var <- as.matrix(task_data[, -which(colnames(task_data) == t_var_log[i])])
  y_var <- task_data[, t_var_log[i]]
  predicted_TT <- c(predicted_TT, t_var_log[i])
  model <- glmnet(x = x_var, y = y_var, alpha = 0)
  cv_model <- cv.glmnet(x = x_var, y = y_var, alpha = 0)
  best_lambda <- cv_model$lambda.min
  best_model <- glmnet(x = x_var, y = y_var, alpha = 0, lambda = best_lambda)
  y_predicted <- predict(object = best_model, s = best_lambda, newx = x_var)
  sst <- sum((y_var - mean(y_var))^2)
  sse <- sum((y_predicted - y_var)^2)
  rsq <- 1 - sse/sst
  R2_TT <- c(R2_TT, rsq)
}
TxT <- data.frame(predicted = predicted_TT, predictors = predictors_TT, 
                  predicted_type = predicted_type_TT, R2 = R2_TT)

# Survey x task (surveys predicted by tasks)
predicted_ST <- c()
predicted_type_ST <- rep("survey", length(q_var_sub))
predictors_ST <- rep("tasks", length(q_var_sub))
R2_ST <- c()
for (i in 1:length(q_var_sub)) {
  x_var <- as.matrix(task_data)
  y_var <- survey_data_sub[, q_var_sub[i]]
  predicted_ST <- c(predicted_ST, q_var_sub[i])
  model <- glmnet(x = x_var, y = y_var, alpha = 0)
  cv_model <- cv.glmnet(x = x_var, y = y_var, alpha = 0)
  best_lambda <- cv_model$lambda.min
  best_model <- glmnet(x = x_var, y = y_var, alpha = 0, lambda = best_lambda)
  y_predicted <- predict(object = best_model, s = best_lambda, newx = x_var)
  sst <- sum((y_var - mean(y_var))^2)
  sse <- sum((y_predicted - y_var)^2)
  rsq <- 1 - sse/sst
  R2_ST <- c(R2_ST, rsq)
}
SxT <- data.frame(predicted = predicted_ST, predictors = predictors_ST, 
                  predicted_type = predicted_type_ST, R2 = R2_ST)

# Tasks x surveys (tasks predicted by surveys)
predicted_TS <- c()
predicted_type_TS <- rep("task", length(t_var_log))
predictors_TS <- rep("surveys", length(t_var_log))
R2_TS <- c()
for (i in 1:length(t_var_log)) {
  x_var <- as.matrix(survey_data_sub)
  y_var <- task_data[, t_var_log[i]]
  predicted_TS <- c(predicted_TS, t_var_log[i])
  model <- glmnet(x = x_var, y = y_var, alpha = 0)
  cv_model <- cv.glmnet(x = x_var, y = y_var, alpha = 0)
  best_lambda <- cv_model$lambda.min
  best_model <- glmnet(x = x_var, y = y_var, alpha = 0, lambda = best_lambda)
  y_predicted <- predict(object = best_model, s = best_lambda, newx = x_var)
  sst <- sum((y_var - mean(y_var))^2)
  sse <- sum((y_predicted - y_var)^2)
  rsq <- 1 - sse/sst
  R2_TS <- c(R2_TS, rsq)
}
TxS <- data.frame(predicted = predicted_TS, predictors = predictors_TS, 
                  predicted_type = predicted_type_TS, R2 = R2_TS)

# Bind the dataframes
R2_crossmethod <- rbind(SxS, TxT, SxT, TxS)

# Describe R^2 per group
R2_crossmethod <- R2_crossmethod %>% 
  mutate(format = case_when((predicted_type == "task" & predictors == "tasks")  ~ "TxT",
                            (predicted_type == "survey" & predictors == "surveys")  ~ "SxS",
                            (predicted_type == "task" & predictors == "surveys")  ~ "TxS",
                            (predicted_type == "survey" & predictors == "tasks")  ~ "SxT"))
R2_crossmethod$format <- as.factor(R2_crossmethod$format)
R2_crossmethod %>% 
  group_by(format) %>% 
  summarise(mean(R2), sd(R2))

# Visualize R^2 per group
# Same plot
ggplot(R2_crossmethod, aes(x = R2)) +
  geom_histogram(aes(color = format, fill = format),
                 position = "identity", bins = 50) +
  scale_color_manual(values = c("#9AC4F8", "#99EDCC", "#CB958E", "#E36588")) +
  scale_fill_manual(values = c("#9AC4F8", "#99EDCC", "#CB958E", "#E36588")) +
  theme_minimal()
# Separate plots
par(mfrow = c(2, 2))
formats <- c("SxS", "SxT", "TxT", "TxS")
for (i in 1:4) {
  hist(R2_crossmethod[which(R2_crossmethod$format == formats[i]), "R2"],
       main = paste0("R2 for ", formats[i]),
       col = sample(colors(), 1),
       breaks = 20,
       xlab = "")
}

# Nicer visualizations?

```

# Exploratory Factor Analysis

This section includes three series of EFAs: all variables, surveys only, and tasks only. In the preregistration, we promised to start with these. 

Correlations are mixed because of the binary variables from the heuristics task

## Create Correlation Matrices

```{r Create Correlation Matrices, include = FALSE}

# A general note: we probably need to justify putting a bunch of task and self-report items in the same factor analysis / at least mention it !
# https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3268653/

# Descriptives
describe(data_efa)

# Create character strings with different classes of variables
# NOTE: do we actually want to treat ordinals as continuous? I think another option would be polytomous, but I am not sure; at least we should be consistent throughout the analyses 
con <- c("AMean", "CMean", "EMean", "ESMean", "OMean", "ICuriositySum", "DCuriositySum", "IH1Sum", "IH2Sum", "IH3Sum", "IH4Sum", "CloSum", "CogSum", "AOTSum", "RPSum", "sci_cur", "sci_tru", "sci_impo", "sci_id", "MatrixCorrectCount", "GNGdprime", "GNGbetalog", "MeanGoRT", "SDGoRTlog", "GlobalToLocalPrecedencelog", "GlobalToLocalInterferencelog", "NeckerTotalRatelog", "FlexSum")
q_con <- c("AMean", "CMean", "EMean", "ESMean", "OMean", "ICuriositySum", "DCuriositySum", "IH1Sum", "IH2Sum", "IH3Sum", "IH4Sum", "CloSum", "CogSum", "AOTSum", "RPSum", "sci_cur", "sci_tru", "sci_impo", "sci_id", "MatrixCorrectCount")
cat <- c("HEscore", "HRscore")

# Create a correlation matrix (polychoric/mixed correlations)
all_cor <- mixedCor(data = data_efa, d = cat, c = con, 
                    use = "pairwise.complete.obs",
                    method = "spearman")
Q_cor <- mixedCor(data = survey_data, d = cat, c = q_con,
                  use = "pairwise.complete.obs",
                  method = "spearman")
T_cor <- mixedCor(data = task_data, 
                  use = "pairwise.complete.obs",
                  method = "pearson")
# NOTE: Caitlin had specified other arguments as well but I am not sure how necessary there are?

# Store correlation matrix
all_rho <- all_cor$rho
Q_rho <- Q_cor$rho
T_rho <- T_cor$rho

# Mean correlation across items
mean(abs(all_rho[lower.tri(all_rho)]))
mean(abs(Q_rho[lower.tri(Q_rho)]))
mean(abs(T_rho[lower.tri(T_rho)]))

# Histogram of correlations
hist(abs(all_rho[lower.tri(all_rho)]), 
     breaks = 20, col = sample(colors(), 1))
hist(abs(Q_rho[lower.tri(Q_rho)]), 
     breaks = 20, col = sample(colors(), 1))
hist(abs(T_rho[lower.tri(T_rho)]), 
     breaks = 20, col = sample(colors(), 1))

# Corrplot without titles
corrplot(all_rho, tl.pos = "n")

```

## Factorability

```{r Factorability, include = FALSE}

# Bartlett test (p<.05 indicates data are suitable for structure detection)
cortest.bartlett(all_rho, n = nrow(data_efa))
cortest.bartlett(Q_rho, n = nrow(survey_data))
cortest.bartlett(T_rho, n = nrow(task_data))

# Kaiser-Meyer-Olkin measure (>.7 is good)
sort(round(KMO(r = all_rho)$MSAi, 2))
sort(round(KMO(r = Q_rho)$MSAi, 2))
sort(round(KMO(r = T_rho)$MSAi, 2))
# NOTE: KMO is not very good for a lot of items?

```

## Identify number of factors to extract

For now I used EBIC as the main criterion following Eisenberg et al. Zmigrod et al. used scree plots and PA.

```{r Number of factors, include = FALSE}

# Scree plots
# All variables
plot(eigen(all_rho)$values, type = "b", pch = 20, col = "red",
     main = "Scree Plot", ylab = "Eigenvalues",
     xlab = "No. Factors")
# Questionnaire variables
plot(eigen(Q_rho)$values, type = "b", pch = 20, col = "red",
     main = "Scree Plot", ylab = "Eigenvalues",
     xlab = "No. Factors")
# Task variables
plot(eigen(T_rho)$values, type = "b", pch = 20, col = "red",
     main = "Scree Plot", ylab = "Eigenvalues",
     xlab = "No. Factors")

# NFactors
# All variables
nfactors(all_rho, n = 6, n.obs  = nrow(data_efa), 
         rotate = "oblimin", diagonal = FALSE, 
         fm = "minrank") 
# Questionnaires
nfactors(Q_rho, n = 6, n.obs  = nrow(survey_data), 
         rotate = "oblimin", diagonal = FALSE,
         fm = "minrank") 
# Tasks
nfactors(T_rho, n = 6, n.obs  = nrow(task_data), 
         rotate = "oblimin", diagonal = FALSE,
         fm = "minrank") 

# Parallel Analysis
# All variables
fa.parallel(all_rho, n.obs = nrow(data_efa), fm = "minrank")
# cor.plot(all_rho)
# Questionnaires
fa.parallel(Q_rho, n.obs = nrow(survey_data), fm = "minrank")
# cor.plot(Q_rho)
# Tasks
fa.parallel(T_rho, n.obs = nrow(task_data), fm = "minrank")
# cor.plot(T_rho)

```

## Run the Factor Analysis with lavaan

Number of factors is determined with EBIC.

```{r EFA for Questionnaires (4 factors according to EBIC)}

# 4-factor model
f4 <- '
efa("efa")*f1 +
efa("efa")*f2 +
efa("efa")*f3 +
efa("efa")*f4 =~ AMean + CMean + EMean + ESMean + OMean + ICuriositySum + DCuriositySum + IH1Sum + IH2Sum + IH3Sum + IH4Sum + CloSum  +  CogSum + AOTSum + HEscore + HRscore + RPSum + sci_cur + sci_tru + sci_impo + sci_id + MatrixCorrectCount'

# Fit the model
efa_f4 <- cfa(model = f4,
              data = survey_data,
              rotation = "oblimin",
              estimator = "WLSMV",
              ordered = c("HEscore", "HRscore", "RPSum"))
summary(efa_f4, fit.measures = TRUE)
parameterEstimates(efa_f4)
fitmeasures(efa_f4)
# Looks like it's a just identified model? Fit measures are too good

# Look at loadings
loadings_4f <- inspect(efa_f4, what = "std")[["lambda"]]
fa.diagram(inspect(efa_f4, what = "std")[["lambda"]])
loadings_4f <- as.data.frame(loadings_4f)
round(fa.sort(loadings_4f), 2)
# Remove EMean, HEscore (no loadings >= .3)

survey_data_sub <- survey_data %>%
  select(-c("EMean", "HEscore", "IH1Sum"))  # removed EMean and HEscore first, and IH1sum second (here all at once)
f4_sub <- '
efa("efa")*f1 +
efa("efa")*f2 +
efa("efa")*f3 +
efa("efa")*f4 =~ AMean + CMean + ESMean + OMean + ICuriositySum + DCuriositySum + IH2Sum + IH3Sum + IH4Sum + CloSum  +  CogSum + AOTSum + HRscore + RPSum + sci_cur + sci_impo + sci_id + sci_tru + MatrixCorrectCount'

# Fit the model
efa_f4_sub <- cfa(model = f4_sub,
                  data = survey_data_sub,
                  rotation = "oblimin",
                  estimator = "WLSMV",
                  ordered = c("HRscore", "RPSum"))
summary(efa_f4_sub, fit.measures = TRUE)
parameterEstimates(efa_f4_sub)
fitmeasures(efa_f4_sub)
# Still looks not identified, look into degrees of freedom

loadings_4f_sub <- inspect(efa_f4_sub, what = "std")[["lambda"]]
fa.diagram(inspect(efa_f4_sub, what = "std")[["lambda"]])
loadings_4f_sub <- as.data.frame(loadings_4f_sub)
round(fa.sort(loadings_4f_sub), 2)


# Visualize
# 19 items, 4 factors
# Create layout matrix (23 nodes altogether)
y <- c(rep(c(-0.9, -0.8), length.out = 19), # items
       rep(0, 4)) # latents
x <- c(seq(from = -0.98, to = 0.98, length.out = 19), # items
       -0.75, -0.25, 0.25, 0.75)
coord <- matrix(data = c(x, y), nrow = 23, ncol = 2, byrow = FALSE)
semPaths(efa_f4_sub,
         what = "est",
         whatLabels = "no",
         as.expression = c("nodes", "edges"),
         theme = "colorblind", 
         curvePivot = TRUE,
         sizeMan = 3,
         sizeLat = 7,
         edge.label.cex = 1,
         reorder = FALSE,
         width = 8,
         height = 5,
         groups = "latents",
         borders = FALSE,
         residuals = FALSE,
         intercepts = FALSE,
         thresholds = FALSE,
         label.prop = .96,
         label.scale = TRUE,
         color = c(rep("#bfbfbf", 4)),
         layout = coord, 
         exoCov = FALSE,
         nodeLabels = c("AMean", "CMean", "ESMean", "OMean", "ICur", "DCur", "IH2", "IH3", "IH4", "Clo ", " Cog", "AOT", "HR", "RP", "sci_cur", "sci_tru", "sci_impo", "sci_id", "Mat_rea", "F1", "F2", "F3", "F4")
         # ,
         # filename = "4f_pathdiagram",
         # filetype = "pdf"
         )
# Omit irrelevant edges??


# THIS DOES NOT WORK FOR US YET! Need to figure out why
# obtain factor loadings and add them to data
loading_mat <- as.matrix(fa.sort(loadings_4f_sub))
loading_mat <- loading_mat[, -ncol(loading_mat)]
rownames(loading_mat) <- c("Respect for Others' Viewpoints (IH)", 
                           "Agreeableness (B5)", 
                           "sci_tru",
                           "sci_impo",
                           "IcuriositySum",
                           "Need for Cognition",
                           "Openness (B5)",
                           "DcuriositySum",
                           "sci_id",
                           "Science Curiosity",
                           "Need for Closure",
                           "Conscientiousness (B5)",
                           "Emotional Stability (B5)",
                           "Openness to Revising One's Viewpoint (IH)",
                           "Actively Openminded Thinking",
                           "Lack of Intellectual Overconfidence (IH)",
                           "Randomness-Probability", 
                           "Heuristic-Representativeness", 
                           "Matrix Reasoning")
colnames(loading_mat) <- c("Factor 1", "Factor 2", "Factor 3", "Factor 4")

corrplot(loading_mat, method = "color",
         addCoef.col = NULL, 
         number.cex = .75,
         tl.cex = .75,
         tl.col = "black", 
         cl.pos = "b",
         cl.length = 4,
         cl.cex = .6
         )
# Make this nicer



```


```{r Extract Factor Scores}

scores_4f <- lavPredict(efa_f4_sub, method = "EBM")

```


