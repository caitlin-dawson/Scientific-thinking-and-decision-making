---
title: "Exploratory Factor Analysis"
author: "Milla Pihlajamaki and Caitlin Dawson"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_notebook
---

Need to do EFA in lavaan, Caitlin had linked a tutorial + had some code; we need to do this to extract the factor scores
-> change all FA to lavaan instead of psych
-> extract factor scores + visualization

```{r Load Packages}

# library(ISLR) 
# library(glmnet) 
# library(dplyr) 
# library(tidyr)
# library(tidyverse)
# library(caret)
library(glmnet)
library(semPlot)
# library(tidyverse)
# library(tidyselect)
# library(psych)
# library(lavaan)
# library(dplyr)
# library(ggplot2)
# library(psychTools)
# library(GPArotation)
# library(devtools)
# library(apaTables)
# library(semPlot)
# library(sjPlot)
# library(glasso)

# Load data
data_efa <- read.csv("data_efa.csv")

# Create task- and survey-only sets
t_var_log <- c("GNGdprime", "GNGbetalog", "MeanGoRT", "SDGoRTlog", "GlobalToLocalPrecedencelog", "GlobalToLocalInterferencelog", "NeckerTotalRatelog", "FlexSum")
q_var <- c("AMean", "CMean", "EMean", "ESMean", "OMean", "ICuriositySum", "DCuriositySum", "IH1Sum", "IH2Sum", "IH3Sum", "IH4Sum", "CloSum", "CogSum", "AOTSum", "HEscore", "HRscore", "RPSum", "sci_cur", "sci_tru", "sci_impo", "sci_id", "MatrixCorrectCount")
survey_data <- data_efa[, q_var]
task_data <- data_efa[, t_var_log]

```


# Cross-method prediction

10-fold cross validated ridge regression to predict survey and task variables *within* and *between* measure types following Eisenberg et al. Intended to help decide whether to factor tasks and surveys together or separately, and also to see if some variables should be left out. 

Taking each DV one by one and trying to predict it using its own measurement category (leaving it out) and by the other measurement category. Each DV is the target; the predictors are the other variables in the category. 

4 distributions of predictions: tasks-by-tasks, tasks-by-surveys, surveys-by-tasks, and surveys-by-surveys -> extract the R^2 values and create four distributions

"Prediction success was assessed by 10-fold cross-validated ridge regression using the RidgeCV function from scikit-learn with default parameters (10). Almost all DVs were able to be predicted to some degree by their respective measurement category (mean task-by-tasks R2 mean survey-by-surveys R2  = .45). In contrast, cross-measurement prediction failed: surveys were unable to predict task DVs and vice versa (mean task-by-surveys R2 = -.13, mean survey-by-tasks R2 = -.29). Note that R2 values below 0 are possible when employing  cross-validation and indicate no discoverable linear relationship. The entire distribution of R2  values for each of these predictions is shown is Figure S2b."

Ridge regression differs from lasso regression in that it can reduce coefficients down to near zero but always includes all predictors. Lasso regression can be used to reduce dimensionality as coefficients can be zero. Elastic net regression combines the two. 

Ridge regression works better than regular linear regression when you have many variables in a model and also collinearity. It is a form of penalized regression that adds a constraint. The constraint constant is defined by *lambda*, which shrinks coefficients. A lambda of 0 is a regular linear regression, and a too large lambda will shrink the coefficients too much to make a usable model. We can identify the best value of lambda by using cross-validation. 

Instead of splitting data into a training and test set, cross-validation trains and tests the model on subsets of the full dataset, creating a distribution. Here we just use it to compare various values of lambda in order to choose the best one for each prediction model. 

The aim of this section is to create a model for predicting each DV in the 4 distributions described above. Ridge regression is used because of the type of dataset and because we want to retain all variables when checking the cross- and between-measure type predictions. 

The question here is, can X variable be predicted equally well by surveys vs. tasks? After we get these R2 for each model (there will be 4 models per variable), we can take the mean R2 for each of the 4 distributions and make a plot as in Eisenberg to examine whether the distributions are similar. This helps us decide whether task and survey variables should be clustered or factored together.  

```{r Ridge Regression (Cross-Method Prediction)}

# Survey x survey 
# NOTE: not sure if the binary items are an issue? not sure if this is the case when they are predictors but I would assume when they are being predicted? I will omit them here just in case
q_var_sub <- q_var[-c(which(q_var == "HEscore"), which(q_var == "HRscore"))]
survey_data_sub <- data_efa[, q_var_sub]

predicted_SS <- c()
predicted_type_SS <- rep("survey", length(q_var_sub))
predictors_SS <- rep("surveys", length(q_var_sub))
R2_SS <- c()

for (i in 1:length(q_var_sub)) {
  
  # Save x and y
  x_var <- as.matrix(survey_data_sub[, -which(colnames(survey_data_sub) == q_var_sub[i])])
  y_var <- survey_data_sub[, q_var_sub[i]]
  predicted_SS <- c(predicted_SS, q_var_sub[i])
  
  # Fit the ridge regression model
  model <- glmnet(x = x_var, y = y_var, alpha = 0)
  
  # Perform k-fold cross-validation to find optimal lambda value
  cv_model <- cv.glmnet(x = x_var, y = y_var, alpha = 0)
  
  # Find optimal lambda
  best_lambda <- cv_model$lambda.min
  
  # Find coefficients for the best model
  best_model <- glmnet(x = x_var, y = y_var, alpha = 0, lambda = best_lambda)
  
  # Use fitted best model to make predictions (NOTE: not 100% sure about this)
  y_predicted <- predict(object = best_model, s = best_lambda, newx = x_var)
  
  # Find SST and SSE
  sst <- sum((y_var - mean(y_var))^2)
  sse <- sum((y_predicted - y_var)^2)
  
  # Find R-squared of the model on the training data
  rsq <- 1 - sse/sst
  R2_SS <- c(R2_SS, rsq)
  
}

SxS <- data.frame(predicted = predicted_SS, predictors = predictors_SS, 
                  predicted_type = predicted_type_SS, R2 = R2_SS)

# Task x task
predicted_TT <- c()
predicted_type_TT <- rep("task", length(t_var_log))
predictors_TT <- rep("tasks", length(t_var_log))
R2_TT <- c()
for (i in 1:length(t_var_log)) {
  x_var <- as.matrix(task_data[, -which(colnames(task_data) == t_var_log[i])])
  y_var <- task_data[, t_var_log[i]]
  predicted_TT <- c(predicted_TT, t_var_log[i])
  model <- glmnet(x = x_var, y = y_var, alpha = 0)
  cv_model <- cv.glmnet(x = x_var, y = y_var, alpha = 0)
  best_lambda <- cv_model$lambda.min
  best_model <- glmnet(x = x_var, y = y_var, alpha = 0, lambda = best_lambda)
  y_predicted <- predict(object = best_model, s = best_lambda, newx = x_var)
  sst <- sum((y_var - mean(y_var))^2)
  sse <- sum((y_predicted - y_var)^2)
  rsq <- 1 - sse/sst
  R2_TT <- c(R2_TT, rsq)
}
TxT <- data.frame(predicted = predicted_TT, predictors = predictors_TT, 
                  predicted_type = predicted_type_TT, R2 = R2_TT)

# Survey x task (surveys predicted by tasks)
predicted_ST <- c()
predicted_type_ST <- rep("survey", length(q_var_sub))
predictors_ST <- rep("tasks", length(q_var_sub))
R2_ST <- c()
for (i in 1:length(q_var_sub)) {
  x_var <- as.matrix(task_data)
  y_var <- survey_data_sub[, q_var_sub[i]]
  predicted_ST <- c(predicted_ST, q_var_sub[i])
  model <- glmnet(x = x_var, y = y_var, alpha = 0)
  cv_model <- cv.glmnet(x = x_var, y = y_var, alpha = 0)
  best_lambda <- cv_model$lambda.min
  best_model <- glmnet(x = x_var, y = y_var, alpha = 0, lambda = best_lambda)
  y_predicted <- predict(object = best_model, s = best_lambda, newx = x_var)
  sst <- sum((y_var - mean(y_var))^2)
  sse <- sum((y_predicted - y_var)^2)
  rsq <- 1 - sse/sst
  R2_ST <- c(R2_ST, rsq)
}
SxT <- data.frame(predicted = predicted_ST, predictors = predictors_ST, 
                  predicted_type = predicted_type_ST, R2 = R2_ST)

# Tasks x surveys (tasks predicted by surveys)
predicted_TS <- c()
predicted_type_TS <- rep("task", length(t_var_log))
predictors_TS <- rep("surveys", length(t_var_log))
R2_TS <- c()
for (i in 1:length(t_var_log)) {
  x_var <- as.matrix(survey_data_sub)
  y_var <- task_data[, t_var_log[i]]
  predicted_TS <- c(predicted_TS, t_var_log[i])
  model <- glmnet(x = x_var, y = y_var, alpha = 0)
  cv_model <- cv.glmnet(x = x_var, y = y_var, alpha = 0)
  best_lambda <- cv_model$lambda.min
  best_model <- glmnet(x = x_var, y = y_var, alpha = 0, lambda = best_lambda)
  y_predicted <- predict(object = best_model, s = best_lambda, newx = x_var)
  sst <- sum((y_var - mean(y_var))^2)
  sse <- sum((y_predicted - y_var)^2)
  rsq <- 1 - sse/sst
  R2_TS <- c(R2_TS, rsq)
}
TxS <- data.frame(predicted = predicted_TS, predictors = predictors_TS, 
                  predicted_type = predicted_type_TS, R2 = R2_TS)

# Bind the dataframes
R2_crossmethod <- rbind(SxS, TxT, SxT, TxS)

# Describe R^2 per group
R2_crossmethod <- R2_crossmethod %>% 
  mutate(format = case_when((predicted_type == "task" & predictors == "tasks")  ~ "TxT",
                            (predicted_type == "survey" & predictors == "surveys")  ~ "SxS",
                            (predicted_type == "task" & predictors == "surveys")  ~ "TxS",
                            (predicted_type == "survey" & predictors == "tasks")  ~ "SxT"))
R2_crossmethod$format <- as.factor(R2_crossmethod$format)
R2_crossmethod %>% 
  group_by(format) %>% 
  summarise(mean(R2), sd(R2))

# Visualize R^2 per group
# Same plot
ggplot(R2_crossmethod, aes(x = R2)) +
  geom_histogram(aes(color = format, fill = format),
                 position = "identity", bins = 50) +
  scale_color_manual(values = c("#9AC4F8", "#99EDCC", "#CB958E", "#E36588")) +
  scale_fill_manual(values = c("#9AC4F8", "#99EDCC", "#CB958E", "#E36588")) +
  theme_minimal()
# Separate plots
par(mfrow = c(2, 2))
formats <- c("SxS", "SxT", "TxT", "TxS")
for (i in 1:4) {
  hist(R2_crossmethod[which(R2_crossmethod$format == formats[i]), "R2"],
       main = paste0("R2 for ", formats[i]),
       col = sample(colors(), 1),
       breaks = 20,
       xlab = "")
}

# Nicer visualizations?

```

# Exploratory Factor Analysis

This section includes three series of EFAs: all variables, surveys only, and tasks only. In the preregistration, we promised to start with these. 

Correlations are mixed because of the binary variables from the heuristics task

## Create Correlation Matrices

```{r Create Correlation Matrices, include = FALSE}

# A general note: we probably need to justify putting a bunch of task and self-report items in the same factor analysis / at least mention it !
# https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3268653/

# Descriptives
describe(data_efa)

# Create character strings with different classes of variables
# NOTE: do we actually want to treat ordinals as continuous? I think another option would be polytomous, but I am not sure; at least we should be consistent throughout the analyses 
con <- c("AMean", "CMean", "EMean", "ESMean", "OMean", "ICuriositySum", "DCuriositySum", "IH1Sum", "IH2Sum", "IH3Sum", "IH4Sum", "CloSum", "CogSum", "AOTSum", "RPSum", "sci_cur", "sci_tru", "sci_impo", "sci_id", "MatrixCorrectCount", "GNGdprime", "GNGbetalog", "MeanGoRT", "SDGoRTlog", "GlobalToLocalPrecedencelog", "GlobalToLocalInterferencelog", "NeckerTotalRatelog", "FlexSum")
q_con <- c("AMean", "CMean", "EMean", "ESMean", "OMean", "ICuriositySum", "DCuriositySum", "IH1Sum", "IH2Sum", "IH3Sum", "IH4Sum", "CloSum", "CogSum", "AOTSum", "RPSum", "sci_cur", "sci_tru", "sci_impo", "sci_id", "MatrixCorrectCount")
cat <- c("HEscore", "HRscore")

# Create a correlation matrix (polychoric/mixed correlations)
all_cor <- mixedCor(data = data_efa, d = cat, c = con, 
                    use = "pairwise.complete.obs",
                    method = "spearman")
Q_cor <- mixedCor(data = survey_data, d = cat, c = q_con,
                  use = "pairwise.complete.obs",
                  method = "spearman")
T_cor <- mixedCor(data = task_data, 
                  use = "pairwise.complete.obs",
                  method = "pearson")
# NOTE: Caitlin had specified other arguments as well but I am not sure how necessary there are?

# Store correlation matrix
all_rho <- all_cor$rho
Q_rho <- Q_cor$rho
T_rho <- T_cor$rho

# Mean correlation across items
mean(abs(all_rho[lower.tri(all_rho)]))
mean(abs(Q_rho[lower.tri(Q_rho)]))
mean(abs(T_rho[lower.tri(T_rho)]))

# Histogram of correlations
hist(abs(all_rho[lower.tri(all_rho)]), 
     breaks = 20, col = sample(colors(), 1))
hist(abs(Q_rho[lower.tri(Q_rho)]), 
     breaks = 20, col = sample(colors(), 1))
hist(abs(T_rho[lower.tri(T_rho)]), 
     breaks = 20, col = sample(colors(), 1))

# Corrplot without titles
corrplot(all_rho, tl.pos = "n")

```

## Factorability

```{r Factorability, include = FALSE}

# Bartlett test (p<.05 indicates data are suitable for structure detection)
cortest.bartlett(all_rho, n = nrow(data_efa))
cortest.bartlett(Q_rho, n = nrow(survey_data))
cortest.bartlett(T_rho, n = nrow(task_data))

# Kaiser-Meyer-Olkin measure (>.7 is good)
sort(round(KMO(r = all_rho)$MSAi, 2))
sort(round(KMO(r = Q_rho)$MSAi, 2))
sort(round(KMO(r = T_rho)$MSAi, 2))
# NOTE: KMO is not very good for a lot of items?

```

## Identify number of factors to extract

For now I used EBIC as the main criterion following Eisenberg et al. Zmigrod et al. used scree plots and PA.

```{r Number of factors, include = FALSE}

# Scree plots
# All variables
plot(eigen(all_rho)$values, type = "b", pch = 20, col = "red",
     main = "Scree Plot", ylab = "Eigenvalues",
     xlab = "No. Factors")
# Questionnaire variables
plot(eigen(Q_rho)$values, type = "b", pch = 20, col = "red",
     main = "Scree Plot", ylab = "Eigenvalues",
     xlab = "No. Factors")
# Task variables
plot(eigen(T_rho)$values, type = "b", pch = 20, col = "red",
     main = "Scree Plot", ylab = "Eigenvalues",
     xlab = "No. Factors")

# NFactors
# All variables
nfactors(all_rho, n = 6, n.obs  = nrow(data_efa), 
         rotate = "oblimin", diagonal = FALSE, 
         fm = "minrank") 
# Questionnaires
nfactors(Q_rho, n = 6, n.obs  = nrow(survey_data), 
         rotate = "oblimin", diagonal = FALSE,
         fm = "minrank") 
# Tasks
nfactors(T_rho, n = 6, n.obs  = nrow(task_data), 
         rotate = "oblimin", diagonal = FALSE,
         fm = "minrank") 

# Parallel Analysis
# All variables
fa.parallel(all_rho, n.obs = nrow(data_efa), fm = "minrank")
# cor.plot(all_rho)
# Questionnaires
fa.parallel(Q_rho, n.obs = nrow(survey_data), fm = "minrank")
# cor.plot(Q_rho)
# Tasks
fa.parallel(T_rho, n.obs = nrow(task_data), fm = "minrank")
# cor.plot(T_rho)

```

## Run the Factor Analysis with lavaan

Number of factors is determined with EBIC.

```{r EFA for Questionnaires (4 factors according to EBIC)}

# 4-factor model
f4 <- '
efa("efa")*f1 +
efa("efa")*f2 +
efa("efa")*f3 +
efa("efa")*f4 =~ AMean + CMean + EMean + ESMean + OMean + ICuriositySum + DCuriositySum + IH1Sum + IH2Sum + IH3Sum + IH4Sum + CloSum  +  CogSum + AOTSum + HEscore + HRscore + RPSum + sci_cur + sci_tru + sci_impo + sci_id + MatrixCorrectCount'

# Fit the model
efa_f4 <- cfa(model = f4,
              data = survey_data,
              rotation = "oblimin",
              estimator = "WLSMV",
              ordered = c("HEscore", "HRscore", "RPSum"))
summary(efa_f4, fit.measures = TRUE)
parameterEstimates(efa_f4)
fitmeasures(efa_f4)
# Looks like it's a just identified model? Fit measures are too good

# Look at loadings
loadings_4f <- inspect(efa_f4, what = "std")[["lambda"]]
fa.diagram(inspect(efa_f4, what = "std")[["lambda"]])
loadings_4f <- as.data.frame(loadings_4f)
round(fa.sort(loadings_4f), 2)
# Remove EMean, sci_tru, HEscore (no loadings >= .33)

survey_data_sub <- survey_data %>%
  select(-c("EMean", "HEscore", "IH1Sum"))  # removed EMean and HEscore first, and IH1sum second (here all at once)
f4_sub <- '
efa("efa")*f1 +
efa("efa")*f2 +
efa("efa")*f3 +
efa("efa")*f4 =~ AMean + CMean + ESMean + OMean + ICuriositySum + DCuriositySum + IH2Sum + IH3Sum + IH4Sum + CloSum  +  CogSum + AOTSum + HRscore + RPSum + sci_cur + sci_impo + sci_id + sci_tru + MatrixCorrectCount'

# Fit the model
efa_f4_sub <- cfa(model = f4_sub,
                  data = survey_data_sub,
                  rotation = "oblimin",
                  estimator = "WLSMV",
                  ordered = c("HRscore", "RPSum"))
summary(efa_f4_sub, fit.measures = TRUE)
parameterEstimates(efa_f4_sub)
fitmeasures(efa_f4_sub)
# Still looks not identified, look into degrees of freedom

loadings_4f_sub <- inspect(efa_f4_sub, what = "std")[["lambda"]]
fa.diagram(inspect(efa_f4_sub, what = "std")[["lambda"]])
loadings_4f_sub <- as.data.frame(loadings_4f_sub)
round(fa.sort(loadings_4f_sub), 2)


# Visualize
# 19 items, 4 factors
# Create layout matrix (23 nodes altogether)
y <- c(rep(c(-0.9, -0.8), length.out = 19), # items
       rep(0, 4)) # latents
x <- c(seq(from = -0.98, to = 0.98, length.out = 19), # items
       -0.75, -0.25, 0.25, 0.75)
coord <- matrix(data = c(x, y), nrow = 23, ncol = 2, byrow = FALSE)
semPaths(efa_f4_sub,
         what = "est",
         whatLabels = "no",
         as.expression = c("nodes", "edges"),
         theme = "colorblind", 
         curvePivot = TRUE,
         sizeMan = 3,
         sizeLat = 7,
         edge.label.cex = 1,
         reorder = FALSE,
         width = 8,
         height = 5,
         groups = "latents",
         borders = FALSE,
         residuals = FALSE,
         intercepts = FALSE,
         thresholds = FALSE,
         label.prop = .96,
         label.scale = TRUE,
         color = c(rep("#bfbfbf", 4)),
         layout = coord, 
         exoCov = FALSE,
         nodeLabels = c("AMean", "CMean", "ESMean", "OMean", "ICur", "DCur", "IH2", "IH3", "IH4", "Clo ", " Cog", "AOT", "HR", "RP", "sci_cur", "sci_tru", "sci_impo", "sci_id", "Mat_rea", "F1", "F2", "F3", "F4"),
         filename = "4f_pathdiagram",
         filetype = "pdf")
# Omit irrelevant edges??


# THIS DOES NOT WORK FOR US YET! Need to figure out why
# obtain factor loadings and add them to data
loading_mat <- as.matrix(fa.sort(loadings_4f_sub))
loading_mat <- loading_mat[, -ncol(loading_mat)]
# rownames(loading_mat) <-
# colnames(loading_mat) <- 

corrplot(loading_mat, method = "color",
         addCoef.col = NULL, 
         number.cex = .75,
         tl.col = "black", 
         cl.pos = "b",
         cl.length = 4
         )
# Make this nicer



```


```{r Extract Factor Scores (test)}

scores_4f <- lavPredict(efa_f4, method = "EBM")

```




# NOT RELEVANT (???)



## Run the Factor Analysis

Method is "minrank" minimum rank or MRFA. Recommended for polychoric/tetrachoric correlation matrices with Heywood cases https://www.tandfonline.com/doi/pdf/10.1080/10705511.2020.1735393 

The criteria used to determine whether to keep items and factors:
* if an item does not have one and only one salient loading (>=.33) *and* it has high complexity (>=2.0), it is removed
* if a factor has less than three items with primary (largest loading of that item) loadings on it, the factor is removed

### Number of Factors from EBIC (Questionnaires)

```{r Factor Analysis (Questionnaires)}

# Questionnaires only
# EBIC: 4 factors
EFA_Q4 <- fa(Q_rho, nfactors = 4, rotate = "oblimin",
             fm = "minrank", scores = "tenBerge") 
fa.sort(EFA_Q4)
# Items to be removed: HEscore, IH4Sum, IH1Sum* (items with no loading > .33)

survey_data_sub <- survey_data %>% 
  select(-c(c("HEscore", "IH4Sum", "IH1Sum")))
Q_rho_sub <- mixedCor(data = survey_data_sub, 
                      d = c("HRscore"),
                      c = c("AMean", "CMean", "EMean", "ESMean", "OMean", "ICuriositySum", "DCuriositySum", "IH2Sum", "IH3Sum", "CloSum", "CogSum", "AOTSum", "RPSum", "sci_cur", "sci_tru", "sci_impo", "sci_id", "MatrixCorrectCount"),
                      use = "pairwise.complete.obs",
                      method = "spearman")
Q_rho_sub <- Q_rho_sub$rho
EFA_Q4_sub <- fa(Q_rho_sub, nfactors = 4, rotate = "oblimin",
                 fm = "minrank", scores = "tenBerge") 
fa.sort(EFA_Q4_sub)

```

```{r Visualize the 4-Factor Solution}

# Bad visualization
fa.diagram(EFA_Q4_sub)

# Get factor loadings and add them to data

# THIS DOES NOT WORK FOR US YET! Need to figure out why
# obtain factor loadings and add them to data
# factor_scores <- as.data.frame(Ideol_EFA$scores)
# 
# combined_data <- add_column(
#   combined_data,
#   Ideol_Factor1_Politics = factor_scores$MR1,
#   Ideol_Factor2_Dogmatism = factor_scores$MR2,
#   Ideol_Factor3_Religion = factor_scores$MR3
#   )
# 
# corrplot(Ideol_EFA$loadings, method="color",
#          addCoef.col = "black", number.cex = .75,
#          tl.col = "black", cl.pos = "b",
#          cl.length=4)

```

### Number of Factors from EBIC (Rest)

```{r Run the Factor Analysis (EBIC), include = FALSE}

# All variables (not very relevant, previous analyses indicate that questionnaires and tasks should be analysed separately)
# EBIC: 6 factors
EFA_all6 <- fa(all_rho, nfactors = 6, rotate = "oblimin",
               fm = "minrank", scores = "tenBerge")
fa.sort(EFA_all6)
# Items to be removed (!= 1 primary salient loading AND complexity >= 2.0): OMean, sci_tru, SDGoRTlog, AMean, FlexSum, sci_impo
data_efa_sub <- data_efa %>% 
  select(-c("OMean", "sci_tru", "SDGoRTlog", "AMean", "FlexSum", "sci_impo"))
all_rho_sub <- mixedCor(data = data_efa_sub,
                        d = cat,
                        c = c("CMean", "EMean", "ESMean", "ICuriositySum", "DCuriositySum", "IH1Sum", "IH2Sum", "IH3Sum", "IH4Sum", "CloSum", "CogSum", "AOTSum", "RPSum", "sci_cur", "sci_id", "MatrixCorrectCount", "GNGdprime", "GNGbetalog", "MeanGoRT", "GlobalToLocalPrecedencelog", "GlobalToLocalInterferencelog", "NeckerTotalRatelog"))
all_rho_sub <- all_rho_sub$rho
EFA_all6_sub <- fa(all_rho_sub, nfactors = 6, rotate = "oblimin",
                  fm = "minrank", scores = "tenBerge")
fa.sort(EFA_all6_sub)
# Items to be removed: sci_id, HEscore, Necker, MeanGoRT, AOTSum

data_efa_sub2 <- data_efa_sub %>% 
  select(-c("sci_id", "HEscore", "NeckerTotalRatelog", "MeanGoRT", "AOTSum"))
all_rho_sub2 <- mixedCor(data = data_efa_sub2,
                         d = c("HRscore"),
                         c = c("CMean", "EMean", "ESMean", "ICuriositySum", "DCuriositySum", "IH1Sum", "IH2Sum", "IH3Sum", "IH4Sum", "CloSum", "CogSum", "RPSum", "sci_cur", "MatrixCorrectCount", "GNGdprime", "GNGbetalog", "GlobalToLocalPrecedencelog", "GlobalToLocalInterferencelog"))
all_rho_sub2 <- all_rho_sub2$rho
EFA_all6_sub2 <- fa(all_rho_sub2, nfactors = 6, rotate = "oblimin",
                   fm = "minrank", scores = "tenBerge")
fa.sort(EFA_all6_sub2)
# Items to be removed: IH2Sum

data_efa_sub3 <- data_efa_sub2 %>% 
  select(-c("IH2Sum"))
all_rho_sub3 <- mixedCor(data = data_efa_sub3,
                         d = c("HRscore"),
                         c = c("CMean", "EMean", "ESMean", "ICuriositySum", "DCuriositySum", "IH1Sum", "IH3Sum", "IH4Sum", "CloSum", "CogSum", "RPSum", "sci_cur", "MatrixCorrectCount", "GNGdprime", "GNGbetalog", "GlobalToLocalPrecedencelog", "GlobalToLocalInterferencelog"))
all_rho_sub3 <- all_rho_sub3$rho
EFA_all6_sub3 <- fa(all_rho_sub3, nfactors = 6, rotate = "oblimin",
                   fm = "minrank", scores = "tenBerge")
fa.sort(EFA_all6_sub3)
# Items to be removed: IH4Sum

data_efa_sub4 <- data_efa_sub3 %>% 
  select(-c("IH4Sum"))
all_rho_sub4 <- mixedCor(data = data_efa_sub4,
                         d = c("HRscore"),
                         c = c("CMean", "EMean", "ESMean", "ICuriositySum", "DCuriositySum", "IH1Sum", "IH3Sum", "CloSum", "CogSum", "RPSum", "sci_cur", "MatrixCorrectCount", "GNGdprime", "GNGbetalog", "GlobalToLocalPrecedencelog", "GlobalToLocalInterferencelog"))
all_rho_sub4 <- all_rho_sub4$rho
EFA_all6_sub4 <- fa(all_rho_sub4, nfactors = 6, rotate = "oblimin",
                   fm = "minrank", scores = "tenBerge")
fa.sort(EFA_all6_sub4)
# Items look ok, but three factors have less than three primary loadings -> let's try with fewer factors

EFA_all5 <- fa(all_rho_sub4, nfactors = 5, rotate = "oblimin",
               fm = "minrank", scores = "tenBerge")
fa.sort(EFA_all5)
EFA_all4 <- fa(all_rho_sub4, nfactors = 4, rotate = "oblimin",
               fm = "minrank", scores = "tenBerge")
fa.sort(EFA_all4)
EFA_all3 <- fa(all_rho_sub4, nfactors = 3, rotate = "oblimin",
               fm = "minrank", scores = "tenBerge")
fa.sort(EFA_all3)


# Questionnaires only
# EBIC: 4 factors
EFA_Q4 <- fa(Q_rho, nfactors = 4, rotate = "oblimin",
             fm = "minrank", scores = "tenBerge") 
fa.sort(EFA_Q4)
# Items to be removed (!= 1 primary salient loading AND complexity >= 2.0): HEscore, sci_impo, DCuriositySum, IH4Mean, sci_tru

survey_data_sub <- survey_data %>% 
  select(-c(c("HEscore", "sci_impo", "DCuriositySum", "IH4Sum", "sci_tru")))
Q_rho_sub <- mixedCor(data = survey_data_sub, 
                      d = c("HRscore"),
                      c = c("AMean", "CMean", "EMean", "ESMean", "OMean",  "ICuriositySum", "IH1Sum", "IH2Sum", "IH3Sum", "CloSum", "CogSum", "AOTSum", "RPSum", "sci_cur", "sci_id", "MatrixCorrectCount"),
                      use = "pairwise.complete.obs",
                      method = "spearman")
Q_rho_sub <- Q_rho_sub$rho
EFA_Q4_sub <- fa(Q_rho_sub, nfactors = 4, rotate = "oblimin",
                 fm = "minrank", scores = "tenBerge") 
fa.sort(EFA_Q4_sub)
# Items to be removed: EMean, IH2Sum

survey_data_sub2 <- survey_data_sub %>% 
  select(-c("EMean", "IH2Sum"))
Q_rho_sub2 <- mixedCor(data = survey_data_sub2, 
                       d = c("HRscore"),
                       c = c("AMean", "CMean", "ESMean", "OMean",  "ICuriositySum", "IH1Sum", "IH3Sum", "CloSum", "CogSum", "AOTSum", "RPSum", "sci_cur", "sci_id", "MatrixCorrectCount"),
                       use = "pairwise.complete.obs",
                       method = "spearman")
Q_rho_sub2 <- Q_rho_sub2$rho
EFA_Q4_sub2 <- fa(Q_rho_sub2, nfactors = 4, rotate = "oblimin",
                  fm = "minrank", scores = "tenBerge") 
fa.sort(EFA_Q4_sub2)
# Items to be removed: AOTSum

survey_data_sub3 <- survey_data_sub2 %>% 
  select(-c("AOTSum"))
Q_rho_sub3 <- mixedCor(data = survey_data_sub3, 
                       d = c("HRscore"),
                       c = c("AMean", "CMean", "ESMean", "OMean",  "ICuriositySum", "IH1Sum", "IH3Sum", "CloSum", "CogSum", "RPSum", "sci_cur", "sci_id", "MatrixCorrectCount"),
                       use = "pairwise.complete.obs",
                       method = "spearman")
Q_rho_sub3 <- Q_rho_sub3$rho
EFA_Q4_sub3 <- fa(Q_rho_sub3, nfactors = 4, rotate = "oblimin",
                  fm = "minrank", scores = "tenBerge") 
fa.sort(EFA_Q4_sub3)
# All items ok now, but the 4th factor has less than three primary loadings -> run a model with only three factors
EFA_Q3_sub3 <- fa(Q_rho_sub3, nfactors = 3, rotate = "oblimin",
                  fm = "minrank", scores = "tenBerge") 
fa.sort(EFA_Q3_sub3)

# Tasks only
EFA_T3 <- fa(T_rho, nfactors = 3, rotate = "oblimin",
             fm = "minrank", scores = "tenBerge")
fa.sort(EFA_T3)

```

### Number of Factors from PA + Scree Plot

```{r Run the Factor Analysis (PA + Scree Plot)}

# All variables
# PA + scree plot: 8 factors
EFA_all8 <- fa(all_rho, nfactors = 8, rotate = "oblimin",
               fm = "minrank", scores = "tenBerge")
fa.sort(EFA_all8)
# # Items to be removed (!= 1 primary salient loading AND complexity >= 2.0): CogSum, sci_id, IH2Sum
# 
# data_efa_sub5 <- data_efa %>% 
#   select(-c(CogSum, sci_id, IH2Sum))
# all_rho_sub5 <- mixedCor(data = data_efa_sub5, 
#                          d = cat,
#                          c = c("AMean", "CMean", "EMean", "ESMean", "OMean", "ICuriositySum", "DCuriositySum", "IH1Sum", "IH3Sum", "IH4Sum", "CloSum", "AOTSum", "RPSum", "sci_cur", "sci_tru", "sci_impo", "MatrixCorrectCount", "GNGdprime", "GNGbetalog", "MeanGoRT", "SDGoRTlog", "GlobalToLocalPrecedencelog", "GlobalToLocalInterferencelog", "NeckerTotalRatelog", "FlexSum"))
# all_rho_sub5 <- all_rho_sub5$rho
# EFA_all8_sub5 <- fa(all_rho_sub5, nfactors = 8, rotate = "oblimin",
#                     fm = "minrank", scores = "tenBerge")
# fa.sort(EFA_all8_sub5)
# # Items to be removed: sci_cur, sci_impo, NeckerTotalRate, RPSum, IH3Sum
# 
# data_efa_sub6 <- data_efa_sub5 %>% 
#   select(-c(sci_cur, sci_impo, NeckerTotalRatelog, RPSum, IH3Sum))
# all_rho_sub6 <- mixedCor(data = data_efa_sub6, 
#                          d = cat,
#                          c = c("AMean", "CMean", "EMean", "ESMean", "OMean", "ICuriositySum", "DCuriositySum", "IH1Sum", "IH4Sum", "CloSum", "AOTSum", "sci_tru", "MatrixCorrectCount", "GNGdprime", "GNGbetalog", "MeanGoRT", "SDGoRTlog", "GlobalToLocalPrecedencelog", "GlobalToLocalInterferencelog", "FlexSum"))
# all_rho_sub6 <- all_rho_sub6$rho
# EFA_all8_sub6 <- fa(all_rho_sub6, nfactors = 8, rotate = "oblimin",
#                     fm = "minrank", scores = "tenBerge")
# fa.sort(EFA_all8_sub6)
# # Items to be removed: IH4Sum, HEscore, IH1Sum
# # NOTE: I will stop this here for now. Point is that with the 8-factor solution and the exclusion criteria we end up removing so many items that we end up with factors with too few items loading onto them.

# Questionnaires only
# PA + scree plot: 5 factors
EFA_Q5 <- fa(Q_rho, nfactors = 5, rotate = "oblimin",
             fm = "minrank", scores = "tenBerge") 
fa.sort(EFA_Q5)
# Items to be removed (!= 1 primary salient loading AND complexity >= 2.0): IH2Sum, EMean, sci_id, ESMean, HEscore

survey_data_sub4 <- survey_data %>% 
  select(-c(IH2Sum, EMean, sci_id, ESMean, HEscore))
Q_rho_sub4 <- mixedCor(data = survey_data_sub4,
                       d = c("HRscore"),
                       c = c("AMean", "CMean", "OMean", "ICuriositySum", "DCuriositySum", "IH1Sum", "IH3Sum", "IH4Sum", "CloSum", "CogSum", "AOTSum", "RPSum", "sci_cur", "sci_tru", "sci_impo", "MatrixCorrectCount"),
                       use = "pairwise.complete.obs",
                       method = "spearman")
Q_rho_sub4 <- Q_rho_sub4$rho
EFA_Q5_sub4 <- fa(Q_rho_sub4, nfactors = 5, rotate = "oblimin",
                  fm = "minrank", scores = "tenBerge") 
fa.sort(EFA_Q5_sub4)
# Items to be removed: sci_cur, IH4Sum

survey_data_sub5 <- survey_data_sub4 %>% 
  select(-c(IH4Sum, sci_cur))
Q_rho_sub5 <- mixedCor(data = survey_data_sub5,
                       d = c("HRscore"),
                       c = c("AMean", "CMean", "OMean", "ICuriositySum", "DCuriositySum", "IH1Sum", "IH3Sum", "CloSum", "CogSum", "AOTSum", "RPSum", "sci_tru", "sci_impo", "MatrixCorrectCount"),
                       use = "pairwise.complete.obs",
                       method = "spearman")
Q_rho_sub5 <- Q_rho_sub5$rho
EFA_Q5_sub5 <- fa(Q_rho_sub5, nfactors = 5, rotate = "oblimin",
                  fm = "minrank", scores = "tenBerge") 
fa.sort(EFA_Q5_sub5)
# Items to be removed: sci_impo
# This is problematic though, because then we have 14 items for 3 factors, resulting in too few items per factor; I will not continue with the 5-factor model.

# Tasks
# PA + scree plot: 3 factors
EFA_T3 <- fa(T_rho, nfactors = 3, rotate = "oblimin",
             fm = "minrank", scores = "tenBerge")
fa.sort(EFA_T3)

```

## Extract Factor Scores

```{r Extract Factor Scores}

## THESE MIGHT NOT BE GOOD?

# Trying this out with the 4-factor survey solution (n of factors from EBIC)
survey_4F_scores <- factor.scores(x = survey_data,  # the data
                                  f = EFA_Q4,  # the factor solution
                                  method = "tenBerge",  # the method; "the ten Berge method focuses on matching the factor score correlation with correlations among factors themselves, while still maintaining factor score determinacy" (https://osf.io/zcsnv/download/?format=pdf)
                                  rho = Q_rho)$scores  # specify that we used polychoric correlations by specifying the rho matrix

# And also with the 5-factor solution
survey_5F_scores <- factor.scores(x = survey_data, f = EFA_Q5,
                                  method = "tenBerge", rho = Q_rho)$scores

# Visualize the factor scores
# 4 factors
par(mfrow = c(2, 2))
for (i in 1:ncol(survey_4F_scores)) {
  hist(survey_4F_scores[, i],
       main = colnames(survey_4F_scores)[i],
       col = sample(colors(), 1),
       breaks = 30,
       xlab = "")
}
# 5 factors
par(mfrow = c(2, 3))
for (i in 1:ncol(survey_5F_scores)) {
  hist(survey_5F_scores[, i],
       main = colnames(survey_5F_scores)[i],
       col = sample(colors(), 1),
       breaks = 30,
       xlab = "")
}

```



```{r Extracting factor scores, include = FALSE}

# Extracting factor scores per case (ppt), for EFA_6f solution
# I'm not sure that this is right, there is some info in factor.scores about polychoric data and I am not sure how the factor scores per ppt are calculated exactly, whether the HR score binary data makes a difference
# I'm also not sure of all the arguments, particularly rho
# taking rho out slightly changes if you plot bergescores$scores
# it's also possible to impute missing data as there is a lot, use "median" or "mean"
# the four combinations of these arguments give basically the same resulting plot but should check them
# the plot is MRFA1 against MRFA6, theyr'e the first 2 columns in $scores
# bergescores <- factor.scores(Q_new, EFA_6f, method = c("tenBerge"),
                             # rho = Q_rho, impute = "median")
# to view as a dataframe
# View(bergescores$scores)

```
