---
title: "Exploratory Factor Analysis"
author: "Milla Pihlajamaki and Caitlin Dawson"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_notebook
---

The data are normal (skew<=1). The correlation matrix uses polychoric correlations (treats data as ordinal).

```{r Load Packages}

# library(ISLR) 
# library(glmnet) 
# library(dplyr) 
# library(tidyr)
# library(tidyverse)
# library(caret)
library(glmnet)
# library(tidyverse)
# library(tidyselect)
# library(psych)
# library(lavaan)
# library(dplyr)
# library(ggplot2)
# library(psychTools)
# library(GPArotation)
# library(devtools)
# library(apaTables)
# library(semPlot)
# library(sjPlot)
# library(glasso)

# Create task- and survey-only sets
survey_data <- data_efa[, q_var]
task_data <- data_efa[, t_var_log]

```


# Cross-method prediction

10-fold cross validated ridge regression to predict survey and task variables *within* and *between* measure types following Eisenberg et al. Intended to help decide whether to factor tasks and surveys together or separately, and also to see if some variables should be left out. 

Taking each DV one by one and trying to predict it using its own measurement category (leaving it out) and by the other measurement category. Each DV is the target; the predictors are the other variables in the category. 

4 distributions of predictions: tasks-by-tasks, tasks-by-surveys, surveys-by-tasks, and surveys-by-surveys -> extract the R^2 values and create four distributions

"Prediction success was assessed by 10-fold cross-validated ridge regression using the RidgeCV function from scikit-learn with default parameters (10). Almost all DVs were able to be predicted to some degree by their respective measurement category (mean task-by-tasks R2 mean survey-by-surveys R2  = .45). In contrast, cross-measurement prediction failed: surveys were unable to predict task DVs and vice versa (mean task-by-surveys R2 = -.13, mean survey-by-tasks R2 = -.29). Note that R2 values below 0 are possible when employing  cross-validation and indicate no discoverable linear relationship. The entire distribution of R2  values for each of these predictions is shown is Figure S2b."

Ridge regression differs from lasso regression in that it can reduce coefficients down to near zero but always includes all predictors. Lasso regression can be used to reduce dimensionality as coefficients can be zero. Elastic net regression combines the two. 

Ridge regression works better than regular linear regression when you have many variables in a model and also collinearity. It is a form of penalized regression that adds a constraint. The constraint constant is defined by *lambda*, which shrinks coefficients. A lambda of 0 is a regular linear regression, and a too large lambda will shrink the coefficients too much to make a usable model. We can identify the best value of lambda by using cross-validation. 

Instead of splitting data into a training and test set, cross-validation trains and tests the model on subsets of the full dataset, creating a distribution. Here we just use it to compare various values of lambda in order to choose the best one for each prediction model. 

The aim of this section is to create a model for predicting each DV in the 4 distributions described above. Ridge regression is used because of the type of dataset and because we want to retain all variables when checking the cross- and between-measure type predictions. 

The question here is, can X variable be predicted equally well by surveys vs. tasks? After we get these R2 for each model (there will be 4 models per variable), we can take the mean R2 for each of the 4 distributions and make a plot as in Eisenberg to examine whether the distributions are similar. This helps us decide whether task and survey variables should be clustered or factored together.  

```{r Ridge Regression (Cross-Method Prediction)}

# Survey x survey 
# NOTE: not sure if the binary items are an issue? not sure if this is the case when they are predictors but I would assume when they are being predicted? I will omit them here just in case
q_var_sub <- q_var[-c(which(q_var == "HEscore"), which(q_var == "HRscore"))]
survey_data_sub <- data_efa[, q_var_sub]

predicted_SS <- c()
predicted_type_SS <- rep("survey", length(q_var_sub))
predictors_SS <- rep("surveys", length(q_var_sub))
R2_SS <- c()

for (i in 1:length(q_var_sub)) {
  
  # Save x and y
  x_var <- as.matrix(survey_data_sub[, -which(colnames(survey_data_sub) == q_var_sub[i])])
  y_var <- survey_data_sub[, q_var_sub[i]]
  predicted_SS <- c(predicted_SS, q_var_sub[i])
  
  # Fit the ridge regression model
  model <- glmnet(x = x_var, y = y_var, alpha = 0)
  
  # Perform k-fold cross-validation to find optimal lambda value
  cv_model <- cv.glmnet(x = x_var, y = y_var, alpha = 0)
  
  # Find optimal lambda
  best_lambda <- cv_model$lambda.min
  
  # Find coefficients for the best model
  best_model <- glmnet(x = x_var, y = y_var, alpha = 0, lambda = best_lambda)
  
  # Use fitted best model to make predictions (NOTE: not 100% sure about this)
  y_predicted <- predict(object = best_model, s = best_lambda, newx = x_var)
  
  # Find SST and SSE
  sst <- sum((y_var - mean(y_var))^2)
  sse <- sum((y_predicted - y_var)^2)
  
  # Find R-squared of the model on the training data
  rsq <- 1 - sse/sst
  R2_SS <- c(R2_SS, rsq)
  
}

SxS <- data.frame(predicted = predicted_SS, predictors = predictors_SS, 
                  predicted_type = predicted_type_SS, R2 = R2_SS)

# Task x task
predicted_TT <- c()
predicted_type_TT <- rep("task", length(t_var_log))
predictors_TT <- rep("tasks", length(t_var_log))
R2_TT <- c()
for (i in 1:length(t_var_log)) {
  x_var <- as.matrix(task_data[, -which(colnames(task_data) == t_var_log[i])])
  y_var <- task_data[, t_var_log[i]]
  predicted_TT <- c(predicted_TT, t_var_log[i])
  model <- glmnet(x = x_var, y = y_var, alpha = 0)
  cv_model <- cv.glmnet(x = x_var, y = y_var, alpha = 0)
  best_lambda <- cv_model$lambda.min
  best_model <- glmnet(x = x_var, y = y_var, alpha = 0, lambda = best_lambda)
  y_predicted <- predict(object = best_model, s = best_lambda, newx = x_var)
  sst <- sum((y_var - mean(y_var))^2)
  sse <- sum((y_predicted - y_var)^2)
  rsq <- 1 - sse/sst
  R2_TT <- c(R2_TT, rsq)
}
TxT <- data.frame(predicted = predicted_TT, predictors = predictors_TT, 
                  predicted_type = predicted_type_TT, R2 = R2_TT)

# Survey x task (surveys predicted by tasks)
predicted_ST <- c()
predicted_type_ST <- rep("survey", length(q_var_sub))
predictors_ST <- rep("tasks", length(q_var_sub))
R2_ST <- c()
for (i in 1:length(q_var_sub)) {
  x_var <- as.matrix(task_data)
  y_var <- survey_data_sub[, q_var_sub[i]]
  predicted_ST <- c(predicted_ST, q_var_sub[i])
  model <- glmnet(x = x_var, y = y_var, alpha = 0)
  cv_model <- cv.glmnet(x = x_var, y = y_var, alpha = 0)
  best_lambda <- cv_model$lambda.min
  best_model <- glmnet(x = x_var, y = y_var, alpha = 0, lambda = best_lambda)
  y_predicted <- predict(object = best_model, s = best_lambda, newx = x_var)
  sst <- sum((y_var - mean(y_var))^2)
  sse <- sum((y_predicted - y_var)^2)
  rsq <- 1 - sse/sst
  R2_ST <- c(R2_ST, rsq)
}
SxT <- data.frame(predicted = predicted_ST, predictors = predictors_ST, 
                  predicted_type = predicted_type_ST, R2 = R2_ST)

# Tasks x surveys (tasks predicted by surveys)
predicted_TS <- c()
predicted_type_TS <- rep("task", length(t_var_log))
predictors_TS <- rep("surveys", length(t_var_log))
R2_TS <- c()
for (i in 1:length(t_var_log)) {
  x_var <- as.matrix(survey_data_sub)
  y_var <- task_data[, t_var_log[i]]
  predicted_TS <- c(predicted_TS, t_var_log[i])
  model <- glmnet(x = x_var, y = y_var, alpha = 0)
  cv_model <- cv.glmnet(x = x_var, y = y_var, alpha = 0)
  best_lambda <- cv_model$lambda.min
  best_model <- glmnet(x = x_var, y = y_var, alpha = 0, lambda = best_lambda)
  y_predicted <- predict(object = best_model, s = best_lambda, newx = x_var)
  sst <- sum((y_var - mean(y_var))^2)
  sse <- sum((y_predicted - y_var)^2)
  rsq <- 1 - sse/sst
  R2_TS <- c(R2_TS, rsq)
}
TxS <- data.frame(predicted = predicted_TS, predictors = predictors_TS, 
                  predicted_type = predicted_type_TS, R2 = R2_TS)

# Bind the dataframes
R2_crossmethod <- rbind(SxS, TxT, SxT, TxS)

# Describe R^2 per group
R2_crossmethod <- R2_crossmethod %>% 
  mutate(format = case_when((predicted_type == "task" & predictors == "tasks")  ~ "TxT",
                            (predicted_type == "survey" & predictors == "surveys")  ~ "SxS",
                            (predicted_type == "task" & predictors == "surveys")  ~ "TxS",
                            (predicted_type == "survey" & predictors == "tasks")  ~ "SxT"))
R2_crossmethod$format <- as.factor(R2_crossmethod$format)
R2_crossmethod %>% 
  group_by(format) %>% 
  summarise(mean(R2), sd(R2))

# Visualize R^2 per group
# Same plot
ggplot(R2_crossmethod, aes(x = R2)) +
  geom_histogram(aes(color = format, fill = format),
                 position = "identity", bins = 50) +
  scale_color_manual(values = c("#9AC4F8", "#99EDCC", "#CB958E", "#E36588")) +
  scale_fill_manual(values = c("#9AC4F8", "#99EDCC", "#CB958E", "#E36588")) +
  theme_minimal()
# Separate plots
par(mfrow = c(2, 2))
formats <- c("SxS", "SxT", "TxT", "TxS")
for (i in 1:4) {
  hist(R2_crossmethod[which(R2_crossmethod$format == formats[i]), "R2"],
       main = paste0("R2 for ", formats[i]),
       col = sample(colors(), 1),
       breaks = 20,
       xlab = "")
}

```

# Exploratory Factor Analysis

This section includes three series of EFAs: all variables, surveys only, and tasks only. In the preregistration, we promised to start with these. 

Correlations are mixed because of the binary variables from the heuristics task

## Create Correlation Matrices

```{r Create Correlation Matrices, include = FALSE}

# A general note: we probably need to justify putting a bunch of task and self-report items in the same factor analysis / at least mention it !
# https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3268653/

# Descriptives
describe(data_efa)

# Create character strings with different classes of variables
con <- c("AMean", "CMean", "EMean", "ESMean", "OMean", "ICuriositySum", "DCuriositySum", "IH1Sum", "IH2Sum", "IH3Sum", "IH4Sum", "CloSum", "CogSum", "AOTSum", "RPSum", "sci_cur", "sci_tru", "sci_impo", "sci_id", "MatrixCorrectCount", "GNGdprime", "GNGbetalog", "MeanGoRT", "SDGoRTlog", "GlobalToLocalPrecedencelog", "GlobalToLocalInterferencelog", "NeckerTotalRatelog", "FlexSum")
q_con <- c("AMean", "CMean", "EMean", "ESMean", "OMean", "ICuriositySum", "DCuriositySum", "IH1Sum", "IH2Sum", "IH3Sum", "IH4Sum", "CloSum", "CogSum", "AOTSum", "RPSum", "sci_cur", "sci_tru", "sci_impo", "sci_id", "MatrixCorrectCount")
cat <- c("HEscore", "HRscore")

# Create a correlation matrix (polychoric/mixed correlations)
all_cor <- mixedCor(data = data_efa, d = cat, c = con, 
                    use = "pairwise.complete.obs",
                    method = "spearman")
Q_cor <- mixedCor(data = survey_data, d = cat, c = q_con,
                  use = "pairwise.complete.obs",
                  method = "spearman")
T_cor <- mixedCor(data = task_data, 
                  use = "pairwise.complete.obs",
                  method = "pearson")
# NOTE: Caitlin had specified other arguments as well but I am not sure how necessary there are?

# Store correlation matrix
all_rho <- all_cor$rho
Q_rho <- Q_cor$rho
T_rho <- T_cor$rho

# Mean correlation across items
mean(abs(all_rho[lower.tri(all_rho)]))
mean(abs(Q_rho[lower.tri(Q_rho)]))
mean(abs(T_rho[lower.tri(T_rho)]))

# Histogram of correlations
hist(abs(all_rho[lower.tri(all_rho)]), 
     breaks = 20, col = sample(colors(), 1))
hist(abs(Q_rho[lower.tri(Q_rho)]), 
     breaks = 20, col = sample(colors(), 1))
hist(abs(T_rho[lower.tri(T_rho)]), 
     breaks = 20, col = sample(colors(), 1))

```

## Factorability

```{r Factorability, include = FALSE}

# Bartlett test (p<.05 indicates data are suitable for structure detection)
cortest.bartlett(all_rho, n = nrow(data_efa))
cortest.bartlett(Q_rho, n = nrow(survey_data))
cortest.bartlett(T_rho, n = nrow(task_data))

# Kaiser-Meyer-Olkin measure (>.7 is good)
KMO(r = all_rho)
KMO(r = Q_rho)
KMO(r = T_rho)
# NOTE: KMO is not very good for a lot of items?

```

## Identify number of factors to extract

For now I used EBIC as the main criterion following Eisenberg et al. Zmigrod et al. used scree plots and PA.

```{r Number of factors, include = FALSE}

# Scree plots
# All variables
plot(eigen(all_rho)$values, type = "b", pch = 20, col = "red",
     main = "Scree Plot", ylab = "Eigenvalues",
     xlab = "No. Factors")
# Questionnaire variables
plot(eigen(Q_rho)$values, type = "b", pch = 20, col = "red",
     main = "Scree Plot", ylab = "Eigenvalues",
     xlab = "No. Factors")
# Task variables
plot(eigen(T_rho)$values, type = "b", pch = 20, col = "red",
     main = "Scree Plot", ylab = "Eigenvalues",
     xlab = "No. Factors")

# NFactors
# All variables
nfactors(all_rho, n = 6, n.obs  = nrow(data_efa), 
         rotate = "oblimin", diagonal = FALSE, 
         fm = "minrank") 
# Questionnaires
nfactors(Q_rho, n = 6, n.obs  = nrow(survey_data), 
         rotate = "oblimin", diagonal = FALSE,
         fm = "minrank") 
# Tasks
nfactors(T_rho, n = 6, n.obs  = nrow(task_data), 
         rotate = "oblimin", diagonal = FALSE,
         fm = "minrank") 

# Parallel Analysis
# All variables
fa.parallel(all_rho, n.obs = nrow(data_efa), fm = "minrank")
# cor.plot(all_rho)
# Questionnaires
fa.parallel(Q_rho, n.obs = nrow(survey_data), fm = "minrank")
# cor.plot(Q_rho)
# Tasks
fa.parallel(T_rho, n.obs = nrow(task_data), fm = "minrank")
# cor.plot(T_rho)

```

## Run the Factor Analysis

Method is "minrank" minimum rank or MRFA. Recommended for polychoric/tetrachoric correlation matrices with Heywood cases https://www.tandfonline.com/doi/pdf/10.1080/10705511.2020.1735393 

The criteria used to determine whether to keep items and factors:
* if an item does not have one and only one salient loading (>=.33) *and* it has high complexity (>=2.0), it is removed
* if a factor has less than three items with primary (largest loading of that item) loadings on it, the factor is removed

### Number of Factors from EBIC

```{r Run the Factor Analysis (EBIC), include = FALSE}

# All variables (not very relevant, previous analyses indicate that questionnaires and tasks should be analysed separately)
# EBIC: 6 factors
EFA_all6 <- fa(all_rho, nfactors = 6, rotate = "oblimin",
               fm = "minrank", scores = "tenBerge")
fa.sort(EFA_all6)
# Items to be removed (!= 1 primary salient loading AND complexity >= 2.0): OMean, sci_tru, SDGoRTlog, AMean, FlexSum, sci_impo
data_efa_sub <- data_efa %>% 
  select(-c("OMean", "sci_tru", "SDGoRTlog", "AMean", "FlexSum", "sci_impo"))
all_rho_sub <- mixedCor(data = data_efa_sub,
                        d = cat,
                        c = c("CMean", "EMean", "ESMean", "ICuriositySum", "DCuriositySum", "IH1Sum", "IH2Sum", "IH3Sum", "IH4Sum", "CloSum", "CogSum", "AOTSum", "RPSum", "sci_cur", "sci_id", "MatrixCorrectCount", "GNGdprime", "GNGbetalog", "MeanGoRT", "GlobalToLocalPrecedencelog", "GlobalToLocalInterferencelog", "NeckerTotalRatelog"))
all_rho_sub <- all_rho_sub$rho
EFA_all6_sub <- fa(all_rho_sub, nfactors = 6, rotate = "oblimin",
                  fm = "minrank", scores = "tenBerge")
fa.sort(EFA_all6_sub)
# Items to be removed: sci_id, HEscore, Necker, MeanGoRT, AOTSum

data_efa_sub2 <- data_efa_sub %>% 
  select(-c("sci_id", "HEscore", "NeckerTotalRatelog", "MeanGoRT", "AOTSum"))
all_rho_sub2 <- mixedCor(data = data_efa_sub2,
                         d = c("HRscore"),
                         c = c("CMean", "EMean", "ESMean", "ICuriositySum", "DCuriositySum", "IH1Sum", "IH2Sum", "IH3Sum", "IH4Sum", "CloSum", "CogSum", "RPSum", "sci_cur", "MatrixCorrectCount", "GNGdprime", "GNGbetalog", "GlobalToLocalPrecedencelog", "GlobalToLocalInterferencelog"))
all_rho_sub2 <- all_rho_sub2$rho
EFA_all6_sub2 <- fa(all_rho_sub2, nfactors = 6, rotate = "oblimin",
                   fm = "minrank", scores = "tenBerge")
fa.sort(EFA_all6_sub2)
# Items to be removed: IH2Sum

data_efa_sub3 <- data_efa_sub2 %>% 
  select(-c("IH2Sum"))
all_rho_sub3 <- mixedCor(data = data_efa_sub3,
                         d = c("HRscore"),
                         c = c("CMean", "EMean", "ESMean", "ICuriositySum", "DCuriositySum", "IH1Sum", "IH3Sum", "IH4Sum", "CloSum", "CogSum", "RPSum", "sci_cur", "MatrixCorrectCount", "GNGdprime", "GNGbetalog", "GlobalToLocalPrecedencelog", "GlobalToLocalInterferencelog"))
all_rho_sub3 <- all_rho_sub3$rho
EFA_all6_sub3 <- fa(all_rho_sub3, nfactors = 6, rotate = "oblimin",
                   fm = "minrank", scores = "tenBerge")
fa.sort(EFA_all6_sub3)
# Items to be removed: IH4Sum

data_efa_sub4 <- data_efa_sub3 %>% 
  select(-c("IH4Sum"))
all_rho_sub4 <- mixedCor(data = data_efa_sub4,
                         d = c("HRscore"),
                         c = c("CMean", "EMean", "ESMean", "ICuriositySum", "DCuriositySum", "IH1Sum", "IH3Sum", "CloSum", "CogSum", "RPSum", "sci_cur", "MatrixCorrectCount", "GNGdprime", "GNGbetalog", "GlobalToLocalPrecedencelog", "GlobalToLocalInterferencelog"))
all_rho_sub4 <- all_rho_sub4$rho
EFA_all6_sub4 <- fa(all_rho_sub4, nfactors = 6, rotate = "oblimin",
                   fm = "minrank", scores = "tenBerge")
fa.sort(EFA_all6_sub4)
# Items look ok, but three factors have less than three primary loadings -> let's try with fewer factors

EFA_all5 <- fa(all_rho_sub4, nfactors = 5, rotate = "oblimin",
               fm = "minrank", scores = "tenBerge")
fa.sort(EFA_all5)
EFA_all4 <- fa(all_rho_sub4, nfactors = 4, rotate = "oblimin",
               fm = "minrank", scores = "tenBerge")
fa.sort(EFA_all4)
EFA_all3 <- fa(all_rho_sub4, nfactors = 3, rotate = "oblimin",
               fm = "minrank", scores = "tenBerge")
fa.sort(EFA_all3)


# Questionnaires only
# EBIC: 4 factors
EFA_Q4 <- fa(Q_rho, nfactors = 4, rotate = "oblimin",
             fm = "minrank", scores = "tenBerge") 
fa.sort(EFA_Q4)
# Items to be removed (!= 1 primary salient loading AND complexity >= 2.0): HEscore, sci_impo, DCuriositySum, IH4Mean, sci_tru

survey_data_sub <- survey_data %>% 
  select(-c(c("HEscore", "sci_impo", "DCuriositySum", "IH4Sum", "sci_tru")))
Q_rho_sub <- mixedCor(data = survey_data_sub, 
                      d = c("HRscore"),
                      c = c("AMean", "CMean", "EMean", "ESMean", "OMean",  "ICuriositySum", "IH1Sum", "IH2Sum", "IH3Sum", "CloSum", "CogSum", "AOTSum", "RPSum", "sci_cur", "sci_id", "MatrixCorrectCount"),
                      use = "pairwise.complete.obs",
                      method = "spearman")
Q_rho_sub <- Q_rho_sub$rho
EFA_Q4_sub <- fa(Q_rho_sub, nfactors = 4, rotate = "oblimin",
                 fm = "minrank", scores = "tenBerge") 
fa.sort(EFA_Q4_sub)
# Items to be removed: EMean, IH2Sum

survey_data_sub2 <- survey_data_sub %>% 
  select(-c("EMean", "IH2Sum"))
Q_rho_sub2 <- mixedCor(data = survey_data_sub2, 
                       d = c("HRscore"),
                       c = c("AMean", "CMean", "ESMean", "OMean",  "ICuriositySum", "IH1Sum", "IH3Sum", "CloSum", "CogSum", "AOTSum", "RPSum", "sci_cur", "sci_id", "MatrixCorrectCount"),
                       use = "pairwise.complete.obs",
                       method = "spearman")
Q_rho_sub2 <- Q_rho_sub2$rho
EFA_Q4_sub2 <- fa(Q_rho_sub2, nfactors = 4, rotate = "oblimin",
                  fm = "minrank", scores = "tenBerge") 
fa.sort(EFA_Q4_sub2)
# Items to be removed: AOTSum

survey_data_sub3 <- survey_data_sub2 %>% 
  select(-c("AOTSum"))
Q_rho_sub3 <- mixedCor(data = survey_data_sub3, 
                       d = c("HRscore"),
                       c = c("AMean", "CMean", "ESMean", "OMean",  "ICuriositySum", "IH1Sum", "IH3Sum", "CloSum", "CogSum", "RPSum", "sci_cur", "sci_id", "MatrixCorrectCount"),
                       use = "pairwise.complete.obs",
                       method = "spearman")
Q_rho_sub3 <- Q_rho_sub3$rho
EFA_Q4_sub3 <- fa(Q_rho_sub3, nfactors = 4, rotate = "oblimin",
                  fm = "minrank", scores = "tenBerge") 
fa.sort(EFA_Q4_sub3)
# All items ok now, but the 4th factor has less than three primary loadings -> run a model with only three factors
EFA_Q3_sub3 <- fa(Q_rho_sub3, nfactors = 3, rotate = "oblimin",
                  fm = "minrank", scores = "tenBerge") 
fa.sort(EFA_Q3_sub3)

# Tasks only
EFA_T3 <- fa(T_rho, nfactors = 3, rotate = "oblimin",
             fm = "minrank", scores = "tenBerge")
fa.sort(EFA_T3)

```

### Number of Factors from PA + Scree Plot

```{r Run the Factor Analysis (PA + Scree Plot)}

# All variables
# PA + scree plot: 8 factors
EFA_all8 <- fa(all_rho, nfactors = 8, rotate = "oblimin",
               fm = "minrank", scores = "tenBerge")
fa.sort(EFA_all8)
# # Items to be removed (!= 1 primary salient loading AND complexity >= 2.0): CogSum, sci_id, IH2Sum
# 
# data_efa_sub5 <- data_efa %>% 
#   select(-c(CogSum, sci_id, IH2Sum))
# all_rho_sub5 <- mixedCor(data = data_efa_sub5, 
#                          d = cat,
#                          c = c("AMean", "CMean", "EMean", "ESMean", "OMean", "ICuriositySum", "DCuriositySum", "IH1Sum", "IH3Sum", "IH4Sum", "CloSum", "AOTSum", "RPSum", "sci_cur", "sci_tru", "sci_impo", "MatrixCorrectCount", "GNGdprime", "GNGbetalog", "MeanGoRT", "SDGoRTlog", "GlobalToLocalPrecedencelog", "GlobalToLocalInterferencelog", "NeckerTotalRatelog", "FlexSum"))
# all_rho_sub5 <- all_rho_sub5$rho
# EFA_all8_sub5 <- fa(all_rho_sub5, nfactors = 8, rotate = "oblimin",
#                     fm = "minrank", scores = "tenBerge")
# fa.sort(EFA_all8_sub5)
# # Items to be removed: sci_cur, sci_impo, NeckerTotalRate, RPSum, IH3Sum
# 
# data_efa_sub6 <- data_efa_sub5 %>% 
#   select(-c(sci_cur, sci_impo, NeckerTotalRatelog, RPSum, IH3Sum))
# all_rho_sub6 <- mixedCor(data = data_efa_sub6, 
#                          d = cat,
#                          c = c("AMean", "CMean", "EMean", "ESMean", "OMean", "ICuriositySum", "DCuriositySum", "IH1Sum", "IH4Sum", "CloSum", "AOTSum", "sci_tru", "MatrixCorrectCount", "GNGdprime", "GNGbetalog", "MeanGoRT", "SDGoRTlog", "GlobalToLocalPrecedencelog", "GlobalToLocalInterferencelog", "FlexSum"))
# all_rho_sub6 <- all_rho_sub6$rho
# EFA_all8_sub6 <- fa(all_rho_sub6, nfactors = 8, rotate = "oblimin",
#                     fm = "minrank", scores = "tenBerge")
# fa.sort(EFA_all8_sub6)
# # Items to be removed: IH4Sum, HEscore, IH1Sum
# # NOTE: I will stop this here for now. Point is that with the 8-factor solution and the exclusion criteria we end up removing so many items that we end up with factors with too few items loading onto them.

# Questionnaires only
# PA + scree plot: 5 factors
EFA_Q5 <- fa(Q_rho, nfactors = 5, rotate = "oblimin",
             fm = "minrank", scores = "tenBerge") 
fa.sort(EFA_Q5)
# Items to be removed (!= 1 primary salient loading AND complexity >= 2.0): IH2Sum, EMean, sci_id, ESMean, HEscore

survey_data_sub4 <- survey_data %>% 
  select(-c(IH2Sum, EMean, sci_id, ESMean, HEscore))
Q_rho_sub4 <- mixedCor(data = survey_data_sub4,
                       d = c("HRscore"),
                       c = c("AMean", "CMean", "OMean", "ICuriositySum", "DCuriositySum", "IH1Sum", "IH3Sum", "IH4Sum", "CloSum", "CogSum", "AOTSum", "RPSum", "sci_cur", "sci_tru", "sci_impo", "MatrixCorrectCount"),
                       use = "pairwise.complete.obs",
                       method = "spearman")
Q_rho_sub4 <- Q_rho_sub4$rho
EFA_Q5_sub4 <- fa(Q_rho_sub4, nfactors = 5, rotate = "oblimin",
                  fm = "minrank", scores = "tenBerge") 
fa.sort(EFA_Q5_sub4)
# Items to be removed: sci_cur, IH4Sum

survey_data_sub5 <- survey_data_sub4 %>% 
  select(-c(IH4Sum, sci_cur))
Q_rho_sub5 <- mixedCor(data = survey_data_sub5,
                       d = c("HRscore"),
                       c = c("AMean", "CMean", "OMean", "ICuriositySum", "DCuriositySum", "IH1Sum", "IH3Sum", "CloSum", "CogSum", "AOTSum", "RPSum", "sci_tru", "sci_impo", "MatrixCorrectCount"),
                       use = "pairwise.complete.obs",
                       method = "spearman")
Q_rho_sub5 <- Q_rho_sub5$rho
EFA_Q5_sub5 <- fa(Q_rho_sub5, nfactors = 5, rotate = "oblimin",
                  fm = "minrank", scores = "tenBerge") 
fa.sort(EFA_Q5_sub5)
# Items to be removed: sci_impo
# This is problematic though, because then we have 14 items for 3 factors, resulting in too few items per factor; I will not continue with the 5-factor model.

# Tasks
# PA + scree plot: 3 factors
EFA_T3 <- fa(T_rho, nfactors = 3, rotate = "oblimin",
             fm = "minrank", scores = "tenBerge")
fa.sort(EFA_T3)

```



## Extract Factor Scores

This section not ready yet

```{r Extracting factor scores, include = FALSE}

# # Extracting factor scores per case (ppt), for EFA_6f solution 
# # I'm not sure that this is right, there is some info in factor.scores about polychoric data and I am not sure how the factor scores per ppt are calculated exactly, whether the HR score binary data makes a difference
# # I'm also not sure of all the arguments, particularly rho
# # taking rho out slightly changes if you plot bergescores$scores
# # it's also possible to impute missing data as there is a lot, use "median" or "mean"
# # the four combinations of these arguments give basically the same resulting plot but should check them 
# # the plot is MRFA1 against MRFA6, theyr'e the first 2 columns in $scores
# bergescores <- factor.scores(Q_new, EFA_6f, method = c("tenBerge"), 
#                              rho = Q_rho, impute = "median")
# # to view as a dataframe
# View(bergescores$scores)

```

#EFA using lavaan
https://solomonkurz.netlify.app/post/2021-05-11-yes-you-can-fit-an-exploratory-factor-analysis-with-lavaan/ 

```{r EFA using lavaan, include=FALSE}

library(tidyverse)
library(lavaan)

d <- Q_new


#EFA in lavaan
#Consider your estimation method.
#An important preliminary step before fitting an EFA is getting a good sense of the data. In the last section, we learned the data are composed of three ordinal categories, which means that conventional estimators, such as maximum likelihood, won’t be the best of choices. Happily, lavaan offers several good options for ordinal data, which you can learn more about at https://lavaan.ugent.be/tutorial/cat.html and https://lavaan.ugent.be/tutorial/est.html. The default is the WLSMV estimator (i.e., estimator = "WLSMV"), which our friends in quantitative methodology have shown is generally a good choice (e.g., Flora & Curran, 2004; Li, 2016).

# 3-factor model
f3 <- '
efa("efa")*f1 +
efa("efa")*f2 +
efa("efa")*f3 =~ AMean + CMean + EMean + ESMean + OMean + ICuriositySum + DCuriositySum + IH1Sum + IH2Sum + IH3Sum + IH4Sum + CloSum  +  CogSum + AOTSum + HEscore + HRscore + RPSum + Sci.cur_Sum + Sci.tru_Sum + Sci.impo_Sum + Sci.id_Sum + MatrixCorrectCount'


# 4-factor model
f4 <- '
efa("efa")*f1 +
efa("efa")*f2 +
efa("efa")*f3 +
efa("efa")*f4 =~ AMean + CMean + EMean + ESMean + OMean + ICuriositySum + DCuriositySum + IH1Sum + IH2Sum + IH3Sum + IH4Sum + CloSum  +  CogSum + AOTSum + HEscore + HRscore + RPSum + Sci.cur_Sum + Sci.tru_Sum + Sci.impo_Sum + Sci.id_Sum + MatrixCorrectCount'

# 5-factor model
f5 <- '
efa("efa")*f1 +
efa("efa")*f2 +
efa("efa")*f3 +
efa("efa")*f4 +
efa("efa")*f5 =~ AMean + CMean + EMean + ESMean + OMean + ICuriositySum + DCuriositySum + IH1Sum + IH2Sum + IH3Sum + IH4Sum + CloSum  +  CogSum + AOTSum + HEscore + HRscore + RPSum + Sci.cur_Sum + Sci.tru_Sum + Sci.impo_Sum + Sci.id_Sum + MatrixCorrectCount'

# 6-factor model
f6 <- '
efa("efa")*f1 +
efa("efa")*f2 +
efa("efa")*f3 +
efa("efa")*f4 +
efa("efa")*f5 +
efa("efa")*f6 =~ AMean + CMean + EMean + ESMean + OMean + ICuriositySum + DCuriositySum + IH1Sum + IH2Sum + IH3Sum + IH4Sum + CloSum  +  CogSum + AOTSum + HEscore + HRscore + RPSum + Sci.cur_Sum + Sci.tru_Sum + Sci.impo_Sum + Sci.id_Sum + MatrixCorrectCount'

#fit the EFAs
#One can currently do so with either the cfa() or sem() functions. The default rotation method is oblique Geomin. If you’re not up on rotation methods, you might check out Sass & Schmitt (2010). Here we’ll give a nod to tradition and use oblique Oblimin by setting rotation = "oblimin". Also, note our use of ordered = TRUE, which explicitly tells lavaan::cfa() to treat all  of our items as ordinal.

#use the ordered argument when using one of the fitting functions (cfa/sem/growth/lavaan), for example, if you have four binary or ordinal variables (say, item1, item2, item3, item4), you can use:

#fit <- cfa(myModel, data = myData,
#ordered = c("item1","item2","item3","item4"))

#If all the (endogenous) variables are to be treated as categorical, you can use ordered = TRUE as a shortcut. When the ordered= argument is used, lavaan will automatically switch to the WLSMV estimator: it will use diagonally weighted least squares (DWLS) to estimate the model parameters, but it will use the full weight matrix to compute robust standard errors, and a mean- and variance-adjusted test stastistic. Other options are unweighted least squares (ULSMV), or pairwise maximum likelihood (PML). Full information maximum likelihood is currently not supported.

##CD: this works, but I'm not sure about the estimator. With psych efa() I was using minrank and inputting the mixed correlation matrix. These give pretty different results. Specifying the binary variables (and using RP as ordinal) SHOULD deal with those issues, but the estimator is still different. I used minrank because the other methods were producing non positive definite matrices even though according to is.positive.definite() the original correlation matrix was OK. 

efa_f3 <- 
  cfa(model = f3,
      data = d,
      rotation = "oblimin",
      estimator = "WLSMV",
      ordered = c("HEscore","HRscore","RPSum"))

summary(efa_f3, fit.measures = TRUE)

efa_f4 <- 
  cfa(model = f4,
      data = d,
      rotation = "oblimin",
      estimator = "WLSMV",
      ordered = c("HEscore","HRscore","RPSum"))

summary(efa_f4, fit.measures = TRUE)

efa_f5 <- 
  cfa(model = f5,
      data = d,
      rotation = "oblimin",
      estimator = "WLSMV",
      ordered = c("HEscore","HRscore","RPSum"))

summary(efa_f5, fit.measures = TRUE)

efa_f6 <- 
  cfa(model = f6,
      data = d,
      rotation = "oblimin",
      estimator = "WLSMV",
      ordered = c("HEscore","HRscore","RPSum"))

summary(efa_f6, fit.measures = TRUE)

#Ccompare the EFAs by some of their fit statistics.

#The parameterEstimates() function returns a data.frame containing all the model parameters in the rows:

#fit <- cfa(HS.model, data=HolzingerSwineford1939)
#parameterEstimates(fit)

parameterEstimates(efa_f3)

# define the fit measures
fit_measures_robust <- c("chisq.scaled", "df", "pvalue.scaled", 
                         "cfi.scaled", "rmsea.scaled", "srmr")

# collect them for each model
rbind(
  fitmeasures(efa_f3, fit_measures_robust),
  fitmeasures(efa_f4, fit_measures_robust),
  fitmeasures(efa_f5, fit_measures_robust),
  fitmeasures(efa_f6, fit_measures_robust)) %>% 
  
  # wrangle
  data.frame() %>% 
  mutate(chisq.scaled  = round(chisq.scaled, digits = 0),
         df            = as.integer(df),
         pvalue.scaled = ifelse(pvalue.scaled == 0, "< .001", pvalue.scaled)) %>% 
  mutate_at(vars(cfi.scaled:srmr), ~round(., digits =  3))


##None of the solutions seems particularly better than the others based on these criteria, it's just adding factors till we overfit. So we'll probably take one based on theoretical sense. 

###"As is often the case, the fit got steadily better with each added factor. Here’s how one might work with the output from the standardizedsolution() function to plot the lambda’s for the 3-factor solution."###

# wrangle
standardizedsolution(efa_f3) %>% 
  filter(op == "=~") %>% 
  mutate(item  = str_remove(rhs, "y") %>% as.double(),
         factor = str_remove(lhs, "f")) %>% 
  
  # plot
  ####this part not working
  
  ggplot(aes(x = est.std, xmin = ci.lower, xmax = ci.upper, y = item)) +
  annotate(geom = "rect",
           xmin = -1, xmax = 1,
           ymin = -Inf, ymax = Inf,
           fill = "grey90") +
  annotate(geom = "rect",
           xmin = -0.7, xmax = 0.7,
           ymin = -Inf, ymax = Inf,
           fill = "grey93") +
  annotate(geom = "rect",
           xmin = -0.4, xmax = 0.4,
           ymin = -Inf, ymax = Inf,
           fill = "grey96") +
  geom_vline(xintercept = 0, color = "white") +
  geom_pointrange(aes(alpha = abs(est.std) < 0.4),
                  fatten = 5) +
  geom_text(aes(label = item, color = abs(est.std) < 0.4),
            size = 2) +
  scale_color_manual(values = c("white", "transparent")) +
  scale_alpha_manual(values = c(1, 1/3)) +
  scale_x_continuous(expression(lambda[standardized]), 
                     expand = c(0, 0), limits = c(-1, 1),
                     breaks = c(-1, -0.7, -0.4, 0, 0.4, 0.7, 1),
                     labels = c("-1", "-.7", "-.4", "0", ".4", ".7", "1")) +
  scale_y_continuous(breaks = 1:22, sec.axis = sec_axis(~ . * 1, breaks = 1:22)) +
  ggtitle("Loadings for the 3-factor model") +
  theme(legend.position = "none") +
  facet_wrap(~ factor, labeller = label_both)

#"To reduce visual complexity, lambda’s less than the conventional 0.4 threshold are semitransparent. Those above the threshold have the item number in the dot. I’m not going to offer an interpretation, here, since that’s not really the point of this post. But hopefully this will get you started fitting all the lavaan-based EFAs your heart desires."


#View loadings of each model
model_loadings_3 <- inspect(efa_f3, what = "std")[["lambda"]]
print(model_loadings_3)
fa.diagram(model_loadings_3)

model_loadings_4 <- inspect(efa_f4, what = "std")[["lambda"]]
print(model_loadings_4)
fa.diagram(model_loadings_4)

model_loadings_5 <- inspect(efa_f5, what = "std")[["lambda"]]
print(model_loadings_5)
fa.diagram(model_loadings_5)

model_loadings_6 <- inspect(efa_f6, what = "std")[["lambda"]]
print(model_loadings_6)
fa.diagram(model_loadings_6)




```

#Factor scores using lavPredict
https://rdrr.io/cran/lavaan/man/lavPredict.html 
https://lavaan.ugent.be/tutorial/tutorial.pdf 

The main purpose of the lavPredict() function is to compute (or ‘predict’) estimated values for the latent variables in the model (‘factor scores’). NOTE: the goal of this function is NOT to predict future values of dependent variables as in the regression framework!

```{r Factor scores using lavPredict,include=FALSE}

#fsm returns factor score matrix as an attribute
#method Empirical Bayes Modal approach
#type A character string. If "lv", estimated values for the latent variables in the model are computed. If "ov", model predicted values for the indicators of the latent variables in the model are computed. If "yhat", the estimated value for the observed indicators, given user-specified values for the latent variables provided by de ETA argument. If "fy", densities (or probabilities) for each observed indicator, given user-specified values for the latent variables provided by de ETA argument.
#se = naive standard errors
#EBMscores <- lavPredict(efa_f3, newdata = Q_new, type = "yhat", 
                       # method = "EBM", se = "none", acov = "none",
                       # label = TRUE, fsm = TRUE, append.data = FALSE, 
                       # assemble = FALSE, level = 1L, optim.method = 
                       #   "bfgs", ETA = NULL)

#seems to work but haven't checked it yet
EBMscores <- lavPredict(efa_f3, method = "EBM")

```







