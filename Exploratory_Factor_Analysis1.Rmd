---
title: "Exploratory Factor Analysis"
author: "Milla Pihlajamaki and Caitlin Dawson"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_notebook
---

The data are normal (skew<=1). The correlation matrix uses polychoric correlations (treats data as ordinal).

```{r Load Packages}

# library(ISLR) 
# library(glmnet) 
# library(dplyr) 
# library(tidyr)
# library(tidyverse)
library(caret)
# library(glmnet)
# library(tidyverse)
# library(tidyselect)
# library(psych)
# library(lavaan)
# library(dplyr)
# library(ggplot2)
# library(psychTools)
# library(GPArotation)
# library(devtools)
# library(apaTables)
# library(semPlot)
# library(sjPlot)
# library(glasso)

# Create task- and survey-only sets
survey_data <- data_efa[, q_var]
task_data <- data_efa[, t_var_log]

```


# Cross-method prediction

10-fold cross validated ridge regression to predict survey and task variables *within* and *between* measure types following Eisenberg et al. Intended to help decide whether to factor tasks and surveys together or separately, and also to see if some variables should be left out. 

Taking each DV one by one and trying to predict it using its own measurement category (leaving it out) and by the other measurement category. Each DV is the target; the predictors are the other variables in the category. 

4 distributions of predictions: tasks-by-tasks, tasks-by-surveys, surveys-by-tasks, and surveys-by-surveys -> extract the R^2 values and create four distributions

"Prediction success was assessed by 10-fold cross-validated ridge regression using the RidgeCV function from scikit-learn with default parameters (10). Almost all DVs were able to be predicted to some degree by their respective measurement category (mean task-by-tasks R2 mean survey-by-surveys R2  = .45). In contrast, cross-measurement prediction failed: surveys were unable to predict task DVs and vice versa (mean task-by-surveys R2 = -.13, mean survey-by-tasks R2 = -.29). Note that R2 values below 0 are possible when employing  cross-validation and indicate no discoverable linear relationship. The entire distribution of R2  values for each of these predictions is shown is Figure S2b."

Ridge regression differs from lasso regression in that it can reduce coefficients down to near zero but always includes all predictors. Lasso regression can be used to reduce dimensionality as coefficients can be zero. Elastic net regression combines the two. 

Ridge regression works better than regular linear regression when you have many variables in a model and also collinearity. It is a form of penalized regression that adds a constraint. The constraint constant is defined by *lambda*, which shrinks coefficients. A lambda of 0 is a regular linear regression, and a too large lambda will shrink the coefficients too much to make a usable model. We can identify the best value of lambda by using cross-validation. 

Instead of splitting data into a training and test set, cross-validation trains and tests the model on subsets of the full dataset, creating a distribution. Here we just use it to compare various values of lambda in order to choose the best one for each prediction model. 

The aim of this section is to create a model for predicting each DV in the 4 distributions described above. Ridge regression is used because of the type of dataset and because we want to retain all variables when checking the cross- and between-measure type predictions. 

The question here is, can X variable be predicted equally well by surveys vs. tasks? After we get these R2 for each model (there will be 4 models per variable), we can take the mean R2 for each of the 4 distributions and make a plot as in Eisenberg to examine whether the distributions are similar. This helps us decide whether task and survey variables should be clustered or factored together. 

MILLA: try to understand the cross-method prediction

```{r Cross-Method Prediction}

# 
# # Train the model
# # the first argument is the column (variable) we want to predict, using the other variables
# # this syntax for AMean and data automatically designates that row as the outcome and leaves it out of the prediction--check this in the summary--there are 23 total variables, 22 of them are being used as predictors
# # other arguments are the dataset, the method which is ridge regression, preprocessing which scales the data, and it uses the control process
# 
# # Define training control
# # set seed for replicability: can be done a variety of ways, the easiest is to set it before each train. NB: setting a different seed, a random seed, or not setting the seed, will identify different models. See [insert link here] for a good discussion on different sources of variability in machine learning models and what kind of tolerance we should have for them, and what to do about it.
# # cross-validation will have 10 folds
# # actually I don't think this first version is even needed because it's just the training algorithm without any tuning hyperparameters set. If I end up using the model with the IDed lambda value, then I can just run the tunegrid version and choose the model there
# 
# ctrl <- trainControl(method = "cv", number = 10)
# 
# set.seed(123)
# 
# model <- train(
#   IH3Sum ~ .,
#   data = survey_data,
#   method = 'ridge',
#   preProcess = c("center", "scale"),
#   trControl = ctrl
# )
# 
# # Summarize the results
# model
# 
# # tune different values of lambda
# # I'm not sure yet how to feed it back to the original model
# 
# # these lines for making dfs that include the 1 cross-method outcome and the predictors
# NeckerTotalRate_target <- data_all_without_binary %>%
#   select(AMean:MatrixCorrectCount, NeckerTotalRate)
# NeckerTotalRate_target <- na.omit(NeckerTotalRate_target)
# View(NeckerTotalRate_target)
# 
# tuneGrid <- expand.grid(
#   .lambda = seq(0, 2, by = 0.01)) # trying to find a lambda value that minimizes RMSE
#   # R MSE: The root mean squared error. This measures the average difference between the predictions made by the model and the actual observations. The lower the RMSE, the more closely a model can predict the actual observations.) # .lambda = seq(0, .1, by = 0.01)
# set.seed(123) # have to call this again to ensure reproducibility; but remember, running with a different seed will produce a different model
# model2 <- train(
#   NeckerTotalRate ~ .,
#   data = NeckerTotalRate_target,
#   method = 'ridge',
#   preProcess = c("center", "scale"),
#   trControl = ctrl,
#   tuneGrid = tuneGrid
# )
# 
# model2
# plot(model2) # this plot shows the lambda value that minimizes RMSE
# 
# # this is an interesting plot showing the importance of the different variables to the model
# # but remember that this probably doesn't mean much unless it's a well fitting model
# plot(varImp(model2))
# 
# # as I understand this now, the original model uses no tuning hyperparameters at all (default TuneGrid  = NULL)? It uses some reasonable standard tuning parameter (e.g. lambda = 0.1) and estimates the model with that. The second model feeds TuneGrid a selection of possible tuning parameters, and it selects the best one, outputting all the possible models according to those parameters. It finds the best parameter in that possible set, and then you have to find the matching R2 and RMSE given for the model with that parameter. It shouldn't be super different to the original model. The parameter lambda increases with increasing regularization, meaning that the higher lambda should mean a bigger difference between model 1 and model 2, and means that that model needed more regularization (probably a worse fitting model?) I'm not sure why or if you have to do the first step with model 1, or how data leakage fits into all this.
# # I'm also not sure what set.seed is doing here
# #the model results and best lambda tend to be slightly different each time, but within a couple of hundredths, generally, in R2
# 
# # make a violin plot to show the distributions of R2 for the 4 types of predictions
# 
# library(ggplot2)
# library(dplyr)
# predictions <- read.csv('4xpredictions.csv', sep = ";")
# predictions$Method <- as.factor(predictions$Method)
# 
# p <- predictions %>%
# ggplot(aes(x = Method, y = Rsquared, fill = Method)) +
#   geom_violin() +
#   coord_flip() +
#   theme(legend.position = "none") +
#   xlab("") +
#   ylab("R-squared")
# p
# 
# library(ggridges)
# library(viridis)
# 
# # ridge plot of all the distributions of the variables
# # need to install ggridges
# 
# library(tidyr)
# datalong <- gather(data_all_without_binary, task, measurement,
#                    AMean:NeckerTotalRate, factor_key = TRUE)
# datalong
# 
# ggplot(datalong, aes(x = task, y = measurement)) +
#   geom_density2d_filled()
#   labs(title = "All variable distributions") +
#   scale_fill_viridis_c()

```

# Exploratory Factor Analysis

This section includes three series of EFAs: all variables, surveys only, and tasks only. In the preregistration, we promised to start with these. 

Correlations are mixed because of the binary variables from the heuristics task

## Create Correlation Matrices

```{r Create Correlation Matrices, include = FALSE}

# A general note: we probably need to justify putting a bunch of task and self-report items in the same factor analysis / at least mention it !
# https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3268653/

# Descriptives
describe(data_efa)

# Create character strings with different classes of variables
con <- c("AMean", "CMean", "EMean", "ESMean", "OMean", "ICuriositySum", "DCuriositySum", "IH1Sum", "IH2Sum", "IH3Sum", "IH4Sum", "CloSum", "CogSum", "AOTSum", "RPSum", "sci_cur", "sci_tru", "sci_impo", "sci_id", "MatrixCorrectCount", "GNGdprime", "GNGbetalog", "MeanGoRT", "SDGoRTlog", "GlobalToLocalPrecedencelog", "GlobalToLocalInterferencelog", "NeckerTotalRatelog", "FlexSum")
q_con <- c("AMean", "CMean", "EMean", "ESMean", "OMean", "ICuriositySum", "DCuriositySum", "IH1Sum", "IH2Sum", "IH3Sum", "IH4Sum", "CloSum", "CogSum", "AOTSum", "RPSum", "sci_cur", "sci_tru", "sci_impo", "sci_id", "MatrixCorrectCount")
cat <- c("HEscore", "HRscore")

# Create a correlation matrix (polychoric/mixed correlations)
all_cor <- mixedCor(data = data_efa, d = cat, c = con, 
                    use = "pairwise.complete.obs",
                    method = "spearman")
Q_cor <- mixedCor(data = survey_data, d = cat, c = q_con,
                  use = "pairwise.complete.obs",
                  method = "spearman")
T_cor <- mixedCor(data = task_data, 
                  use = "pairwise.complete.obs",
                  method = "pearson")
# NOTE: Caitlin had specified other arguments as well but I am not sure how necessary there are?

# Store correlation matrix
all_rho <- all_cor$rho
Q_rho <- Q_cor$rho
T_rho <- T_cor$rho

# Mean correlation across items
mean(abs(all_rho[lower.tri(all_rho)]))
mean(abs(Q_rho[lower.tri(Q_rho)]))
mean(abs(T_rho[lower.tri(T_rho)]))

# Histogram of correlations
hist(abs(all_rho), breaks = 20, col = sample(colors(), 1))
hist(abs(Q_rho), breaks = 20, col = sample(colors(), 1))
hist(abs(T_rho), breaks = 20, col = sample(colors(), 1))

```

## Factorability

```{r Factorability, include = FALSE}

# Bartlett test (p<.05 indicates data are suitable for structure detection)
cortest.bartlett(all_rho, n = nrow(data_efa))
cortest.bartlett(Q_rho, n = nrow(data_efa))
cortest.bartlett(T_rho, n = nrow(data_efa))

# Kaiser-Meyer-Olkin measure (>.7 is good)
KMO(r = all_rho)
KMO(r = Q_rho)
KMO(r = T_rho)
# NOTE: KMO is not very good for a lot of items?

```

## Identify number of factors to extract

For now I used EBIC as the main criterion following Eisenberg et al.

```{r Number of factors, include = FALSE}

# Scree plots
# All variables
plot(eigen(all_rho)$values, type = "b", pch = 20, col = "red",
     main = "Scree Plot", ylab = "Eigenvalues",
     xlab = "No. Factors")
# Questionnaire variables
plot(eigen(Q_rho)$values, type = "b", pch = 20, col = "red",
     main = "Scree Plot", ylab = "Eigenvalues",
     xlab = "No. Factors")
# Task variables
plot(eigen(T_rho)$values, type = "b", pch = 20, col = "red",
     main = "Scree Plot", ylab = "Eigenvalues",
     xlab = "No. Factors")

# NFactors
# All variables
nfactors(all_rho, n = 6, n.obs  = nrow(data_efa), 
         rotate = "oblimin", diagonal = FALSE, 
         fm = "minrank") 
# Questionnaires
nfactors(Q_rho, n = 6, n.obs  = nrow(survey_data), 
         rotate = "oblimin", diagonal = FALSE,
         fm = "minrank") 
# Tasks
nfactors(T_rho, n = 6, n.obs  = nrow(task_data), 
         rotate = "oblimin", diagonal = FALSE,
         fm = "minrank") 

# Parallel Analysis
# All variables
fa.parallel(all_rho, n.obs = nrow(data_efa), fm = "minrank", 
            fa = "both", nfactors = 12)
# cor.plot(all_rho)
# Questionnaires
fa.parallel(Q_rho, n.obs = nrow(survey_data), fm = "wls", 
            fa = "both", nfactors = 6)
# cor.plot(Q_rho)
# Tasks
fa.parallel(T_rho, n.obs = nrow(task_data), fm = "wls", 
            fa = "both", nfactors = 6)
# cor.plot(T_rho)

```

## Run the Factor Analysis

Method is "minrank" minimum rank or MRFA. Recommended for polychoric/tetrachoric correlation matrices with Heywood cases https://www.tandfonline.com/doi/pdf/10.1080/10705511.2020.1735393 

The criteria used to determine whether to keep items and factors:
* if an item does not have one and only one salient loading (>=.33) *and* it has high complexity (>=2.0), it is removed
* if a factor has less than three items with primary (largest loading of that item) loadings on it, the factor is removed

```{r Run the Factor Analysis, include = FALSE}

# All variables (not very relevant, previous analyses indicate that questionnaires and tasks should be analysed separately)
# EBIC: 6 factors
EFA_all6 <- fa(all_rho, nfactors = 6, rotate = "oblimin",
               fm = "minrank", scores = "tenBerge")
fa.sort(EFA_all6)
# Items to be removed (!= 1 primary salient loading AND complexity >= 2.0): OMean, sci_tru, SDGoRTlog, AMean, FlexSum, sci_impo
data_efa_sub <- data_efa %>% 
  select(-c("OMean", "sci_tru", "SDGoRTlog", "AMean", "FlexSum", "sci_impo"))
all_rho_sub <- mixedCor(data = data_efa_sub,
                        d = cat,
                        c = c("CMean", "EMean", "ESMean", "ICuriositySum", "DCuriositySum", "IH1Sum", "IH2Sum", "IH3Sum", "IH4Sum", "CloSum", "CogSum", "AOTSum", "RPSum", "sci_cur", "sci_id", "MatrixCorrectCount", "GNGdprime", "GNGbetalog", "MeanGoRT", "GlobalToLocalPrecedencelog", "GlobalToLocalInterferencelog", "NeckerTotalRatelog"))
all_rho_sub <- all_rho_sub$rho
EFA_all6_sub <- fa(all_rho_sub, nfactors = 6, rotate = "oblimin",
                  fm = "minrank", scores = "tenBerge")
fa.sort(EFA_all6_sub)
# Items to be removed: sci_id, HEscore, Necker, MeanGoRT, AOTSum

data_efa_sub2 <- data_efa_sub %>% 
  select(-c("sci_id", "HEscore", "NeckerTotalRatelog", "MeanGoRT", "AOTSum"))
all_rho_sub2 <- mixedCor(data = data_efa_sub2,
                         d = c("HRscore"),
                         c = c("CMean", "EMean", "ESMean", "ICuriositySum", "DCuriositySum", "IH1Sum", "IH2Sum", "IH3Sum", "IH4Sum", "CloSum", "CogSum", "RPSum", "sci_cur", "MatrixCorrectCount", "GNGdprime", "GNGbetalog", "GlobalToLocalPrecedencelog", "GlobalToLocalInterferencelog"))
all_rho_sub2 <- all_rho_sub2$rho
EFA_all6_sub2 <- fa(all_rho_sub2, nfactors = 6, rotate = "oblimin",
                   fm = "minrank", scores = "tenBerge")
fa.sort(EFA_all6_sub2)
# Items to be removed: IH2Sum

data_efa_sub3 <- data_efa_sub2 %>% 
  select(-c("IH2Sum"))
all_rho_sub3 <- mixedCor(data = data_efa_sub3,
                         d = c("HRscore"),
                         c = c("CMean", "EMean", "ESMean", "ICuriositySum", "DCuriositySum", "IH1Sum", "IH3Sum", "IH4Sum", "CloSum", "CogSum", "RPSum", "sci_cur", "MatrixCorrectCount", "GNGdprime", "GNGbetalog", "GlobalToLocalPrecedencelog", "GlobalToLocalInterferencelog"))
all_rho_sub3 <- all_rho_sub3$rho
EFA_all6_sub3 <- fa(all_rho_sub3, nfactors = 6, rotate = "oblimin",
                   fm = "minrank", scores = "tenBerge")
fa.sort(EFA_all6_sub3)
# Items to be removed: IH4Sum

data_efa_sub4 <- data_efa_sub3 %>% 
  select(-c("IH4Sum"))
all_rho_sub4 <- mixedCor(data = data_efa_sub4,
                         d = c("HRscore"),
                         c = c("CMean", "EMean", "ESMean", "ICuriositySum", "DCuriositySum", "IH1Sum", "IH3Sum", "CloSum", "CogSum", "RPSum", "sci_cur", "MatrixCorrectCount", "GNGdprime", "GNGbetalog", "GlobalToLocalPrecedencelog", "GlobalToLocalInterferencelog"))
all_rho_sub4 <- all_rho_sub4$rho
EFA_all6_sub4 <- fa(all_rho_sub4, nfactors = 6, rotate = "oblimin",
                   fm = "minrank", scores = "tenBerge")
fa.sort(EFA_all6_sub4)
# Items look ok, but three factors have less than three primary loadings -> let's try with fewer factors

EFA_all5 <- fa(all_rho_sub4, nfactors = 5, rotate = "oblimin",
               fm = "minrank", scores = "tenBerge")
fa.sort(EFA_all5)
EFA_all4 <- fa(all_rho_sub4, nfactors = 4, rotate = "oblimin",
               fm = "minrank", scores = "tenBerge")
fa.sort(EFA_all4)
EFA_all3 <- fa(all_rho_sub4, nfactors = 3, rotate = "oblimin",
               fm = "minrank", scores = "tenBerge")
fa.sort(EFA_all3)


# Questionnaires only
# EBIC: 4 factors
EFA_Q4 <- fa(Q_rho, nfactors = 4, rotate = "oblimin",
             fm = "minrank", scores = "tenBerge") 
fa.sort(EFA_Q4)
# Items to be removed (!= 1 primary salient loading AND complexity >= 2.0): HEscore, sci_impo, DCuriositySum, IH4Mean, sci_tru

survey_data_sub <- survey_data %>% 
  select(-c(c("HEscore", "sci_impo", "DCuriositySum", "IH4Sum", "sci_tru")))
Q_rho_sub <- mixedCor(data = survey_data_sub, 
                      d = c("HRscore"),
                      c = c("AMean", "CMean", "EMean", "ESMean", "OMean",  "ICuriositySum", "IH1Sum", "IH2Sum", "IH3Sum", "CloSum", "CogSum", "AOTSum", "RPSum", "sci_cur", "sci_id", "MatrixCorrectCount"),
                      use = "pairwise.complete.obs",
                      method = "spearman")
Q_rho_sub <- Q_rho_sub$rho
EFA_Q4_sub <- fa(Q_rho_sub, nfactors = 4, rotate = "oblimin",
                 fm = "minrank", scores = "tenBerge") 
fa.sort(EFA_Q4_sub)
# Items to be removed: EMean, IH2Sum

survey_data_sub2 <- survey_data_sub %>% 
  select(-c("EMean", "IH2Sum"))
Q_rho_sub2 <- mixedCor(data = survey_data_sub2, 
                       d = c("HRscore"),
                       c = c("AMean", "CMean", "ESMean", "OMean",  "ICuriositySum", "IH1Sum", "IH3Sum", "CloSum", "CogSum", "AOTSum", "RPSum", "sci_cur", "sci_id", "MatrixCorrectCount"),
                       use = "pairwise.complete.obs",
                       method = "spearman")
Q_rho_sub2 <- Q_rho_sub2$rho
EFA_Q4_sub2 <- fa(Q_rho_sub2, nfactors = 4, rotate = "oblimin",
                  fm = "minrank", scores = "tenBerge") 
fa.sort(EFA_Q4_sub2)
# Items to be removed: AOTSum

survey_data_sub3 <- survey_data_sub2 %>% 
  select(-c("AOTSum"))
Q_rho_sub3 <- mixedCor(data = survey_data_sub3, 
                       d = c("HRscore"),
                       c = c("AMean", "CMean", "ESMean", "OMean",  "ICuriositySum", "IH1Sum", "IH3Sum", "CloSum", "CogSum", "RPSum", "sci_cur", "sci_id", "MatrixCorrectCount"),
                       use = "pairwise.complete.obs",
                       method = "spearman")
Q_rho_sub3 <- Q_rho_sub3$rho
EFA_Q4_sub3 <- fa(Q_rho_sub3, nfactors = 4, rotate = "oblimin",
                  fm = "minrank", scores = "tenBerge") 
fa.sort(EFA_Q4_sub3)
# All items ok now, but the 4th factor has less than three primary loadings -> run a model with only three factors
EFA_Q3_sub3 <- fa(Q_rho_sub3, nfactors = 3, rotate = "oblimin",
                  fm = "minrank", scores = "tenBerge") 
fa.sort(EFA_Q3_sub3)

# Tasks only
EFA_T3 <- fa(T_rho, nfactors = 3, rotate = "oblimin",
             fm = "minrank", scores = "tenBerge")
fa.sort(EFA_T3)

```

## Extract Factor Scores

This section not ready yet

```{r Extracting factor scores, include = FALSE}

# Extracting factor scores per case (ppt), for EFA_6f solution 
# I'm not sure that this is right, there is some info in factor.scores about polychoric data and I am not sure how the factor scores per ppt are calculated exactly, whether the HR score binary data makes a difference
# I'm also not sure of all the arguments, particularly rho
# taking rho out slightly changes if you plot bergescores$scores
# it's also possible to impute missing data as there is a lot, use "median" or "mean"
# the four combinations of these arguments give basically the same resulting plot but should check them 
# the plot is MRFA1 against MRFA6, theyr'e the first 2 columns in $scores
bergescores <- factor.scores(Q_new, EFA_6f, method = c("tenBerge"), 
                             rho = Q_rho, impute = "median")
# to view as a dataframe
View(bergescores$scores)

```
