describe(data_Q_total$sci_cur)
hist(data_Q_total$sci_cur,
col = sample(colors(), 1),
xlab = "")
# Exclusion criteria
# <=6000ms response time
data_Q_total <- data_Q_total %>%
mutate(HR_omit = case_when(END.QUESTIONNAIRE.HEURISTIC <= 6000 ~ 1,
END.QUESTIONNAIRE.HEURISTIC > 6000 ~ 0))
# Q1
data_Q_total$hr.1 <- as.factor(data_Q_total$hr.1)
levels(data_Q_total$hr.1) <- list("N" = "Kumpikin on yhtÃ¤ todennÃ¤kÃ¶istÃ¤", "H-R" = "Kuudes lapsi on tyttÃ¶", "Neither" = "Kuudes lapsi on poika")
# Q2
data_Q_total$hr.2 <- as.factor(data_Q_total$hr.2)
levels(data_Q_total$hr.2) <- list("N" = "Kummatkin sarjat ovat yhtÃ¤ todennÃ¤kÃ¶isiÃ¤", "H-R" = "THHTHT", "Neither" = "HTHTHT")
# Q3
# 3.1
data_Q_total$hr.3.1 <- as.factor(data_Q_total$hr.3.1)
levels(data_Q_total$hr.3.1) <- list("b" = "Herra F. sai sydÃ¤nkohtauksen", "a" = "Herra F. on yli 55-vuotias ja on saanut sydÃ¤nkohtauksen", "c" = "Herra F.:llÃ¤ on suuri perhe")
# 3.2
data_Q_total$hr.3.2 <- as.factor(data_Q_total$hr.3.2)
levels(data_Q_total$hr.3.2) <- list("b" = "Herra F. sai sydÃ¤nkohtauksen", "a" = "Herra F. on yli 55-vuotias ja on saanut sydÃ¤nkohtauksen", "c" = "Herra F.:llÃ¤ on suuri perhe")
# 3.3
data_Q_total$hr.3.3 <- as.factor(data_Q_total$hr.3.3)
levels(data_Q_total$hr.3.3) <- list("b" = "Herra F. sai sydÃ¤nkohtauksen", "a" = "Herra F. on yli 55-vuotias ja on saanut sydÃ¤nkohtauksen", "c" = "Herra F.:llÃ¤ on suuri perhe")
# Create the variables for the combinations of answers
# abc / acb / cab = N; bac / bca /cba = H-R
data_Q_total <- data_Q_total %>%
mutate(hr.3.updated = case_when(
((hr.3.1 == "b" & (hr.3.2 == "a"|hr.3.3 == "a")) | hr.3.2 == "b" & hr.3.3 == "a") ~ "H-R",
((hr.3.1 == "a" & (hr.3.2 == "b"|hr.3.3 == "b")) | (hr.3.2 == "a" & hr.3.3 == "b")) ~ "N"
))
data_Q_total$hr.3.updated <- factor(data_Q_total$hr.3.updated)
# Q4
data_Q_total$hr.4 <- as.factor(data_Q_total$hr.4)
levels(data_Q_total$hr.4) <- list("H-E" = "Kumpikin on yhtÃ¤ tehokas", "N" = "Kognitiivis-behavioraalista terapiaa", "Neither" = "LÃ¤Ã¤kehoitoa")
# Kognitiivis-behavioraalista terapiaa = N; Yhta tehokas = H-E
# Q5
data_Q_total$hr.5 <- as.factor(data_Q_total$hr.5)
levels(data_Q_total$hr.5) <- list("N" = "Huomenna luultavasti sataa", "H-E" = "On mahdotonta sanoa sataako huomenna vai ei", "Neither" = "Huomenna sataa")
# Huomenna luultavasti sataa = N; On mahdotonta sanoa sataako huomenna vai ei = H-E
# Q6
data_Q_total$hr.6 <- as.factor(data_Q_total$hr.6)
levels(data_Q_total$hr.6) <- list("N" = "TyttÃ¶", "H-E" = "Kumpikin on yhtÃ¤ todennÃ¤kÃ¶istÃ¤", "Neither" = "Poika")
# Tytto = N; Kumpikin on yht? todenn?k?ist? = H-E
# Calculate sums
data_Q_total <- data_Q_total %>%
mutate(
total_N = apply(., 1, function(x) length(which(x == "N"))),
total_HR = apply(., 1, function(x) length(which(x == "H-R"))),
total_HE = apply(., 1, function(x) length(which(x == "H-E"))),
total_Neither = apply(., 1, function(x) length(which(x == "Neither")))
)
# Omit heuristic reasoning data based on the HR_omit column
heur <- c("total_N", "total_HR", "total_HE", "total_Neither")
data_Q_total[which(data_Q_total$HR_omit == 1), heur] <- NA
# Omit participants with 0s in all the sum columns
data_Q_total[which(data_Q_total$total_N == 0 & data_Q_total$total_HR == 0 & data_Q_total$total_HE == 0 & data_Q_total$total_Neither == 0), heur] <- NA
# Describe: sum score level
describe(data_Q_total[, heur])
par(mfrow = c(2, 2))
for (i in 1:length(heur)) {
hist(data_Q_total[, heur[i]],
main = colnames(data_Q_total[heur[i]]),
col = sample(colors(), 1),
xlab = "")
}
# Create binary heuristic scores (0=0, 1=everything else)
data_Q_total <- data_Q_total %>%
mutate(HEscore = case_when(total_HE == 0 ~ 0,
total_HE != 0 ~ 1),
HRscore = case_when(total_HR == 0 ~ 0,
total_HR != 0 ~ 1))
# Add a measure of total heuristic response, where 1 = at least one mistake in one heuristic category, 2 = at least 1 heuristic mistake in both heuristic categories, 0 = all normative responses
data_Q_total <- data_Q_total %>%
group_by(Participant.Private.ID) %>%
mutate(HEHRscore = HEscore + HRscore)
# Exclusion criteria
# <=7000ms reaction time
data_Q_total <- data_Q_total %>%
mutate(RP_omit = case_when(END.QUESTIONNAIRE.RANDOM.PROB <= 4000 ~ 1,
END.QUESTIONNAIRE.RANDOM.PROB > 4000 ~ 0))
# Q1: Sairaala B is the correct answer
# Create a new column
data_Q_total$rp.1 <- as.factor(data_Q_total$rp.1)
data_Q_total <- data_Q_total %>%
mutate(rp.1.int = case_when(rp.1 == "Sairaalassa B (jossa syntyy 10 lasta pÃ¤ivÃ¤ssÃ¤)." ~ 1,
rp.1 != "Sairaalassa B (jossa syntyy 10 lasta pÃ¤ivÃ¤ssÃ¤)." ~ 0))
# Q2: Pyydys1 AND Pyydys 2 is the correct combination
data_Q_total$rp.2.1 <- as.factor(data_Q_total$rp.2.1)
data_Q_total$rp.2.2 <- as.factor(data_Q_total$rp.2.2)
data_Q_total$rp.2.3 <- as.factor(data_Q_total$rp.2.3)
data_Q_total$rp.2.4 <- as.factor(data_Q_total$rp.2.4)
data_Q_total$rp.2.5 <- as.factor(data_Q_total$rp.2.5)
data_Q_total$rp.2.6 <- as.factor(data_Q_total$rp.2.6)
data_Q_total$rp.2.7 <- as.factor(data_Q_total$rp.2.7)
data_Q_total$rp.2.8 <- as.factor(data_Q_total$rp.2.8)
# Create a new column
data_Q_total <- data_Q_total %>%
mutate(rp.2.int = case_when((rp.2.1 == "Pyydys 1" & rp.2.2 == "Pyydys 2" & rp.2.3 == "" & rp.2.4 == "" & rp.2.5 == "" & rp.2.6 == "" & rp.2.7 == "" & rp.2.8 == "") ~ 1)) %>%
mutate(rp.2.int = case_when(rp.2.int == "1" ~ 1,
is.na(rp.2.int) ~ 0))
# Q3: Kanava 1; 2 tai 3 is the correct answer
# Create a new column
data_Q_total <- data_Q_total %>%
mutate(rp.3.int = case_when(rp.3.quantised == "4" ~ 1,
rp.3.quantised != "4" ~ 0))
# Q4: Ruudukot A; B ja C is the correct answer
# Create a new column
data_Q_total <- data_Q_total %>%
mutate(rp.4.int = case_when(rp.4.quantised == "5" ~ 1,
rp.4.quantised != "5" ~ 0))
# Calculate the sum score for the randomness/probability questions
data_Q_total <- data_Q_total %>%
mutate(RPSum = rp.1.int + rp.2.int + rp.3.int + rp.4.int)
# Omit Randomness-Probability data based on the RP_omit column
data_Q_total$RPSum[which(data_Q_total$RP_omit == 1)] <- NA
data_Q_total$RPSum[which(is.na(data_Q_total$RP_omit))] <- NA
# Describe
describe(data_Q_total$RPSum)
hist(data_Q_total$RPSum,
main = "RPSum",
col = sample(colors(), 1),
xlab = "")
# Omit columns that are not relevant (experiment-general columns) + rows that are not relevant (e.g., practice trials)
data_matreasT <- data_matreasT %>%
select(Participant.Private.ID, Spreadsheet:ANSWER) %>%
filter(display %in% c("TehtÃ¤vÃ¤_6", "TehtÃ¤vÃ¤_8"))
# Omit participants who commented that they had technical issues with this task or did not understand the task
data_matreasT <- data_matreasT %>%
filter(Participant.Private.ID != "5555882" & Participant.Private.ID != "5608075" & Participant.Private.ID != "4887607")
# Exclusion Criteria
# median reaction time <= 500ms
data_matreasT <- data_matreasT %>%
group_by(Participant.Private.ID) %>%
mutate(medianRT = median(Reaction.Time, na.rm = TRUE)) %>%
mutate(Matrix_omit = case_when((medianRT > 500) ~ 0,
(medianRT <= 500) ~ 1))
# Create a separate df for subsetting
data_matrix_omit <- data_matreasT %>%
group_by(Participant.Private.ID) %>%
summarise(Matrix_omit = mean(Matrix_omit)) %>%
select(Participant.Private.ID, Matrix_omit)
# Extract the relevant information
data_matreasT <- data_matreasT %>%
group_by(Participant.Private.ID) %>%
summarise(MatrixCorrectCount = sum(Correct, na.rm = TRUE)) %>%
select(Participant.Private.ID, MatrixCorrectCount)
# Omit data based on matrix_omit
data_matreasT_final <- merge(data_matreasT, data_matrix_omit,
by = "Participant.Private.ID",
all = TRUE)
data_matreasT_final[which(data_matreasT_final$Matrix_omit == 1), "MatrixCorrectCount"] <- NA
data_matreasT_final <- data_matreasT_final %>%
select(Participant.Private.ID, MatrixCorrectCount)
# Exclude participants with invalid responses
data_ValidityFlag <- data_validQ %>%
select(Participant.Private.ID, Validity.quantised)
data_Language <- data_demoQ %>%
select(Participant.Private.ID, Language)
data_matrixValidity <- merge(data_Language, data_ValidityFlag,
by = "Participant.Private.ID",
all = TRUE)
data_matreasT_final <- merge(data_matreasT_final, data_matrixValidity,
by = "Participant.Private.ID",
all = TRUE)
# Create separate df for nonvalid ppts
data_nonvalid_matrix <- data_matreasT_final %>%
filter(Validity.quantised %in% c(2, 3, 4))
data_Q_lang_omit_matrix <- data_matreasT_final %>%
filter(Language != "Sujuva / Ã¤idinkieli" & Language != "Keskitaso / keskusteleva" & Language != "Muu, mikÃ¤?")
data_matreasT_final <- data_matreasT_final %>%
filter(Validity.quantised %in% c("1", NA, "") & Language %in% c("Sujuva / Ã¤idinkieli", "Keskitaso / keskusteleva", "Muu, mikÃ¤?"))
# Delete the irrelevant columns
data_matreasT_final <- data_matreasT_final %>%
select(Participant.Private.ID, MatrixCorrectCount)
# Describe
describe(data_matreasT_final$MatrixCorrectCount)
hist(data_matreasT_final$MatrixCorrectCount,
main = "Matrix Score",
col = rainbow(14),
xlab = "")
# Merge the important columns
data_Q_sub <- data_Q_total %>%
select(Participant.Private.ID, gender, Age, Country, Language, Education, AMean, CMean, EMean, ESMean, OMean, ICuriositySum, DCuriositySum, IH1Sum, IH2Sum, IH3Sum, IH4Sum, CloSum, CogSum, AOTSum, HEscore, HRscore, RPSum, sci_cur, sci_tru, sci_impo, sci_id)
# Add matrix reasoning data to the dataframe
data_Q_sub <- merge(data_Q_sub, data_matreasT_final,
by = "Participant.Private.ID",
all = TRUE)
# Get rid of NaNs or string NAs introduced by calculations
is.nan.data.frame <- function(x)
do.call(cbind, lapply(x, is.nan))
data_Q_sub[is.nan(data_Q_sub)] <- NA
# Numeric variables into numeric
data_Q_sub$MatrixCorrectCount <- as.numeric(data_Q_sub$MatrixCorrectCount)
data_Q_sub$Age <- as.numeric(data_Q_sub$Age)
# Add a column for count of missing data per ppt
data_Q_sub <- data_Q_sub %>%
mutate(missingdata = rowSums(is.na(data_Q_sub)))
# Plot number of missing values
ggplot(data_Q_sub, aes(x = as.factor(Participant.Private.ID), y = missingdata)) +
geom_boxplot(fill = "slateblue", alpha = 0.2) +
xlab("Missing variables count")
# Plot to look for an elbow value
MissingData <- sort(data_Q_sub$missingdata, decreasing = FALSE)
plot(MissingData)
# Correlation between age and amount of missing values
gg_miss_fct(x = data_Q_sub, fct = Age)
cor.test(data_Q_sub$Age, data_Q_sub$missingdata, method = "spearman", exact = FALSE)
# Visualize missing data by demographics
gg_miss_fct(x = data_Q_sub, fct = Language)
gg_miss_fct(x = data_Q_sub, fct = Country)
gg_miss_fct(x = data_Q_sub, fct = gender)
gg_miss_fct(x = data_Q_sub, fct = Education)
# Remove ppts with 16 or fewer variables missing out of 22 (30%)
data_Q_sub <- data_Q_sub %>%
filter(missingdata <= 16)
# Normality of variables: visualize with histograms
vars <- c("AMean", "CMean", "EMean", "OMean", "ESMean", "ICuriositySum", "DCuriositySum", "IH1Sum", "IH2Sum", "IH3Sum", "IH4Sum", "CloSum", "CogSum", "AOTSum", "RPSum", "sci_cur", "sci_tru", "sci_impo", "sci_id", "MatrixCorrectCount")
par(mfrow = c(2, 3))
for (i in 1:length(vars)) {
hist(data_Q_sub[, vars[i]],
main = colnames(data_Q_sub[vars[i]]),
col = sample(colors(), 1),
xlab = "")
}
# Describe: mean, skew, kurtosis
describe(data_Q_sub[, vars])[c("mean", "skew", "kurtosis")]
# Correlation matrix
round(cor(data_Q_sub[, vars], method = "pearson", use = "pairwise.complete.obs"), 2)
abs(round(cor(data_Q_sub[, vars], method = "pearson", use = "pairwise.complete.obs"), 2)) > .85
corrplot(cor(data_Q_sub[, vars], method = "pearson", use = "pairwise.complete.obs"))
# Subset data: exclude demographic variables
data_Q_sub <- data_Q_sub %>%
select(AMean:MatrixCorrectCount)
# Omit experiment-general columns & practice (etc.) rows
data_nogoT <- data_nogoT %>%
select(Participant.Private.ID, Spreadsheet:Answer) %>%
filter(display == "Trials")
# Exclusion Criteria: Trial-Level
# NOTE: Caitlin had used different ones but I think this might be better?
# Reaction time upper boundary: In the Go No-Go task, the screens advance automatically after 1000ms (therefore, a no-go response should be recorded as 1000ms). According to Gorilla, there is also a 16.67ms refresh rate and ~8ms latency between a computer and a mouse. This means that there might be no-go responses up to ~1025ms. This is used as a threshold.
# Reaction time lower boundary: the brain needs time to react (Jaana's 2017 paper) -> 150ms
data_nogoT$Reaction.Time[which(data_nogoT$Reaction.Time >= 1025 | data_nogoT$Reaction.Time <= 150)] <- NA
# Exclusion Criteria: Participant-Level
# <=200ms median RT
# <=60% accuracy
data_nogoT <- data_nogoT %>%
group_by(Participant.Private.ID) %>%
mutate(medianRT = median(Reaction.Time, na.rm = TRUE),
Accuracy = sum(Correct, na.rm = TRUE) / (sum(Correct, na.rm = TRUE) + sum(Incorrect, na.rm = TRUE))) %>%
mutate(go.nogo_omit = case_when((medianRT > 200 & Accuracy > .60) ~ 0,
(medianRT <= 200 | Accuracy <= .60) ~ 1))
# Omit rows based on exclusion criteria
data_nogoT$Response[which(data_nogoT$go.nogo_omit == 1)] <- NA
# Create Signal Detection Theory categories for each row
data_nogoT <- data_nogoT %>%
filter(display == "Trials") %>%
mutate(SDT = case_when((Array %in% c("H.png",  "T.png") & Response == "Go") ~ "Hit",
(Array %in% c("H.png", "T.png") & Response == "No Go") ~ "Miss",
(Array == "N.png" & Response == "Go") ~ "False Alarm",
(Array == "N.png" & Response == "No Go") ~ "Correct Rejection")) %>%
mutate(Hit = case_when(SDT == "Hit" ~ 1,
SDT != "Hit" ~ 0),
Miss = case_when(SDT == "Miss" ~ 1,
SDT != "Miss" ~ 0),
FA = case_when(SDT == "False Alarm" ~ 1,
SDT != "False Alarm" ~ 0),
CR = case_when(SDT == "Correct Rejection" ~ 1,
SDT != "Correct Rejection" ~ 0))
# Calculate the mean and SD of hits and FA (trials with a go response)
data_go_RT <- data_nogoT %>%
filter(SDT %in% c("Hit", "FA")) %>%
group_by(Participant.Private.ID) %>%
summarise(MeanGoRT = mean(Reaction.Time, na.rm = TRUE),
SDGoRT = sd(Reaction.Time, na.rm = TRUE))
# NOTE: Caitlin had removed participants based on these but I think the distributions look ok?
# Compute D-Prime and Bias for each participant
# https://www.rdocumentation.org/packages/psycho/versions/0.6.1/topics/dprime
# http://wise.cgu.edu/wise-tutorials/tutorial-signal-detection-theory/signal-detection-d-defined-2/
data_dprime <- data_nogoT %>%
group_by(Participant.Private.ID) %>%
summarise(dp = dprime(n_hit = sum(Hit, na.rm = TRUE),
n_fa = sum(FA, na.rm = TRUE),
n_miss = sum(Miss, na.rm = TRUE),
n_cr = sum(CR, na.rm = TRUE),
n_targets = 300,
n_distractors = 100))
# Add a row to specify what the five different values given mean
data_dprime$value <- rep_len(c("GNGdprime", "GNGbeta", "GNGaprime", "GNGbppd", "GNGc"),
length.out = nrow(data_dprime))
# Into wide format
data_dprime <- data_dprime %>%
pivot_wider(names_from = value, values_from = dp)
data_GNGdprime <- as.data.frame(data_dprime)
# Create a sum score of each response category for each participant
data_nogoT_final <- data_nogoT %>%
select(Participant.Private.ID, Hit, Miss, FA, CR) %>%
group_by(Participant.Private.ID) %>%
summarise(GNGHitSum = sum(Hit, na.rm = TRUE),
GNGMissSum = sum(Miss, na.rm = TRUE),
GNGFASum = sum(FA, na.rm = TRUE),
GNGCRSum = sum(CR, na.rm = TRUE))
# Combine the dataframes (keep all rows)
data_nogoT_final <- merge(data_nogoT_final, data_GNGdprime,
by = "Participant.Private.ID", all = TRUE)
data_nogoT_final <- merge(data_nogoT_final, data_go_RT,
by = "Participant.Private.ID", all = TRUE)
# Exclude participants with invalid responses
data_ConsentValidity <- data_Q_total %>%
select(Participant.Private.ID, Language, Validity.quantised)
# Final dataset
data_nogoT_final <- merge(data_nogoT_final, data_ConsentValidity,
by = "Participant.Private.ID")
# Exclude participants according to validity + language criteria; exclude the participants who stopped mid-task; exclude participants whose sum score for all hit/miss/cr/fa are 0s
data_nogoT_final <- data_nogoT_final %>%
filter(Validity.quantised %in% c(1, NA, "") & Language %in% c("Sujuva / Ã¤idinkieli", "Keskitaso / keskusteleva","Muu, mikÃ¤?")) %>%
filter(!Participant.Private.ID %in% c("5201242", "4886650", "5117494", "5282634", "5500754", "5552405", "5635997")) %>%
filter(!(GNGHitSum == 0 & GNGMissSum == 0 & GNGFASum == 0 & GNGCRSum == 0))
# Final set of columns
data_nogoT_final <- data_nogoT_final %>%
select(Participant.Private.ID, GNGHitSum, GNGMissSum, GNGFASum, GNGCRSum, MeanGoRT, SDGoRT, GNGdprime, GNGbeta, GNGaprime, GNGbppd, GNGc)
# Change columns into numeric
nogo <- c("GNGHitSum", "GNGMissSum", "GNGFASum", "GNGCRSum", "MeanGoRT", "SDGoRT", "GNGdprime", "GNGbeta", "GNGaprime", "GNGbppd", "GNGc")
for (i in 1:length(nogo)) {
data_nogoT_final[, nogo[i]] <- as.numeric(data_nogoT_final[, nogo[i]])
}
# Describe
describe(data_nogoT_final[, nogo])
par(mfrow = c(2, 3))
for (i in 1:length(nogo)) {
hist(data_nogoT_final[, nogo[i]],
main = colnames(data_nogoT_final[nogo[i]]),
col = sample(colors(), 1),
xlab = "")
}
# Omit experiment-general columns & practice (etc.) rows
data_NavonT <- data_NavonT %>%
select(Participant.Private.ID, Spreadsheet:Image) %>%
filter(display %in% c("task1", "task2")) %>%
filter(Screen.Name == "Screen 3")
# Create a column for consistency
data_NavonT <- data_NavonT %>%
mutate(Consistency = case_when((Image == "bigHsmallH.png" | Image == "bigSsmallS.png") ~ "Consistent",
(Image == "bigHsmallS.png" | Image == "bigSsmallH.png") ~ "Inconsistent"))
data_NavonT$Consistency <- as.factor(data_NavonT$Consistency)
data_NavonT$display <- as.factor(data_NavonT$display)
# Accuracy for each task and consistency
Accuracy_Navon <- data_NavonT %>%
group_by(Consistency, display) %>%
summarise(Accuracy_cond = sum(Correct, na.rm = TRUE) / (sum(Correct, na.rm = TRUE) + sum(Incorrect, na.rm = TRUE)))
# Exclusion Criteria: Trial-Level
# NOTE: Caitlin had used different ones (interquartile range or trials that are outside 2.5 SDs of the mean per participant); we should discuss this; WHY 50ms median RT and not 200ms like in gonogo?
# Based on the histogram, I picked an arbitrary cut-off point of 2000ms to maintain a less skewed distribution but also keep most of the data.
data_NavonT$Reaction.Time[which(data_NavonT$Reaction.Time >= 2000)] <- NA
# Exclusion Criteria: Participant-Level
# <= 50ms median RT
# <= 60% accuracy
# ratio of most frequent answer to all answers >= .95
data_NavonT <- data_NavonT %>%
group_by(Participant.Private.ID) %>%
mutate(MedianRT = median(Reaction.Time, na.rm = TRUE),
Accuracy = sum(Correct, na.rm = TRUE) / (sum(Correct, na.rm = TRUE) + sum(Incorrect, na.rm = TRUE)),
Ratio = max(table(Response), na.rm = TRUE) / length(Response)) %>%
mutate(Navon_omit = case_when(MedianRT > 50 & MedianRT < 1000 & Accuracy > .60 & Ratio < .95 ~ 0,
MedianRT <= 50 | MedianRT >= 1000 | Accuracy <= .60 | Ratio >= .95 ~ 1))
# Omit data based on Navon_omit
data_NavonT$Reaction.Time[which(data_NavonT$Navon_omit == 1)] <- NA
# Subset to only look at correct answers
data_NavonT <- data_NavonT %>%
filter(Correct == "1")
# Ungroup df
data_NavonT <- ungroup(data_NavonT)
# Global SD 1 (consistent trials only)
sd_Global_con <- data_NavonT %>%
filter((Consistency == "Consistent") & display == "task1") %>%
summarise(sd(Reaction.Time, na.rm = TRUE))
sd_Global_con <- sd_Global_con[[1]]
# Local SD 1 (consistent trials only)
sd_Local_con <- data_NavonT %>%
filter((Consistency == "Consistent") & display == "task2") %>%
summarise(sd(Reaction.Time, na.rm = TRUE))
sd_Local_con <- sd_Local_con[[1]]
# Local SD 2 (inconsistent trials only)
sd_Local_incon <- data_NavonT %>%
filter((Consistency == "Inconsistent") & display == "task2") %>%
summarise(sd(Reaction.Time, na.rm = TRUE))
sd_Local_incon <- sd_Local_incon[[1]]
# Calculate pooled SD
# https://www.statisticshowto.com/pooled-standard-deviation/
# Pooled SD consistent (local and global)
pooled_SD_con <- sqrt((sd_Global_con^2 + sd_Local_con^2)/2)
# Pooled SD local (consistent and inconsistent)
pooled_SD_local <- sqrt((sd_Local_incon^2 + sd_Local_con^2)/2)
# Final form of the Navon data
# Select relevant columns and transform the data so columns reflect mean reaction times in each of the four conditions (local/global x consistent/inconsistent)
data_NavonT_final <- data_NavonT %>%
select(Participant.Private.ID, Consistency, display, Reaction.Time) %>%
group_by(Participant.Private.ID, Consistency, display) %>%
summarise(NavonReactionTimeMean = mean(Reaction.Time, na.rm = TRUE),
.groups = "keep") %>%
pivot_wider(names_from = c(Consistency, display),
values_from = NavonReactionTimeMean)
# Global-local precedence index: Standardized mean difference (cohen’s d) in RT between global and local judgments on consistent trials only
data_NavonT_final <- data_NavonT_final %>%
mutate(GlobalToLocalPrecedence = ((Consistent_task1 - Consistent_task2) / as.numeric(pooled_SD_con)))
# Global-to-local interference index: Standardized mean difference (cohen’s d) in RT between inconsistent and consistent trials in local condition only
data_NavonT_final <- data_NavonT_final %>%
mutate(GlobalToLocalInterference = ((Inconsistent_task1 - Consistent_task2) / as.numeric(pooled_SD_local)))
# Exclude nonvalid participants
data_NavonT_final <- merge(data_NavonT_final, data_ConsentValidity,
by = "Participant.Private.ID")
# Validity/languege filter + get rid of participants who stopped mid-task + select relevant columns
data_NavonT_final <- data_NavonT_final %>%
filter(Validity.quantised %in% c(1, NA, "") & Language %in% c("Sujuva / Ã¤idinkieli", "Keskitaso / keskusteleva","Muu, mikÃ¤?")) %>%
filter(Participant.Private.ID != "4907987" & Participant.Private.ID != "4960307" & Participant.Private.ID != "5317567" & Participant.Private.ID != "5372338" & Participant.Private.ID != "5526450" & Participant.Private.ID != "5578226" & Participant.Private.ID != "5626669" & Participant.Private.ID != "5808176") %>%
select(Participant.Private.ID, Consistent_task1, Consistent_task2, Inconsistent_task1, Inconsistent_task2, GlobalToLocalPrecedence, GlobalToLocalInterference)
# NOTE: for some reason there are two NaN rows, removing them here but we should look into why this is?
# one of them was omitted (all RTs replaced with NA), one did not have recorded reaction times in the original data?
data_NavonT_final <- data_NavonT_final %>%
filter(Participant.Private.ID != "5608075" & Participant.Private.ID != "5411218")
# Describe
Navon <- c("Consistent_task1", "Consistent_task2", "Inconsistent_task1", "Inconsistent_task2", "GlobalToLocalPrecedence", "GlobalToLocalInterference")
describe(data_NavonT_final[, Navon])
par(mfrow = c(2, 3))
for (i in 1:length(Navon)) {
hist(data_NavonT_final[, Navon[i]],
main = colnames(data_NavonT_final[Navon[i]]),
col = sample(colors(), 1),
xlab = "")
}
# Omit irrelevant rows and columns
data_NeckerT <- data_NeckerT %>%
select(Participant.Private.ID, Spreadsheet:Image) %>%
filter(display %in% c("Trial 1", "Trial 2"))
# Calculate sum score for how many times space bar was hit in total
data_NeckerT_final <- data_NeckerT %>%
select(Participant.Private.ID, Response) %>%
group_by(Participant.Private.ID) %>%
summarise(NeckerCountTotal = sum(Response == "space", na.rm = TRUE))
# Calculate switches per second
data_NeckerT_final <- data_NeckerT_final %>%
mutate(NeckerTotalRate = NeckerCountTotal/60)
# Merge to exclude participants
data_NeckerT_final <- merge(data_NeckerT_final, data_ConsentValidity,
by = "Participant.Private.ID")
# Subset by validity and consent info + omit the irrelevant columns
data_NeckerT_final <- data_NeckerT_final %>%
filter(Validity.quantised %in% c("1", NA, "") & Language %in% c("Sujuva / Ã¤idinkieli", "Keskitaso / keskusteleva", "Muu, mikÃ¤?")) %>%
filter(Participant.Private.ID != "5385143") %>%
select(Participant.Private.ID, NeckerCountTotal, NeckerTotalRate)
# Describe
describe(data_NeckerT_final$NeckerTotalRate)
hist(data_NeckerT_final$NeckerTotalRate,
main = "Necker Total Rate",
col = sample(colors(), 1),
xlab = "")
# Load data
data_UUT <- read.csv("UUT_scoring_COMBINED_REVISED_Rcsv.csv", sep = ";")
# Rename participant ID column
names(data_UUT)[1] <- "Participant.Private.ID"
# Select relevant columns + calculate the fluency and flexibility sum score for each participant per rater
data_UUT <- data_UUT %>%
select(Participant.Private.ID, S.Fluency, S.Flex, K.Fluency, K.Flex) %>%
group_by(Participant.Private.ID) %>%
summarise(SFlexibilitySum = sum(S.Flex), SFluencySum = sum(S.Fluency),
KFlexibilitySum = sum(K.Flex), KFluencySum = sum(K.Fluency))
# Calculate participant scores as a mean of the two raters' scores (one for fluency, one for flexibility)
data_UUT$FlexSum <- rowMeans(cbind(data_UUT$SFlexibilitySum,
data_UUT$KFlexibilitySum),
na.rm = TRUE)
data_UUT$FluencySum <- rowMeans(cbind(data_UUT$SFluencySum,
data_UUT$KFluencySum),
na.rm = TRUE)
# Check validity
data_UUT <- merge(data_UUT, data_ConsentValidity,
by = "Participant.Private.ID")
data_UUT <- data_UUT %>%
filter(Validity.quantised %in% c("1", NA, "") & Language %in% c("Sujuva / Ã¤idinkieli", "Keskitaso / keskusteleva", "Muu, mikÃ¤?")) %>%
select(Participant.Private.ID, FlexSum, FluencySum)
# Describe
uut <- c("FlexSum", "FluencySum")
# Describe
uut <- c("FlexSum", "FluencySum")
describe(data_UUT[, uut])
par(mfrow = c(2, 1))
for (i in 1:length(uut)) {
hist(data_UUT[, uut[i]],
main = colnames(data_UUT[uut[i]]),
col = sample(colors(), 1),
xlab = "")
}
# Merge dataframes
data_T_total1 <- merge(data_nogoT_final, data_NavonT_final,
by = "Participant.Private.ID", all = TRUE)
data_T_total2 <- merge(data_UUT, data_NeckerT_final,
by = "Participant.Private.ID", all = TRUE)
data_T_total <- merge(data_T_total1, data_T_total2,
by = "Participant.Private.ID", all = TRUE)
rm(data_T_total1, data_T_total2)
# Extract the relevant columns
data_T_sub <- data_T_total %>%
select(Participant.Private.ID, GNGdprime, GNGbeta, meanHitRT, sdHitRT, meanFA_RT, sdFA_RT, GlobalToLocalPrecedence, GlobalToLocalInterference, NeckerTotalRate, FlexSum, FluencySum)
data_T_total
colnames(data_T_total)
# Extract the relevant columns
data_T_sub <- data_T_total %>%
select(Participant.Private.ID, GNGdprime, GNGbeta, meanGoRT, sdGoRT, GlobalToLocalPrecedence, GlobalToLocalInterference, NeckerTotalRate, FlexSum, FluencySum)
# Extract the relevant columns
data_T_sub <- data_T_total %>%
select(Participant.Private.ID, GNGdprime, GNGbeta, MeanGoRT, SDGoRT, GlobalToLocalPrecedence, GlobalToLocalInterference, NeckerTotalRate, FlexSum, FluencySum)
View(data_T_sub)
