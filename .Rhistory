xlab = "")
}
# Describe: mean, skew, kurtosis
describe(data_Q_sub[, vars])[c("mean", "skew", "kurtosis")]
# Correlation matrix
round(cor(data_Q_sub[, vars], method = "pearson", use = "pairwise.complete.obs"), 2)
abs(round(cor(data_Q_sub[, vars], method = "pearson", use = "pairwise.complete.obs"), 2)) > .85
corrplot(cor(data_Q_sub[, vars], method = "pearson", use = "pairwise.complete.obs"))
# Subset data: exclude demographic variables
data_Q_sub <- data_Q_sub %>%
select(Participant.Private.ID, AMean:MatrixCorrectCount)
# Omit experiment-general columns & practice (etc.) rows
data_nogoT <- data_nogoT %>%
select(Participant.Private.ID, Spreadsheet:Answer) %>%
filter(display == "Trials")
# Exclusion Criteria: Trial-Level
# NOTE: Caitlin had used different ones but I think this might be better?
# Reaction time upper boundary: In the Go No-Go task, the screens advance automatically after 1000ms (therefore, a no-go response should be recorded as 1000ms). According to Gorilla, there is also a 16.67ms refresh rate and ~8ms latency between a computer and a mouse. This means that there might be no-go responses up to ~1025ms. This is used as a threshold.
# Reaction time lower boundary: the brain needs time to react (Jaana's 2017 paper) -> 150ms
data_nogoT$Reaction.Time[which(data_nogoT$Reaction.Time >= 1025 | data_nogoT$Reaction.Time <= 150)] <- NA
# Exclusion Criteria: Participant-Level
# <=200ms median RT
# <=60% accuracy
data_nogoT <- data_nogoT %>%
group_by(Participant.Private.ID) %>%
mutate(medianRT = median(Reaction.Time, na.rm = TRUE),
Accuracy = sum(Correct, na.rm = TRUE) / (sum(Correct, na.rm = TRUE) + sum(Incorrect, na.rm = TRUE))) %>%
mutate(go.nogo_omit = case_when((medianRT > 200 & Accuracy > .60) ~ 0,
(medianRT <= 200 | Accuracy <= .60) ~ 1))
# Omit rows based on exclusion criteria
data_nogoT$Response[which(data_nogoT$go.nogo_omit == 1)] <- NA
# Create Signal Detection Theory categories for each row
data_nogoT <- data_nogoT %>%
filter(display == "Trials") %>%
mutate(SDT = case_when((Array %in% c("H.png",  "T.png") & Response == "Go") ~ "Hit",
(Array %in% c("H.png", "T.png") & Response == "No Go") ~ "Miss",
(Array == "N.png" & Response == "Go") ~ "False Alarm",
(Array == "N.png" & Response == "No Go") ~ "Correct Rejection")) %>%
mutate(Hit = case_when(SDT == "Hit" ~ 1,
SDT != "Hit" ~ 0),
Miss = case_when(SDT == "Miss" ~ 1,
SDT != "Miss" ~ 0),
FA = case_when(SDT == "False Alarm" ~ 1,
SDT != "False Alarm" ~ 0),
CR = case_when(SDT == "Correct Rejection" ~ 1,
SDT != "Correct Rejection" ~ 0))
# Calculate the mean and SD of hits and FA (trials with a go response)
data_go_RT <- data_nogoT %>%
filter(SDT %in% c("Hit", "FA")) %>%
group_by(Participant.Private.ID) %>%
summarise(MeanGoRT = mean(Reaction.Time, na.rm = TRUE),
SDGoRT = sd(Reaction.Time, na.rm = TRUE))
# NOTE: Caitlin had removed participants based on these but I think the distributions look ok?
# Compute D-Prime and Bias for each participant
# https://www.rdocumentation.org/packages/psycho/versions/0.6.1/topics/dprime
# http://wise.cgu.edu/wise-tutorials/tutorial-signal-detection-theory/signal-detection-d-defined-2/
data_dprime <- data_nogoT %>%
group_by(Participant.Private.ID) %>%
summarise(dp = dprime(n_hit = sum(Hit, na.rm = TRUE),
n_fa = sum(FA, na.rm = TRUE),
n_miss = sum(Miss, na.rm = TRUE),
n_cr = sum(CR, na.rm = TRUE),
n_targets = 300,
n_distractors = 100))
# Add a row to specify what the five different values given mean
data_dprime$value <- rep_len(c("GNGdprime", "GNGbeta", "GNGaprime", "GNGbppd", "GNGc"),
length.out = nrow(data_dprime))
# Into wide format
data_dprime <- data_dprime %>%
pivot_wider(names_from = value, values_from = dp)
data_GNGdprime <- as.data.frame(data_dprime)
# Create a sum score of each response category for each participant
data_nogoT_final <- data_nogoT %>%
select(Participant.Private.ID, Hit, Miss, FA, CR) %>%
group_by(Participant.Private.ID) %>%
summarise(GNGHitSum = sum(Hit, na.rm = TRUE),
GNGMissSum = sum(Miss, na.rm = TRUE),
GNGFASum = sum(FA, na.rm = TRUE),
GNGCRSum = sum(CR, na.rm = TRUE))
# Combine the dataframes (keep all rows)
data_nogoT_final <- merge(data_nogoT_final, data_GNGdprime,
by = "Participant.Private.ID", all = TRUE)
data_nogoT_final <- merge(data_nogoT_final, data_go_RT,
by = "Participant.Private.ID", all = TRUE)
# Exclude participants with invalid responses
data_ConsentValidity <- data_Q_total %>%
select(Participant.Private.ID, Language, Validity.quantised)
# Final dataset
data_nogoT_final <- merge(data_nogoT_final, data_ConsentValidity,
by = "Participant.Private.ID")
# Exclude participants according to validity + language criteria; exclude the participants who stopped mid-task; exclude participants whose sum score for all hit/miss/cr/fa are 0s
data_nogoT_final <- data_nogoT_final %>%
filter(Validity.quantised %in% c(1, NA, "") & Language %in% c("Sujuva / Ã¤idinkieli", "Keskitaso / keskusteleva","Muu, mikÃ¤?")) %>%
filter(!Participant.Private.ID %in% c("5201242", "4886650", "5117494", "5282634", "5500754", "5552405", "5635997")) %>%
filter(!(GNGHitSum == 0 & GNGMissSum == 0 & GNGFASum == 0 & GNGCRSum == 0))
# Final set of columns
data_nogoT_final <- data_nogoT_final %>%
select(Participant.Private.ID, GNGHitSum, GNGMissSum, GNGFASum, GNGCRSum, MeanGoRT, SDGoRT, GNGdprime, GNGbeta, GNGaprime, GNGbppd, GNGc)
# Change columns into numeric
nogo <- c("GNGHitSum", "GNGMissSum", "GNGFASum", "GNGCRSum", "MeanGoRT", "SDGoRT", "GNGdprime", "GNGbeta", "GNGaprime", "GNGbppd", "GNGc")
for (i in 1:length(nogo)) {
data_nogoT_final[, nogo[i]] <- as.numeric(data_nogoT_final[, nogo[i]])
}
# Describe
describe(data_nogoT_final[, nogo])
par(mfrow = c(2, 3))
for (i in 1:length(nogo)) {
hist(data_nogoT_final[, nogo[i]],
main = colnames(data_nogoT_final[nogo[i]]),
col = sample(colors(), 1),
xlab = "")
}
# Omit experiment-general columns & practice (etc.) rows
data_NavonT <- data_NavonT %>%
select(Participant.Private.ID, Spreadsheet:Image) %>%
filter(display %in% c("task1", "task2")) %>%
filter(Screen.Name == "Screen 3")
# Create a column for consistency
data_NavonT <- data_NavonT %>%
mutate(Consistency = case_when((Image == "bigHsmallH.png" | Image == "bigSsmallS.png") ~ "Consistent",
(Image == "bigHsmallS.png" | Image == "bigSsmallH.png") ~ "Inconsistent"))
data_NavonT$Consistency <- as.factor(data_NavonT$Consistency)
data_NavonT$display <- as.factor(data_NavonT$display)
# Accuracy for each task and consistency
Accuracy_Navon <- data_NavonT %>%
group_by(Consistency, display) %>%
summarise(Accuracy_cond = sum(Correct, na.rm = TRUE) / (sum(Correct, na.rm = TRUE) + sum(Incorrect, na.rm = TRUE)))
# Exclusion Criteria: Trial-Level
# NOTE: Caitlin had used different ones (interquartile range or trials that are outside 2.5 SDs of the mean per participant); we should discuss this; WHY 50ms median RT and not 200ms like in gonogo?
# Based on the histogram, I picked an arbitrary cut-off point of 2000ms to maintain a less skewed distribution but also keep most of the data.
data_NavonT$Reaction.Time[which(data_NavonT$Reaction.Time >= 2000)] <- NA
# Exclusion Criteria: Participant-Level
# <= 50ms median RT
# <= 60% accuracy
# ratio of most frequent answer to all answers >= .95
data_NavonT <- data_NavonT %>%
group_by(Participant.Private.ID) %>%
mutate(MedianRT = median(Reaction.Time, na.rm = TRUE),
Accuracy = sum(Correct, na.rm = TRUE) / (sum(Correct, na.rm = TRUE) + sum(Incorrect, na.rm = TRUE)),
Ratio = max(table(Response), na.rm = TRUE) / length(Response)) %>%
mutate(Navon_omit = case_when(MedianRT > 50 & MedianRT < 1000 & Accuracy > .60 & Ratio < .95 ~ 0,
MedianRT <= 50 | MedianRT >= 1000 | Accuracy <= .60 | Ratio >= .95 ~ 1))
# Omit data based on Navon_omit
data_NavonT$Reaction.Time[which(data_NavonT$Navon_omit == 1)] <- NA
# Subset to only look at correct answers
data_NavonT <- data_NavonT %>%
filter(Correct == "1")
# Ungroup df
data_NavonT <- ungroup(data_NavonT)
# Global SD 1 (consistent trials only)
sd_Global_con <- data_NavonT %>%
filter((Consistency == "Consistent") & display == "task1") %>%
summarise(sd(Reaction.Time, na.rm = TRUE))
sd_Global_con <- sd_Global_con[[1]]
# Local SD 1 (consistent trials only)
sd_Local_con <- data_NavonT %>%
filter((Consistency == "Consistent") & display == "task2") %>%
summarise(sd(Reaction.Time, na.rm = TRUE))
sd_Local_con <- sd_Local_con[[1]]
# Local SD 2 (inconsistent trials only)
sd_Local_incon <- data_NavonT %>%
filter((Consistency == "Inconsistent") & display == "task2") %>%
summarise(sd(Reaction.Time, na.rm = TRUE))
sd_Local_incon <- sd_Local_incon[[1]]
# Calculate pooled SD
# https://www.statisticshowto.com/pooled-standard-deviation/
# Pooled SD consistent (local and global)
pooled_SD_con <- sqrt((sd_Global_con^2 + sd_Local_con^2)/2)
# Pooled SD local (consistent and inconsistent)
pooled_SD_local <- sqrt((sd_Local_incon^2 + sd_Local_con^2)/2)
# Final form of the Navon data
# Select relevant columns and transform the data so columns reflect mean reaction times in each of the four conditions (local/global x consistent/inconsistent)
data_NavonT_final <- data_NavonT %>%
select(Participant.Private.ID, Consistency, display, Reaction.Time) %>%
group_by(Participant.Private.ID, Consistency, display) %>%
summarise(NavonReactionTimeMean = mean(Reaction.Time, na.rm = TRUE),
.groups = "keep") %>%
pivot_wider(names_from = c(Consistency, display),
values_from = NavonReactionTimeMean)
# Global-local precedence index: Standardized mean difference (cohen’s d) in RT between global and local judgments on consistent trials only
data_NavonT_final <- data_NavonT_final %>%
mutate(GlobalToLocalPrecedence = ((Consistent_task1 - Consistent_task2) / as.numeric(pooled_SD_con)))
# Global-to-local interference index: Standardized mean difference (cohen’s d) in RT between inconsistent and consistent trials in local condition only
data_NavonT_final <- data_NavonT_final %>%
mutate(GlobalToLocalInterference = ((Inconsistent_task1 - Consistent_task2) / as.numeric(pooled_SD_local)))
# Exclude nonvalid participants
data_NavonT_final <- merge(data_NavonT_final, data_ConsentValidity,
by = "Participant.Private.ID")
# Validity/languege filter + get rid of participants who stopped mid-task + select relevant columns
data_NavonT_final <- data_NavonT_final %>%
filter(Validity.quantised %in% c(1, NA, "") & Language %in% c("Sujuva / Ã¤idinkieli", "Keskitaso / keskusteleva","Muu, mikÃ¤?")) %>%
filter(Participant.Private.ID != "4907987" & Participant.Private.ID != "4960307" & Participant.Private.ID != "5317567" & Participant.Private.ID != "5372338" & Participant.Private.ID != "5526450" & Participant.Private.ID != "5578226" & Participant.Private.ID != "5626669" & Participant.Private.ID != "5808176") %>%
select(Participant.Private.ID, Consistent_task1, Consistent_task2, Inconsistent_task1, Inconsistent_task2, GlobalToLocalPrecedence, GlobalToLocalInterference)
# NOTE: for some reason there are two NaN rows, removing them here but we should look into why this is?
# one of them was omitted (all RTs replaced with NA), one did not have recorded reaction times in the original data?
data_NavonT_final <- data_NavonT_final %>%
filter(Participant.Private.ID != "5608075" & Participant.Private.ID != "5411218")
# Describe
Navon <- c("Consistent_task1", "Consistent_task2", "Inconsistent_task1", "Inconsistent_task2", "GlobalToLocalPrecedence", "GlobalToLocalInterference")
describe(data_NavonT_final[, Navon])
par(mfrow = c(2, 3))
for (i in 1:length(Navon)) {
hist(data_NavonT_final[, Navon[i]],
main = colnames(data_NavonT_final[Navon[i]]),
col = sample(colors(), 1),
xlab = "")
}
# Omit irrelevant rows and columns
data_NeckerT <- data_NeckerT %>%
select(Participant.Private.ID, Spreadsheet:Image) %>%
filter(display %in% c("Trial 1", "Trial 2"))
# Calculate sum score for how many times space bar was hit in total
data_NeckerT_final <- data_NeckerT %>%
select(Participant.Private.ID, Response) %>%
group_by(Participant.Private.ID) %>%
summarise(NeckerCountTotal = sum(Response == "space", na.rm = TRUE))
# Calculate switches per second
data_NeckerT_final <- data_NeckerT_final %>%
mutate(NeckerTotalRate = NeckerCountTotal/60)
# Merge to exclude participants
data_NeckerT_final <- merge(data_NeckerT_final, data_ConsentValidity,
by = "Participant.Private.ID")
# Subset by validity and consent info + omit the irrelevant columns
data_NeckerT_final <- data_NeckerT_final %>%
filter(Validity.quantised %in% c("1", NA, "") & Language %in% c("Sujuva / Ã¤idinkieli", "Keskitaso / keskusteleva", "Muu, mikÃ¤?")) %>%
filter(Participant.Private.ID != "5385143") %>%
select(Participant.Private.ID, NeckerCountTotal, NeckerTotalRate)
# Describe
describe(data_NeckerT_final$NeckerTotalRate)
hist(data_NeckerT_final$NeckerTotalRate,
main = "Necker Total Rate",
col = sample(colors(), 1),
xlab = "")
# Load data
data_UUT <- read.csv("UUT_scoring_COMBINED_REVISED_Rcsv.csv", sep = ";")
# Rename participant ID column
names(data_UUT)[1] <- "Participant.Private.ID"
# Select relevant columns + calculate the fluency and flexibility sum score for each participant per rater
data_UUT <- data_UUT %>%
select(Participant.Private.ID, S.Fluency, S.Flex, K.Fluency, K.Flex) %>%
group_by(Participant.Private.ID) %>%
summarise(SFlexibilitySum = sum(S.Flex), SFluencySum = sum(S.Fluency),
KFlexibilitySum = sum(K.Flex), KFluencySum = sum(K.Fluency))
# Calculate participant scores as a mean of the two raters' scores (one for fluency, one for flexibility)
data_UUT$FlexSum <- rowMeans(cbind(data_UUT$SFlexibilitySum,
data_UUT$KFlexibilitySum),
na.rm = TRUE)
data_UUT$FluencySum <- rowMeans(cbind(data_UUT$SFluencySum,
data_UUT$KFluencySum),
na.rm = TRUE)
# Check validity + extract relevant columns
data_UUT <- merge(data_UUT, data_ConsentValidity,
by = "Participant.Private.ID")
data_UUT <- data_UUT %>%
filter(Validity.quantised %in% c("1", NA, "") & Language %in% c("Sujuva / Ã¤idinkieli", "Keskitaso / keskusteleva", "Muu, mikÃ¤?")) %>%
select(Participant.Private.ID, FlexSum, FluencySum)
# Describe
uut <- c("FlexSum", "FluencySum")
describe(data_UUT[, uut])
par(mfrow = c(2, 1))
for (i in 1:length(uut)) {
hist(data_UUT[, uut[i]],
main = colnames(data_UUT[uut[i]]),
col = sample(colors(), 1),
xlab = "")
}
# Merge dataframes
data_T_total1 <- merge(data_nogoT_final, data_NavonT_final,
by = "Participant.Private.ID", all = TRUE)
data_T_total2 <- merge(data_UUT, data_NeckerT_final,
by = "Participant.Private.ID", all = TRUE)
data_T_total <- merge(data_T_total1, data_T_total2,
by = "Participant.Private.ID", all = TRUE)
rm(data_T_total1, data_T_total2)
# Extract the relevant columns
data_T_sub <- data_T_total %>%
select(Participant.Private.ID, GNGdprime, GNGbeta, MeanGoRT, SDGoRT, GlobalToLocalPrecedence, GlobalToLocalInterference, NeckerTotalRate, FlexSum, FluencySum)
# Participant ID into factor
data_T_sub$Participant.Private.ID <- as.factor(data_T_sub$Participant.Private.ID)
# Add a column for count of missing data per ppt
data_T_sub <- data_T_sub %>%
mutate(missingtasks = rowSums(is.na(data_T_sub)))
table(data_T_sub$missingtasks)
plot(sort(data_T_sub$missingtasks))
# NOTE: which cut off? 4 was used before but not sure as there is no elbow here
# Merge demographic information to investigate missingness
# NOTE: what to do about the age correlation?
data_T_all <- data_Q_total %>%
select(Participant.Private.ID, gender, Age, Country, Language, Education)
data_T_sub <- merge(data_T_all, data_T_sub,
by = "Participant.Private.ID", all = TRUE)
gg_miss_fct(x = data_T_sub, fct = Age)
cor.test(data_T_sub$Age, data_T_sub$missing,
method = "spearman", exact = FALSE)
# Remove participants with 5 or more missing task variables
data_T_sub <- data_T_sub %>%
group_by(Participant.Private.ID) %>%
filter(missingtasks < 4)
# Omit irrelevant columns
data_T_sub <- data_T_sub %>%
select(-missingtasks)
# Normality of variables: visualize with histograms
data_T_sub <- as.data.frame(data_T_sub)
vars_T <- c("GNGdprime", "GNGbeta", "MeanGoRT", "SDGoRT", "GlobalToLocalPrecedence", "GlobalToLocalInterference", "NeckerTotalRate", "FlexSum", "FluencySum")
par(mfrow = c(2, 2))
for (i in 1:length(vars_T)) {
hist(data_T_sub[, vars_T[i]],
main = colnames(data_T_sub[vars_T[i]]),
col = sample(colors(), 1),
xlab = "")
}
# Describe: mean, skew, kurtosis
describe(data_T_sub[, vars_T])[c("mean", "skew", "kurtosis")]
# Correlation matrix
round(cor(data_T_sub[, vars_T], method = "pearson",
use = "pairwise.complete.obs"), 2)
abs(round(cor(data_T_sub[, vars_T], method = "pearson",
use = "pairwise.complete.obs"), 2)) > .85
corrplot(cor(data_T_sub[, vars_T], method = "pearson",
use = "pairwise.complete.obs"))
# NOTE: we should discuss this, only transforming GNGbeta for now
data_T_sub$GNGbetalog <- log10(data_T_sub$GNGbeta)
# NOTE: we should also discuss which variables to remove; here removing GlobalToLocalInterference and FluencySum
data_T_sub <- data_T_sub %>%
select(Participant.Private.ID, GNGdprime, GNGbetalog, MeanGoRT, SDGoRT, GlobalToLocalPrecedence, NeckerTotalRate, FlexSum)
data_all_sub_cleaned <- merge(data_Q_sub, data_T_sub,
by = "Participant.Private.ID",
all = TRUE)
# NOTE: should we check missingness again?
missing_data <- data_all_sub_cleaned %>%
mutate(missing = rowSums(is.na(data_all_sub_cleaned))) %>%
select(Participant.Private.ID, missing)
# Ungroup the dataframe
ungroup(data_all_sub_cleaned)
data_all_sub_cleaned <- data_all_sub_cleaned %>%
select(AMean:FlexSum)
# Binary variables
binary <- c("HEscore", "HRscore")
# Go No-Go variables
gng <- c("GNGdprime", "GNGbetalog", "MeanGoRT", "SDGoRT")
# NOTE: do we want to get rid of them for analysis? binary might be an issue still but gng are ok in terms of correlations
# will add the code when it works on its own
# Exclude nonvalid participants
data_citiT <- merge(data_citiT, data_ConsentValidity,
by = "Participant.Private.ID")
# Validity/languege filter + select relevant columns
data_citiT <- data_citiT %>%
filter(Validity.quantised %in% c(1, NA, "") & Language %in% c("Sujuva / Ã¤idinkieli", "Keskitaso / keskusteleva","Muu, mikÃ¤?")) %>%
select(Participant.Private.ID, Spreadsheet.Row:Reaction.Time, Response, display, Text, Source_topic)
# Remove the Instructions/Finish rows
data_citiT <- data_citiT %>%
filter(display != "Instructions" & display!= "Finish")
# Add a column to code the source authority: 1=Authority, 2=Personal
data_citiT <- data_citiT %>%
mutate(source_authority = case_when((Source_topic == "Turvetuotanto on ajettava alas â€“ oikeudenmukaisesti") ~ 1,
(Source_topic == "Suo syntyy uudestaan") ~ 1,
(Source_topic == "Palaako peltoviljely pÃ¤Ã¤osin kangasmaille?") ~ 1,
(Source_topic == "Punnittua puhetta turpeesta") ~ 2,
(Source_topic == "Turve on uusiutuva luonnonvara") ~ 2,
(Source_topic == "Auttaa akneen, reumaan, selluliittiin ja hiustenlÃ¤htÃ¶Ã¶n â€“ mikÃ¤ se on?") ~ 2))
# Add a column to code the source quality: 1=Reliable, 2=Unreliable, 3=irrelevant
data_citiT <- data_citiT %>%
mutate(source_quality = case_when((Source_topic == "Turvetuotanto on ajettava alas â€“ oikeudenmukaisesti") ~ 1,
(Source_topic == "Suo syntyy uudestaan") ~ 2,
(Source_topic == "Palaako peltoviljely pÃ¤Ã¤osin kangasmaille?") ~ 3,
(Source_topic == "Punnittua puhetta turpeesta") ~ 1,
(Source_topic == "Turve on uusiutuva luonnonvara") ~ 2,
(Source_topic == "Auttaa akneen, reumaan, selluliittiin ja hiustenlÃ¤htÃ¶Ã¶n â€“ mikÃ¤ se on?") ~ 3))
# Extract reaction time for the entire task to a different df
data_citiT_overallRTs <- data_citiT %>%
filter(Trial.Number == "END TASK") %>%
select(Participant.Private.ID, Reaction.Time)
names(data_citiT_overallRTs)[2] <- "overall_rt"
# Remove BEGIN TASK and END TASK rows from df
data_citiT <- data_citiT %>%
filter(!Trial.Number %in% c("END TASK", "BEGIN TASK"))
# Extract reaction times for each source
# NOTE: NOT SURE ABOUT THIS
# Baseline SAM and 4 baseline topic questions (about the petition) coded as order = 1; Rest of the trials coded based on Source_topic so that order numbers always refer to the same source
data_citiT <- data_citiT %>%
mutate(Order = case_when((Source_topic == "") ~ 1,
(Source_topic == "Auttaa akneen, reumaan, selluliittiin ja hiustenlÃ¤htÃ¶Ã¶n â€“ mikÃ¤ se on?") ~ 2,
(Source_topic == "Palaako peltoviljely pÃ¤Ã¤osin kangasmaille?") ~ 3,
(Source_topic == "Punnittua puhetta turpeesta") ~ 4,
(Source_topic == "Suo syntyy uudestaan") ~ 5,
(Source_topic == "Turve on uusiutuva luonnonvara") ~ 6,
(Source_topic == "Turvetuotanto on ajettava alas â€“ oikeudenmukaisesti") ~ 7))
data_citiT_srcRTs <- data_citiT %>%
filter(Zone.Name == "advancementZone") %>%
select(Participant.Private.ID, Order, display, Reaction.Time)
names(data_citiT_srcRTs)[4] <- "src_rt"
# Combine overall RTs and source RTs
data_citiT_RTs <- merge(data_citiT_overallRTs, data_citiT_srcRTs,
by = "Participant.Private.ID",
all = TRUE)
# Overall RTs
describe((data_citiT_overallRTs$overall_rt/1000)/60) # in minutes
hist((data_citiT_overallRTs$overall_rt/1000)/60) # in minutes
plot(sort((data_citiT_overallRTs$overall_rt/1000)/60)) # in minutes
sort((data_citiT_overallRTs$overall_rt/1000)/60)
# Reading RTs
describe((data_citiT_RTs$src_rt/1000)/60) # in minutes
hist((data_citiT_RTs$src_rt/1000)/60) # in minutes
hist((data_citiT_RTs$src_rt/1000)/60, breaks = 100) # in minutes
plot(sort((data_citiT_RTs$src_rt/1000)/60)) # in minutes
# Reading RTs (for online sources)
which(data_citiT_RTs$display == "Reading")
# Reading RTs (for online sources)
which(data_citiT_RTs$display == "PDF")
# Reading RTs (for online sources)
describe((data_citiT_RTs$src_rt[which(data_citiT_RTs$display == "Reading")]/1000)/60) # in minutes
hist((data_citiT_RTs$src_rt[which(data_citiT_RTs$display == "Reading")]/1000)/60, breaks = 100) # in minutes
plot(sort((data_citiT_RTs$src_rt[which(data_citiT_RTs$display == "Reading")]/1000)/60)) # in minutes
# Reading RTs (for PDF)
describe((data_citiT_RTs$src_rt[which(data_citiT_RTs$display == "PDF")]/1000)/60) # in minutes
hist((data_citiT_RTs$src_rt[which(data_citiT_RTs$display == "PDF")]/1000)/60, breaks = 100) # in minutes
plot(sort((data_citiT_RTs$src_rt[which(data_citiT_RTs$display == "PDF")]/1000)/60)) # in minutes
tapply(X = data_citiT_RTs$src_rt, INDEX = data_citiT_RTs$display, FUN = summary)
tapply(X = data_citiT_RTs$src_rt, INDEX = data_citiT_RTs$display, FUN = describe)
tapply(X = data_citiT_RTs$src_rt, INDEX = data_citiT_RTs$display, FUN = describe())
60*1000
tapply(X = data_citiT_RTs$src_rt/60000,
INDEX = data_citiT_RTs$display,
FUN = summary)
# Reading RTs (for online sources)
describe((data_citiT_RTs$src_rt[which(data_citiT_RTs$display == "Reading")]/1000)/60) # in minutes
# Reading RTs (for PDF)
describe((data_citiT_RTs$src_rt[which(data_citiT_RTs$display == "PDF")]/1000)/60) # in minutes
hist(data_citiT_RTs$src_rt[which(data_citiT_RTs$display == "Reading")]/60000,
main = "Online source RTs",
col = sample(colors(), 1),
xlab = "")
par(mfrow = c(1, 2))
hist(data_citiT_RTs$src_rt[which(data_citiT_RTs$display == "Reading")]/60000,
main = "Online source RTs",
col = sample(colors(), 1),
xlab = "",
breaks = 100)
hist(data_citiT_RTs$src_rt[which(data_citiT_RTs$display == "PDF")]/60000,
main = "PDF RTs",
col = sample(colors(), 1),
xlab = "",
breaks = 100)
par(mfrow = c(1, 2))
hist(data_citiT_RTs$src_rt[which(data_citiT_RTs$display == "Reading")]/60000,
main = "Online source RTs",
col = sample(colors(), 1),
xlab = "",
breaks = 100)
hist(data_citiT_RTs$src_rt[which(data_citiT_RTs$display == "PDF")]/60000,
main = "PDF RTs",
col = sample(colors(), 1),
xlab = "",
breaks = 100)
par(mfrow = c(1, 2))
hist(data_citiT_RTs$src_rt[which(data_citiT_RTs$display == "Reading")]/60000,
main = "Online source RTs",
col = sample(colors(), 1),
xlab = "",
breaks = 100)
hist(data_citiT_RTs$src_rt[which(data_citiT_RTs$display == "PDF")]/60000,
main = "PDF RTs",
col = sample(colors(), 1),
xlab = "",
breaks = 100)
plot(sort(data_citiT_RTs$src_rt[which(data_citiT_RTs$display == "Reading")]/60000))
plot(sort(data_citiT_RTs$src_rt[which(data_citiT_RTs$display == "PDF")]/60000))
plot(sort(data_citiT_RTs$src_rt[which(data_citiT_RTs$display == "Reading")]/60000),
ylab = "Online source")
plot(sort(data_citiT_RTs$src_rt[which(data_citiT_RTs$display == "PDF")]/60000),
ylab = "PDF")
# Extract reaction times for the Likert questions
data_citiT_likertRTs <- data_citiT %>%
filter(Zone.Type != "continue_button") %>%
select(Participant.Private.ID, Order, Reaction.Time, Zone.Type)
# Describe
describe(data_citiT_likertRTs$Reaction.Time/60000) # in minutes
# Describe
describe(data_citiT_likertRTs$Reaction.Time/1000) # in seconds
hist(data_citiT_likertRTs$src_rt/1000, breaks = 100) # in minutes
hist(data_citiT_likertRTs$Reaction.Time/1000, breaks = 100) # in minutes
hist(data_citiT_RTs$src_rt,
main = "Online source RTs",
col = sample(colors(), 1),
xlab = "",
breaks = 100)plot(sort(data_citiT_RTs$src_rt/60000)) # in minutes
hist(data_citiT_RTs$src_rt,
main = "Online source RTs",
col = sample(colors(), 1),
xlab = "",
breaks = 100)
hist(data_citiT_RTs$src_rt/60000,
main = "Online source RTs",
col = sample(colors(), 1),
xlab = "",
breaks = 100)
hist(data_citiT_likertRTs$Reaction.Time/1000, # in seconds
main = "Likert RTs",
col = sample(colors(), 1),
xlab = "",
breaks = 100)
plot(sort(data_citiT_likertRTs$Reaction.Time/1000)) # in seconds
# Describe
describe(data_citiT_likertRTs$Reaction.Time/1000) # in seconds
hist(data_citiT_likertRTs$Reaction.Time/1000, # in seconds
main = "Likert RTs",
col = sample(colors(), 1),
xlab = "",
breaks = 100)
plot(sort(data_citiT_likertRTs$Reaction.Time/1000)) # in seconds
# Describe
describe(data_citiT_likertRTs$Reaction.Time/1000) # in seconds
