data_Q_total[which(data_Q_total$total_N == 0 & data_Q_total$total_HR == 0 & data_Q_total$total_HE == 0 & data_Q_total$total_Neither == 0), heur] <- NA
# Check that sum scores add up to 6
sum_heur <- data_Q_total %>%
select(total_N, total_HR, total_HE, total_Neither) %>%
rowwise() %>%
mutate(sum_heur = total_N + total_HR + total_HE + total_Neither) %>%
select(sum_heur)
table(sum_heur$sum_heur)
# Describe: sum score level
describe(data_Q_total[, heur])
par(mfrow = c(2, 2))
for (i in 1:length(heur)) {
hist(data_Q_total[, heur[i]],
main = colnames(data_Q_total[heur[i]]),
col = sample(colors(), 1),
xlab = "")
}
# Create binary heuristic scores (0=0, 1=everything else)
data_Q_total <- data_Q_total %>%
mutate(HEscore = case_when(total_HE == 0 ~ 0,
total_HE == 1 ~ 1,
total_HE == 2 ~ 1),
HRscore = case_when(total_HR == 0 ~ 0,
total_HR == 1 ~ 1,
total_HR == 2 ~ 1))
sort(rowSums(is.na(data_Q_total)))
# Q1: Sairaala B is the correct answer
# Create a new column
data_Q_total$rp.1 <- as.factor(data_Q_total$rp.1)
data_Q_total <- data_Q_total %>%
mutate(rp.1.int = case_when(rp.1 == "Sairaalassa B (jossa syntyy 10 lasta pÃ¤ivÃ¤ssÃ¤)." ~ 1,
rp.1 %in% c("Sairaalassa A (jossa syntyy 50 lasta pÃ¤ivÃ¤ssÃ¤).", "TÃ¤mÃ¤ on yhtÃ¤ todennÃ¤kÃ¶istÃ¤ kummassakin sairaalassa.") ~ 0))
# Q2: Pyydys1 AND Pyydys 2 is the correct combination
data_Q_total$rp.2.1 <- as.factor(data_Q_total$rp.2.1)
data_Q_total$rp.2.2 <- as.factor(data_Q_total$rp.2.2)
data_Q_total$rp.2.3 <- as.factor(data_Q_total$rp.2.3)
data_Q_total$rp.2.4 <- as.factor(data_Q_total$rp.2.4)
data_Q_total$rp.2.5 <- as.factor(data_Q_total$rp.2.5)
data_Q_total$rp.2.6 <- as.factor(data_Q_total$rp.2.6)
data_Q_total$rp.2.7 <- as.factor(data_Q_total$rp.2.7)
data_Q_total$rp.2.8 <- as.factor(data_Q_total$rp.2.8)
# Create a new column
# NAs are initially coded as 2 and wrong answers (0s) as NAs; then these are recoded
data_Q_total <- data_Q_total %>%
mutate(rp.2.int = case_when((rp.2.1 == "Pyydys 1" & rp.2.2 == "Pyydys 2" & rp.2.3 == "" & rp.2.4 == "" & rp.2.5 == "" & rp.2.6 == "" & rp.2.7 == "" & rp.2.8 == "") ~ 1,
is.na(rp.2.8) ~ 2))
data_Q_total[which(is.na(data_Q_total$rp.2.int)), "rp.2.int"] <- 0
data_Q_total[which(data_Q_total$rp.2.int == 2), "rp.2.int"] <- NA
# Q3: Kanava 1; 2 tai 3 is the correct answer
# Create a new column
data_Q_total <- data_Q_total %>%
mutate(rp.3.int = case_when(rp.3.quantised == "4" ~ 1,
rp.3.quantised != "4" ~ 0))
# Q4: Ruudukot A; B ja C is the correct answer
# Create a new column
data_Q_total <- data_Q_total %>%
mutate(rp.4.int = case_when(rp.4.quantised == "5" ~ 1,
rp.4.quantised != "5" ~ 0))
# Calculate the sum score for the randomness/probability questions
data_Q_total <- data_Q_total %>%
mutate(RPSum = rp.1.int + rp.2.int + rp.3.int + rp.4.int)
# Describe
describe(data_Q_total$RPSum)
hist(data_Q_total$RPSum,
main = "RPSum",
col = sample(colors(), 1),
xlab = "")
# Omit columns that are not relevant (experiment-general columns) + rows that are not relevant (e.g., practice trials)
data_matreasT <- data_matreasT %>%
select(Participant.Private.ID, Spreadsheet:ANSWER) %>%
filter(display %in% c("TehtÃ¤vÃ¤_6", "TehtÃ¤vÃ¤_8"))
# Omit participants who commented that they had technical issues with this task or did not understand the task
data_matreasT <- data_matreasT %>%
filter(Participant.Private.ID != "5555882" & Participant.Private.ID != "5608075" & Participant.Private.ID != "4887607")
# Extract the relevant information
data_matreasT_final <- data_matreasT %>%
group_by(Participant.Private.ID) %>%
summarise(MatrixCorrectCount = sum(Correct, na.rm = TRUE)) %>%
select(Participant.Private.ID, MatrixCorrectCount)
# Exclude participants with invalid responses
data_ValidityFlag <- data_validQ %>%
select(Participant.Private.ID, Validity.quantised)
data_Language <- data_demoQ %>%
select(Participant.Private.ID, Language)
data_matrixValidity <- merge(data_Language, data_ValidityFlag,
by = "Participant.Private.ID",
all = TRUE)
data_matreasT_final <- merge(data_matreasT_final, data_matrixValidity,
by = "Participant.Private.ID",
all = TRUE)
# Create separate df for nonvalid ppts
data_nonvalid_matrix <- data_matreasT_final %>%
filter(Validity.quantised %in% c(2, 3, 4))
data_Q_lang_omit_matrix <- data_matreasT_final %>%
filter(Language != "Sujuva / Ã¤idinkieli" & Language != "Keskitaso / keskusteleva" & Language != "Muu, mikÃ¤?")
data_matreasT_final <- data_matreasT_final %>%
filter(Validity.quantised %in% c("1", NA, "") & Language %in% c("Sujuva / Ã¤idinkieli", "Keskitaso / keskusteleva", "Muu, mikÃ¤?"))
# Delete the irrelevant columns
data_matreasT_final <- data_matreasT_final %>%
select(Participant.Private.ID, MatrixCorrectCount)
# Describe
describe(data_matreasT_final$MatrixCorrectCount)
hist(data_matreasT_final$MatrixCorrectCount,
main = "Matrix Score",
col = rainbow(14),
xlab = "")
# Merge the important columns
data_Q_sub <- data_Q_total %>%
select(Participant.Private.ID, gender, Age, Country, Language, Education, AMean, CMean, EMean, ESMean, OMean, ICuriositySum, DCuriositySum, IH1Sum, IH2Sum, IH3Sum, IH4Sum, CloSum, CogSum, AOTSum, HEscore, HRscore, RPSum, sci_cur, sci_tru, sci_impo, sci_id)
# Add matrix reasoning data to the dataframe
data_Q_sub <- merge(data_Q_sub, data_matreasT_final,
by = "Participant.Private.ID",
all = TRUE)
# Get rid of NaNs or string NAs introduced by calculations
is.nan.data.frame <- function(x)
do.call(cbind, lapply(x, is.nan))
data_Q_sub[is.nan(data_Q_sub)] <- NA
# Numeric variables into numeric
data_Q_sub$MatrixCorrectCount <- as.numeric(data_Q_sub$MatrixCorrectCount)
data_Q_sub$Age <- as.numeric(data_Q_sub$Age)
# Questionnaire variables
q_var <- c("AMean", "CMean", "EMean", "ESMean", "OMean", "ICuriositySum", "DCuriositySum", "IH1Sum", "IH2Sum", "IH3Sum", "IH4Sum", "CloSum", "CogSum", "AOTSum", "HEscore", "HRscore", "RPSum", "sci_cur", "sci_tru", "sci_impo", "sci_id", "MatrixCorrectCount")
# Add a column for count of missing data for questionnaire variables per participant
data_Q_sub$q_missingdata <- rowSums(is.na(data_Q_sub[, q_var]))
# Plot number of missing values
ggplot(data_Q_sub, aes(x = as.factor(Participant.Private.ID), y = q_missingdata)) +
geom_boxplot(fill = "slateblue", alpha = 0.2) +
xlab("Missing variables count")
# Plot to look for an elbow value
plot(sort(data_Q_sub$q_missingdata))
# Correlation between age and amount of missing values
gg_miss_fct(x = data_Q_sub, fct = Age)
cor.test(data_Q_sub$Age, data_Q_sub$q_missingdata,
method = "spearman", exact = FALSE)
# Visualize missing data by demographics
gg_miss_fct(x = data_Q_sub, fct = Language)
gg_miss_fct(x = data_Q_sub, fct = Country)
gg_miss_fct(x = data_Q_sub, fct = gender)
gg_miss_fct(x = data_Q_sub, fct = Education)
# Remove ppts with 8 or more variables missing out of 22 (~32%)
data_Q_sub <- data_Q_sub %>%
filter(q_missingdata < 8) %>%
select(-q_missingdata)
# Questionnaire variables without binary ones
q_var_con <- c("AMean", "CMean", "EMean", "ESMean", "OMean", "ICuriositySum", "DCuriositySum", "IH1Sum", "IH2Sum", "IH3Sum", "IH4Sum", "CloSum", "CogSum", "AOTSum", "RPSum", "sci_cur", "sci_tru", "sci_impo", "sci_id", "MatrixCorrectCount")
# Normality of variables: visualize with histograms
par(mfrow = c(2, 3))
for (i in 1:length(q_var_con)) {
hist(data_Q_sub[, q_var_con[i]],
main = colnames(data_Q_sub[q_var_con[i]]),
col = sample(colors(), 1),
xlab = "")
}
# Describe: mean, skew, kurtosis
describe(data_Q_sub[, q_var_con])
# Correlation matrix
round(cor(data_Q_sub[, q_var_con], method = "pearson",
use = "pairwise.complete.obs"), 2)
abs(round(cor(data_Q_sub[, q_var_con], method = "pearson",
use = "pairwise.complete.obs"), 2)) > .85
corrplot(cor(data_Q_sub[, q_var_con], method = "pearson",
use = "pairwise.complete.obs"))
corrplot(cor(data_Q_sub[, q_var], method = "spearman",
use = "pairwise.complete.obs"))
# Subset data: exclude demographic variables
data_Q_sub <- data_Q_sub[, c("Participant.Private.ID", q_var)]
# Omit experiment-general columns & practice (etc.) rows
data_nogoT <- data_nogoT %>%
select(Participant.Private.ID, Spreadsheet:Answer) %>%
filter(display == "Trials")
# Exclusion Criteria: Trial-Level
# RT <= 150ms or >= 1025ms
data_nogoT$Reaction.Time[which(data_nogoT$Reaction.Time >= 1025 | data_nogoT$Reaction.Time <= 150)] <- NA
# Exclusion Criteria: Participant-Level
# <=60% accuracy
data_nogoT <- data_nogoT %>%
group_by(Participant.Private.ID) %>%
mutate(Accuracy = sum(Correct, na.rm = TRUE) / (sum(Correct, na.rm = TRUE) + sum(Incorrect, na.rm = TRUE))) %>%
mutate(go.nogo_omit = case_when(Accuracy > .60 ~ 0,
Accuracy <= .60 ~ 1))
# Omit rows based on exclusion criteria
data_nogoT$Response[which(data_nogoT$go.nogo_omit == 1)] <- NA
# Create Signal Detection Theory categories for each row
data_nogoT <- data_nogoT %>%
filter(display == "Trials") %>%
mutate(SDT = case_when((Array %in% c("H.png",  "T.png") & Response == "Go") ~ "Hit",
(Array %in% c("H.png", "T.png") & Response == "No Go") ~ "Miss",
(Array == "N.png" & Response == "Go") ~ "False Alarm",
(Array == "N.png" & Response == "No Go") ~ "Correct Rejection")) %>%
mutate(Hit = case_when(SDT == "Hit" ~ 1,
SDT != "Hit" ~ 0),
Miss = case_when(SDT == "Miss" ~ 1,
SDT != "Miss" ~ 0),
FA = case_when(SDT == "False Alarm" ~ 1,
SDT != "False Alarm" ~ 0),
CR = case_when(SDT == "Correct Rejection" ~ 1,
SDT != "Correct Rejection" ~ 0))
# Calculate the mean and SD of hits and FA (trials with a go response)
data_go_RT <- data_nogoT %>%
filter(SDT %in% c("Hit", "FA")) %>%
group_by(Participant.Private.ID) %>%
summarise(MeanGoRT = mean(Reaction.Time, na.rm = TRUE),
SDGoRT = sd(Reaction.Time, na.rm = TRUE))
# Compute D-Prime and Bias for each participant
# https://www.rdocumentation.org/packages/psycho/versions/0.6.1/topics/dprime
# http://wise.cgu.edu/wise-tutorials/tutorial-signal-detection-theory/signal-detection-d-defined-2/
data_dprime <- data_nogoT %>%
group_by(Participant.Private.ID) %>%
summarise(dp = dprime(n_hit = sum(Hit, na.rm = TRUE),
n_fa = sum(FA, na.rm = TRUE),
n_miss = sum(Miss, na.rm = TRUE),
n_cr = sum(CR, na.rm = TRUE),
n_targets = 300,
n_distractors = 100))
# Add a row to specify what the five different values given mean
data_dprime$value <- rep_len(c("GNGdprime", "GNGbeta", "GNGaprime", "GNGbppd", "GNGc"),
length.out = nrow(data_dprime))
# Into wide format
data_dprime <- data_dprime %>%
pivot_wider(names_from = value, values_from = dp)
data_GNGdprime <- as.data.frame(data_dprime)
# Create a sum score of each response category for each participant
data_nogoT_final <- data_nogoT %>%
select(Participant.Private.ID, Hit, Miss, FA, CR) %>%
group_by(Participant.Private.ID) %>%
summarise(GNGHitSum = sum(Hit, na.rm = TRUE),
GNGMissSum = sum(Miss, na.rm = TRUE),
GNGFASum = sum(FA, na.rm = TRUE),
GNGCRSum = sum(CR, na.rm = TRUE))
# Combine the dataframes (keep all rows)
data_nogoT_final <- merge(data_nogoT_final, data_GNGdprime,
by = "Participant.Private.ID", all = TRUE)
data_nogoT_final <- merge(data_nogoT_final, data_go_RT,
by = "Participant.Private.ID", all = TRUE)
# Exclude participants with invalid responses
data_ConsentValidity <- data_Q_total %>%
select(Participant.Private.ID, Language, Validity.quantised)
# Final dataset
data_nogoT_final <- merge(data_nogoT_final, data_ConsentValidity,
by = "Participant.Private.ID")
# Exclude participants according to validity + language criteria; exclude the participants who stopped mid-task; exclude participants whose sum score for all hit/miss/cr/fa are 0s
data_nogoT_final <- data_nogoT_final %>%
filter(Validity.quantised %in% c(1, NA, "") & Language %in% c("Sujuva / Ã¤idinkieli", "Keskitaso / keskusteleva", "Muu, mikÃ¤?")) %>%
filter(!Participant.Private.ID %in% c("5201242", "4886650", "5117494", "5282634", "5500754", "5552405", "5635997")) %>%
filter(!(GNGHitSum == 0 & GNGMissSum == 0 & GNGFASum == 0 & GNGCRSum == 0))
# Final set of columns
data_nogoT_final <- data_nogoT_final %>%
select(Participant.Private.ID, GNGHitSum, GNGMissSum, GNGFASum, GNGCRSum, MeanGoRT, SDGoRT, GNGdprime, GNGbeta, GNGaprime, GNGbppd, GNGc)
# Change columns into numeric
nogo <- c("GNGHitSum", "GNGMissSum", "GNGFASum", "GNGCRSum", "MeanGoRT", "SDGoRT", "GNGdprime", "GNGbeta", "GNGaprime", "GNGbppd", "GNGc")
for (i in 1:length(nogo)) {
data_nogoT_final[, nogo[i]] <- as.numeric(data_nogoT_final[, nogo[i]])
}
# Describe
describe(data_nogoT_final[, nogo])
par(mfrow = c(2, 3))
for (i in 1:length(nogo)) {
hist(data_nogoT_final[, nogo[i]],
main = colnames(data_nogoT_final[nogo[i]]),
col = sample(colors(), 1),
xlab = "")
}
# Omit experiment-general columns & practice (etc.) rows
data_NavonT <- data_NavonT %>%
select(Participant.Private.ID, Spreadsheet:Image) %>%
filter(display %in% c("task1", "task2")) %>%
filter(Screen.Name == "Screen 3")
# Create a column for consistency
data_NavonT <- data_NavonT %>%
mutate(Consistency = case_when((Image == "bigHsmallH.png" | Image == "bigSsmallS.png") ~ "Consistent",
(Image == "bigHsmallS.png" | Image == "bigSsmallH.png") ~ "Inconsistent"))
data_NavonT$Consistency <- as.factor(data_NavonT$Consistency)
data_NavonT$display <- as.factor(data_NavonT$display)
# Accuracy for each task and consistency
Accuracy_Navon <- data_NavonT %>%
group_by(Consistency, display) %>%
summarise(Accuracy_cond = sum(Correct, na.rm = TRUE) / (sum(Correct, na.rm = TRUE) + sum(Incorrect, na.rm = TRUE)))
# Exclusion Criteria: Trial-Level
# RT <= 150ms
data_NavonT$Reaction.Time[which(data_NavonT$Reaction.Time <= 150)] <- NA
# Exclusion Criteria: Participant-Level
# <= 60% accuracy
data_NavonT <- data_NavonT %>%
group_by(Participant.Private.ID) %>%
mutate(Accuracy = sum(Correct, na.rm = TRUE) / (sum(Correct, na.rm = TRUE) + sum(Incorrect, na.rm = TRUE))) %>%
mutate(Navon_omit = case_when(Accuracy > .60 ~ 0,
Accuracy <= .60 ~ 1))
# Omit data based on Navon_omit
data_NavonT$Reaction.Time[which(data_NavonT$Navon_omit == 1)] <- NA
# Subset to only look at correct answers
data_NavonT <- data_NavonT %>%
filter(Correct == "1")
# Ungroup df
data_NavonT <- ungroup(data_NavonT)
# Global SD 1 (consistent trials only)
sd_Global_con <- data_NavonT %>%
filter((Consistency == "Consistent") & display == "task1") %>%
summarise(sd(Reaction.Time, na.rm = TRUE))
sd_Global_con <- sd_Global_con[[1]]
# Local SD 1 (consistent trials only)
sd_Local_con <- data_NavonT %>%
filter((Consistency == "Consistent") & display == "task2") %>%
summarise(sd(Reaction.Time, na.rm = TRUE))
sd_Local_con <- sd_Local_con[[1]]
# Local SD 2 (inconsistent trials only)
sd_Local_incon <- data_NavonT %>%
filter((Consistency == "Inconsistent") & display == "task2") %>%
summarise(sd(Reaction.Time, na.rm = TRUE))
sd_Local_incon <- sd_Local_incon[[1]]
# Calculate pooled SD
# https://www.statisticshowto.com/pooled-standard-deviation/
# Pooled SD consistent (local and global)
pooled_SD_con <- sqrt((sd_Global_con^2 + sd_Local_con^2)/2)
# Pooled SD local (consistent and inconsistent)
pooled_SD_local <- sqrt((sd_Local_incon^2 + sd_Local_con^2)/2)
# Final form of the Navon data
# Select relevant columns and transform the data so columns reflect mean reaction times in each of the four conditions (local/global x consistent/inconsistent)
data_NavonT_final <- data_NavonT %>%
select(Participant.Private.ID, Consistency, display, Reaction.Time) %>%
group_by(Participant.Private.ID, Consistency, display) %>%
summarise(NavonReactionTimeMean = mean(Reaction.Time, na.rm = TRUE),
.groups = "keep") %>%
pivot_wider(names_from = c(Consistency, display),
values_from = NavonReactionTimeMean)
# Global-local precedence index: Standardized mean difference (cohen’s d) in RT between global and local judgments on consistent trials only
data_NavonT_final <- data_NavonT_final %>%
mutate(GlobalToLocalPrecedence = ((Consistent_task1 - Consistent_task2) / as.numeric(pooled_SD_con)))
# Global-to-local interference index: Standardized mean difference (cohen’s d) in RT between inconsistent and consistent trials in local condition only
data_NavonT_final <- data_NavonT_final %>%
mutate(GlobalToLocalInterference = ((Inconsistent_task1 - Consistent_task2) / as.numeric(pooled_SD_local)))
# Exclude nonvalid participants
data_NavonT_final <- merge(data_NavonT_final, data_ConsentValidity,
by = "Participant.Private.ID")
# Validity/languege filter + get rid of participants who stopped mid-task + select relevant columns
data_NavonT_final <- data_NavonT_final %>%
filter(Validity.quantised %in% c(1, NA, "") & Language %in% c("Sujuva / Ã¤idinkieli", "Keskitaso / keskusteleva","Muu, mikÃ¤?")) %>%
filter(Participant.Private.ID != "4907987" & Participant.Private.ID != "4960307" & Participant.Private.ID != "5317567" & Participant.Private.ID != "5372338" & Participant.Private.ID != "5526450" & Participant.Private.ID != "5578226" & Participant.Private.ID != "5626669" & Participant.Private.ID != "5808176") %>%
select(Participant.Private.ID, Consistent_task1, Consistent_task2, Inconsistent_task1, Inconsistent_task2, GlobalToLocalPrecedence, GlobalToLocalInterference)
# Two participants' reaction times were replaced with NAs due to their accuracy being below 60%; this produced NaNs in the calculations, so the NaNs are here replaced with NAs
data_NavonT_final <- data_NavonT_final %>%
filter(Participant.Private.ID != "5608075" & Participant.Private.ID != "5411218")
# Describe
Navon <- c("Consistent_task1", "Consistent_task2", "Inconsistent_task1", "Inconsistent_task2", "GlobalToLocalPrecedence", "GlobalToLocalInterference")
describe(data_NavonT_final[, Navon])
par(mfrow = c(2, 3))
for (i in 1:length(Navon)) {
hist(data_NavonT_final[, Navon[i]],
main = colnames(data_NavonT_final[Navon[i]]),
col = sample(colors(), 1),
xlab = "")
}
# Omit irrelevant rows and columns
data_NeckerT <- data_NeckerT %>%
select(Participant.Private.ID, Spreadsheet:Image) %>%
filter(display %in% c("Trial 1", "Trial 2"))
# Calculate sum score for how many times space bar was hit in total
data_NeckerT_final <- data_NeckerT %>%
select(Participant.Private.ID, Response) %>%
group_by(Participant.Private.ID) %>%
summarise(NeckerCountTotal = sum(Response == "space", na.rm = TRUE))
# Calculate switches per second
data_NeckerT_final <- data_NeckerT_final %>%
mutate(NeckerTotalRate = NeckerCountTotal/60)
# Merge to exclude participants
data_NeckerT_final <- merge(data_NeckerT_final, data_ConsentValidity,
by = "Participant.Private.ID")
# Subset by validity and consent info + omit the irrelevant columns
data_NeckerT_final <- data_NeckerT_final %>%
filter(Validity.quantised %in% c("1", NA, "") & Language %in% c("Sujuva / Ã¤idinkieli", "Keskitaso / keskusteleva", "Muu, mikÃ¤?")) %>%
filter(Participant.Private.ID != "5385143") %>%
select(Participant.Private.ID, NeckerCountTotal, NeckerTotalRate)
# Describe
describe(data_NeckerT_final$NeckerTotalRate)
hist(data_NeckerT_final$NeckerTotalRate,
main = "Necker Total Rate",
col = sample(colors(), 1),
xlab = "")
# Load data
data_UUT <- read.csv("UUT_scoring_COMBINED_REVISED_Rcsv.csv", sep = ";")
# Rename participant ID column
names(data_UUT)[1] <- "Participant.Private.ID"
# Select relevant columns + calculate the fluency and flexibility sum score for each participant per rater
data_UUT <- data_UUT %>%
select(Participant.Private.ID, S.Fluency, S.Flex, K.Fluency, K.Flex) %>%
group_by(Participant.Private.ID) %>%
summarise(SFlexibilitySum = sum(S.Flex), SFluencySum = sum(S.Fluency),
KFlexibilitySum = sum(K.Flex), KFluencySum = sum(K.Fluency))
# Calculate participant scores as a mean of the two raters' scores (one for fluency, one for flexibility)
data_UUT$FlexSum <- rowMeans(cbind(data_UUT$SFlexibilitySum,
data_UUT$KFlexibilitySum),
na.rm = TRUE)
data_UUT$FluencySum <- rowMeans(cbind(data_UUT$SFluencySum,
data_UUT$KFluencySum),
na.rm = TRUE)
# Check validity + extract relevant columns
data_UUT <- merge(data_UUT, data_ConsentValidity,
by = "Participant.Private.ID")
data_UUT <- data_UUT %>%
filter(Validity.quantised %in% c("1", NA, "") & Language %in% c("Sujuva / Ã¤idinkieli", "Keskitaso / keskusteleva", "Muu, mikÃ¤?")) %>%
select(Participant.Private.ID, FlexSum, FluencySum)
# Describe
uut <- c("FlexSum", "FluencySum")
describe(data_UUT[, uut])
par(mfrow = c(2, 1))
for (i in 1:length(uut)) {
hist(data_UUT[, uut[i]],
main = colnames(data_UUT[uut[i]]),
col = sample(colors(), 1),
xlab = "")
}
# Merge dataframes
data_T_total1 <- merge(data_nogoT_final, data_NavonT_final,
by = "Participant.Private.ID", all = TRUE)
data_T_total2 <- merge(data_UUT, data_NeckerT_final,
by = "Participant.Private.ID", all = TRUE)
data_T_total <- merge(data_T_total1, data_T_total2,
by = "Participant.Private.ID", all = TRUE)
rm(data_T_total1, data_T_total2)
# Extract the relevant columns
data_T_sub <- data_T_total %>%
select(Participant.Private.ID, GNGdprime, GNGbeta, MeanGoRT, SDGoRT, GlobalToLocalPrecedence, GlobalToLocalInterference, NeckerTotalRate, FlexSum, FluencySum)
# Participant ID into factor
data_T_sub$Participant.Private.ID <- as.factor(data_T_sub$Participant.Private.ID)
# Task variables
t_var <- c("GNGdprime", "GNGbeta", "MeanGoRT", "SDGoRT", "GlobalToLocalPrecedence", "GlobalToLocalInterference", "NeckerTotalRate", "FlexSum", "FluencySum")
# Add a column for count of missing data per ppt
data_T_sub$t_missingdata <- rowSums(is.na(data_T_sub[, t_var]))
table(data_T_sub$t_missingdata)
plot(sort(data_T_sub$t_missingdata))
# Merge demographic information to investigate missingness
# NOTE: what to do about the age correlation?
data_T_all <- data_Q_total %>%
select(Participant.Private.ID, gender, Age, Country, Language, Education)
data_T_sub <- merge(data_T_all, data_T_sub,
by = "Participant.Private.ID", all = TRUE)
gg_miss_fct(x = data_T_sub, fct = Age)
cor.test(data_T_sub$Age, data_T_sub$t_missingdata,
method = "spearman", exact = FALSE)
# Remove participants with 4 or more missing task variables
data_T_sub <- data_T_sub %>%
group_by(Participant.Private.ID) %>%
filter(t_missingdata < 4) %>%
select(-t_missingdata)
# Normality of variables: visualize with histograms
data_T_sub <- as.data.frame(data_T_sub)
par(mfrow = c(2, 2))
for (i in 1:length(t_var)) {
hist(data_T_sub[, t_var[i]],
main = colnames(data_T_sub[t_var[i]]),
col = sample(colors(), 1),
xlab = "")
}
# Describe: mean, skew, kurtosis
describe(data_T_sub[, t_var])
# Transforming all variables with absolute skew > 1
data_T_sub$GNGbetalog <- log10(data_T_sub$GNGbeta)
data_T_sub$SDGoRTlog <- log10(data_T_sub$SDGoRT)
data_T_sub$GlobalToLocalInterferencelog <- log10(data_T_sub$GlobalToLocalInterference + 3) # added 3 because log10 cannot handle 0s/negative numbers
data_T_sub$NeckerTotalRatelog <- log10(data_T_sub$NeckerTotalRate + 0.5) # added 1 because log10 cannot handle 0s
data_T_sub$GlobalToLocalPrecedencelog <- log10(max(data_T_sub$GlobalToLocalPrecedence + 1, na.rm = TRUE) - data_T_sub$GlobalToLocalPrecedence)
# Task variables: transformed
t_var_log <- c("GNGdprime", "GNGbetalog", "MeanGoRT", "SDGoRTlog", "GlobalToLocalPrecedencelog", "GlobalToLocalInterferencelog", "NeckerTotalRatelog", "FlexSum", "FluencySum")
# Describe
describe(data_T_sub[, t_var_log])
# Visualize
par(mfrow = c(2, 2))
for (i in 1:length(t_var_log)) {
hist(data_T_sub[, t_var_log[i]],
main = colnames(data_T_sub[t_var_log[i]]),
col = sample(colors(), 1),
xlab = "")
}
# Correlation matrix
round(cor(data_T_sub[, t_var_log], method = "pearson",
use = "pairwise.complete.obs"), 2)
abs(round(cor(data_T_sub[, t_var_log], method = "pearson",
use = "pairwise.complete.obs"), 2)) > .85
corrplot(cor(data_T_sub[, t_var_log], method = "pearson",
use = "pairwise.complete.obs"))
# Exclude FluencySum as it correlates highly with FlexSum; only include log-transformed variables where necessary
data_T_sub <- data_T_sub %>%
select(Participant.Private.ID, GNGdprime, GNGbetalog, MeanGoRT, SDGoRTlog, GlobalToLocalInterferencelog, GlobalToLocalPrecedencelog, NeckerTotalRatelog, FlexSum)
data_all_sub_cleaned <- merge(data_Q_sub, data_T_sub,
by = "Participant.Private.ID",
all = TRUE)
# Ungroup the dataframe
ungroup(data_all_sub_cleaned)
# Select relevant columns
data_all_sub_cleaned <- data_all_sub_cleaned %>%
select(-Participant.Private.ID)
# Check missing data
sort(colSums(is.na(data_all_sub_cleaned)))
sort(rowSums(is.na(data_all_sub_cleaned)))
# As more participants were excluded based on their task data than based on their questionnaire data, there are participants that have more missing variables than they should based on the thresholds set for tasks and questionnaires (7+3=10).
# NOTE: not sure what to do here, will remove them for now
data_all_sub_cleaned <- data_all_sub_cleaned[-which(rowSums(is.na(data_all_sub_cleaned)) > 10),]
# Rename
data_efa <- data_all_sub_cleaned
# Remove all rows with missing values
data_efa <- na.omit(data_efa)
# Describe
par(mfrow = c(2, 5))
for (i in 1:ncol(data_efa)) {
hist(data_efa[, i], col = sample(colors(), 1))
}
describe(data_efa)
# Check how much data is missing per row and column
# sort(table(colSums(is.na(data_efa))))
# sort(table(rowSums(is.na(data_efa))))
# % of missing data overall
# sum(is.na(data_efa))/(ncol(data_efa)*nrow(data_efa))*100 # 0.9% of data missing
# NOTE: do we want to impute? 127 participants with no missing data, 139 overall
