---
title: "Citizen's Initiative Draft Code"
author: "Caitlin Dawson"
date: "15 6 2022"
output: html_document
---

# Notes

This is a draft version of code to format and process data from the Citizen's Initiative Task, meant to be combined with the full code Notebook after testing.

Notes: 
Last edited 15.6.2022 by Caitlin. Takes data downloaded from Gorilla in **blinded, semicolon separater, csv format, short form**, versions 22-25.

Version history: 
*version 23: HJ updated the experiment (a source was deleted from CI task so this version has only 5 sources. The Viiskunta source was taken down so we could not get a stable URL for it.)
*version 24: HJ updated CI task to include all 6 sources again, Viiskunta source as an image
*version 25: changes made elsewhere, CI is same as version 24

- Missing data needs to be replaced with NA
- version 22 has the full set of 6 links
- version 23 has fewer rows per ppt because of only 5 sources. All sources are links. 
- versions 24 and 25 have all sources but one source as a PDF. Find the source implementation in the column 'display'

# Check the Validity and Demographics

This chunk removes nonvalid participants according to the validity check and language exclusion criteria.

```{r Load Data, include=FALSE}

# Exclude nonvalid participants
data_citiT <- merge(data_citiT, data_ConsentValidity, 
                    by = "Participant.Private.ID")

# Validity/languege filter + select relevant columns
data_citiT <- data_citiT %>%
  filter(Validity.quantised %in% c(1, NA, "") & Language %in% c("Sujuva / Ã¤idinkieli", "Keskitaso / keskusteleva","Muu, mikÃ¤?")) %>% 
  select(Participant.Private.ID, Spreadsheet.Row:Reaction.Time, Response, display, Text, Source_topic)

# Remove the Instructions/Finish rows
data_citiT <- data_citiT %>%
  filter(display != "Instructions" & display!= "Finish")

```

# Code Source Types

```{r Code Source Types}

# Add a column to code the source authority: 1=Authority, 2=Personal
data_citiT <- data_citiT %>%
  mutate(source_authority = case_when((Source_topic == "Turvetuotanto on ajettava alas â€“ oikeudenmukaisesti") ~ 1,
                                      (Source_topic == "Suo syntyy uudestaan") ~ 1,
                                      (Source_topic == "Palaako peltoviljely pÃ¤Ã¤osin kangasmaille?") ~ 1,
                                      (Source_topic == "Punnittua puhetta turpeesta") ~ 2,
                                      (Source_topic == "Turve on uusiutuva luonnonvara") ~ 2,
                                      (Source_topic == "Auttaa akneen, reumaan, selluliittiin ja hiustenlÃ¤htÃ¶Ã¶n â€“ mikÃ¤ se on?") ~ 2))

# Add a column to code the source quality: 1=Reliable, 2=Unreliable, 3=irrelevant
data_citiT <- data_citiT %>%
  mutate(source_quality = case_when((Source_topic == "Turvetuotanto on ajettava alas â€“ oikeudenmukaisesti") ~ 1,
                                    (Source_topic == "Suo syntyy uudestaan") ~ 2,
                                    (Source_topic == "Palaako peltoviljely pÃ¤Ã¤osin kangasmaille?") ~ 3,
                                    (Source_topic == "Punnittua puhetta turpeesta") ~ 1,
                                    (Source_topic == "Turve on uusiutuva luonnonvara") ~ 2,
                                    (Source_topic == "Auttaa akneen, reumaan, selluliittiin ja hiustenlÃ¤htÃ¶Ã¶n â€“ mikÃ¤ se on?") ~ 3))

```

# Extract Reaction Times

## Reading Times

From Caitlin: RT data has issues for outlier removal due to the distribution, not sure yet the best method for this kind of task. We don't set an automatic upper limit and we cannot assume that the reading times represent a single cognitive process. However, I don't think we can keep all times in, since they range from one second to over an hour, with the mean around 3 minutes. The goal should be to capture as much valid data as possible, with the acknowledgement that times e.g. 1 second are valid but mean something different than longer times, e.g. someone has skimmed or skipped that source. 
* should we look into the thresholds for skimming and reading comprehension to make a point about which times we can expect to mean actually reading and which are skipping? 
* it is clear where obviously meaningless long outliers are in this data, but the question is how to justify their removal 
* can reading times be analyzed like reaction times, or is there another way? 

From Milla: I think we need separate exclusion criteria (possibly) for the overall RT (i.e., if someone went through the entire task super quickly), the reading times for each source (i.e., if someone went through a particular source very quickly), and for the Likert RTs (i.e., if someone was so quick at responsing to the questions that it can be assumed they didn't even read the question, although this might actually be true as participants might memorize the questions quickly)
* also need to think about whether we want to set maximum time limits (and if so, what these would reflect)
* can we have some missing data on a participant-level for this task (from an analysis point of view)?
* should we have exclusion criteria for someone just choosing the same response over and over again? Not sure if this is relevant here as all likert scales are presented on different pages (there is not the same effect of just clicking though buttons on the same page)

```{r Extract Reading Times}

# Extract reaction time for the entire task to a different df
data_citiT_overallRTs <- data_citiT %>% 
  filter(Trial.Number == "END TASK") %>% 
  select(Participant.Private.ID, Reaction.Time)
names(data_citiT_overallRTs)[2] <- "overall_rt"

# Remove BEGIN TASK and END TASK rows from df
data_citiT <- data_citiT %>% 
  filter(!Trial.Number %in% c("END TASK", "BEGIN TASK"))

# Extract reaction times for each source
# NOTE: NOT SURE ABOUT THIS
# Baseline SAM and 4 baseline topic questions (about the petition) coded as order = 1; Rest of the trials coded based on Source_topic so that order numbers always refer to the same source
data_citiT <- data_citiT %>% 
  mutate(Order = case_when((Source_topic == "") ~ 1,
                           (Source_topic == "Auttaa akneen, reumaan, selluliittiin ja hiustenlÃ¤htÃ¶Ã¶n â€“ mikÃ¤ se on?") ~ 2,
                           (Source_topic == "Palaako peltoviljely pÃ¤Ã¤osin kangasmaille?") ~ 3,
                           (Source_topic == "Punnittua puhetta turpeesta") ~ 4,
                           (Source_topic == "Suo syntyy uudestaan") ~ 5,
                           (Source_topic == "Turve on uusiutuva luonnonvara") ~ 6,
                           (Source_topic == "Turvetuotanto on ajettava alas â€“ oikeudenmukaisesti") ~ 7))
data_citiT_srcRTs <- data_citiT %>% 
  filter(Zone.Name == "advancementZone") %>% 
  select(Participant.Private.ID, Order, display, Reaction.Time)
names(data_citiT_srcRTs)[4] <- "src_rt"

# Combine overall RTs and source RTs
data_citiT_RTs <- merge(data_citiT_overallRTs, data_citiT_srcRTs,
                        by = "Participant.Private.ID",
                        all = TRUE)

# Overall RTs
describe(data_citiT_overallRTs$overall_rt/60000) # in minutes
hist(data_citiT_overallRTs$overall_rt/60000) # in minutes
plot(sort(data_citiT_overallRTs$overall_rt/60000)) # in minutes
# NOTE: do we want to exclude based on overall task completion time? There is a clear "gap" between 122min and 292min; or maybe we want to exclude participants who took less then x minutes to finish the task?

# Reading RTs (for all sources)
describe(data_citiT_RTs$src_rt/60000) # in minutes
hist(data_citiT_RTs$src_rt/60000,
     main = "Online source RTs",
     col = sample(colors(), 1),
     xlab = "", 
     breaks = 100)
plot(sort(data_citiT_RTs$src_rt/60000)) # in minutes
# NOTE: again, what should we exclude based upon? what should the minimum be? what about the maximum? I feel IQR is not good with skewed data, we should rather base it on something else

# Reading RTs: Grouped summaries by display (online source vs PDF)
tapply(X = data_citiT_RTs$src_rt/60000, # in minutes
       INDEX = data_citiT_RTs$display, 
       FUN = summary)
par(mfrow = c(1, 2))
hist(data_citiT_RTs$src_rt[which(data_citiT_RTs$display == "Reading")]/60000,
     main = "Online source RTs",
     col = sample(colors(), 1),
     xlab = "", 
     breaks = 100)
hist(data_citiT_RTs$src_rt[which(data_citiT_RTs$display == "PDF")]/60000,
     main = "PDF RTs",
     col = sample(colors(), 1),
     xlab = "", 
     breaks = 100)
plot(sort(data_citiT_RTs$src_rt[which(data_citiT_RTs$display == "Reading")]/60000),
     ylab = "Online source")
plot(sort(data_citiT_RTs$src_rt[which(data_citiT_RTs$display == "PDF")]/60000),
     ylab = "PDF")

```

## Likert Response Times

```{r Extract Likert Response Times}

# Extract reaction times for the Likert questions
data_citiT_likertRTs <- data_citiT %>% 
  filter(Zone.Type != "continue_button") %>% 
  select(Participant.Private.ID, Order, Reaction.Time, Zone.Type)
# NOTE: what do we want to know about these?
# The other continue_button rows (ones where Zone.Name != advancementZone) are the same reaction times as the response_text_area rows before them; they are not needed here

# Describe
describe(data_citiT_likertRTs$Reaction.Time/1000) # in seconds
hist(data_citiT_likertRTs$Reaction.Time/1000, # in seconds
      main = "Likert RTs",
     col = sample(colors(), 1),
     xlab = "", 
     breaks = 100)
plot(sort(data_citiT_likertRTs$Reaction.Time/1000)) # in seconds
# NOTE: exclusion criteria???

```


# Recode the SAM Responses as Likert Intensity and Valence Scores

Recode the SAM responses as Likert intensity and valence scores and add a line to extract the relevant rows, e.g. emotional state associated with the source topic, authority and quality. Add that to the reading time df.

```{r Recode the SAM + Petition + Source Responses}

# SAM intensity
data_citiT <- data_citiT %>%
  mutate(SAMintensity = case_when((Response == "SAM_1.1.PNG") ~ 1,
                                  (Response == "SAM_1.2.PNG") ~ 2,
                                  (Response == "SAM_1.3.PNG") ~ 3,
                                  (Response == "SAM_1.4.PNG") ~ 4,
                                  (Response == "SAM_1.5.PNG") ~ 5,
                                  (Response == "SAM_1.6.PNG") ~ 6,
                                  (Response == "SAM_1.7.PNG") ~ 7,
                                  (Response == "SAM_1.8.PNG") ~ 8,
                                  (Response == "SAM_1.9.PNG") ~ 9))

# SAM valence
data_citiT <- data_citiT %>%
  mutate(SAMvalence = case_when((Response == "SAM_2.1.PNG") ~ 1,
                                (Response == "SAM_2.2.PNG") ~ 2,
                                (Response == "SAM_2.3.PNG") ~ 3,
                                (Response == "SAM_2.4.PNG") ~ 4,
                                (Response == "SAM_2.5.PNG") ~ 5,
                                (Response == "SAM_2.6.PNG") ~ 6,
                                (Response == "SAM_2.7.PNG") ~ 7,
                                (Response == "SAM_2.8.PNG") ~ 8,
                                (Response == "SAM_2.9.PNG") ~ 9))

# Curiosity (petition)
data_citiT <- data_citiT %>%
  mutate(Curiosity = case_when((Response == "En ollenkaan uteliaasti") ~ 1,
                               (Response == "Hiukan uteliaasti") ~ 2,
                               (Response == "Jokseenkin uteliaasti") ~ 3,
                               (Response == "Hyvin uteliaasti") ~ 4,
                               (Response == "ErittÃ¤in uteliaasti") ~ 5))

# Interest (petition)
data_citiT <- data_citiT %>%
  mutate(Interest = case_when(((Response == "Ei ollenkaan kiinnostava") | 
                              (Response == "En ollenkaan kiinnostavana")) ~ 1,
                              ((Response == "Hiukan kiinnostava") | 
                              (Response == "Hiukan kiinnostavana")) ~ 2,
                              ((Response == "Jokseenkin kiinnostava") | 
                              (Response == "Jokseenkin kiinnostavana")) ~ 3,
                              ((Response == "Hyvin kiinnostava") | 
                              (Response == "Hyvin kiinnostavana")) ~ 4,
                              ((Response == "ErittÃ¤in kiinnostava") | 
                              (Response == "ErittÃ¤in kiinnostavana")) ~ 5))

# Familiar (petition)
data_citiT <- data_citiT %>%
  mutate(Familiarity = case_when((Response == "Ei ollenkaan tuttu") ~ 1,
                                 (Response == "Hiukan tuttu") ~ 2,
                                 (Response == "Jokseenkin tuttu") ~ 3,
                                 (Response == "Hyvin tuttu") ~ 4,
                                 (Response == "ErittÃ¤in tuttu") ~ 5))
# Support (petition)
data_citiT <- data_citiT %>%
  mutate(Support = case_when(((Response == "Varmasti en" & display == "Reading" & Screen.Name == "Screen 11") | (Response == "Varmasti en" & display == "Initial_assessment" & Screen.Name == "Screen 1") | (Response == "Varmasti en" & display == "PDF" & Screen.Name == "Screen 8")) ~ 1,
                                  ((Response == "Luultavasti en" & display == "Reading" & Screen.Name == "Screen 11") | (Response == "Luultavasti en" & display == "Initial_assessment" & Screen.Name == "Screen 1") | (Response == "Luultavasti en" & display == "PDF" & Screen.Name == "Screen 8")) ~ 2,
                                  ((Response == "En osaa sanoa" & display == "Reading" & Screen.Name == "Screen 11") | (Response == "En osaa sanoa" & display == "Initial_assessment" & Screen.Name == "Screen 1") | (Response == "En osaa sanoa" & display == "PDF" & Screen.Name == "Screen 8")) ~ 3,
                                  ((Response == "Luultavasti kyllÃ¤" & display == "Reading" & Screen.Name == "Screen 11") | (Response == "Luultavasti kyllÃ¤" & display == "Initial_assessment" & Screen.Name == "Screen 1") | (Response == "Luultavasti kyllÃ¤" & display == "PDF" & Screen.Name == "Screen 8")) ~ 4,
                                  ((Response == "Varmasti kyllÃ¤" & display == "Reading" & Screen.Name == "Screen 11") | (Response == "Varmasti kyllÃ¤" & display == "Initial_assessment" & Screen.Name == "Screen 1") | (Response == "Varmasti kyllÃ¤" & display == "PDF" & Screen.Name == "Screen 8")) ~ 5))

# Shareability (source)
data_citiT <- data_citiT %>%
  mutate(Shareability = case_when(((Response == "Varmasti en" & display == "Reading" & Screen.Name == "Screen 2") | (Response == "Varmasti en" & display == "PDF" & Screen.Name == "Screen 7")) ~ 1,
                                      ((Response == "Luultavasti en" & display == "Reading" & Screen.Name == "Screen 2") | (Response == "Luultavasti en" & display == "PDF" & Screen.Name == "Screen 7")) ~ 2,
                                      ((Response == "En osaa sanoa" & display == "Reading" & Screen.Name == "Screen 2") | (Response == "En osaa sanoa" & display == "PDF" & Screen.Name == "Screen 7")) ~ 3,
                                      ((Response == "Luultavasti kyllÃ¤" & display == "Reading" & Screen.Name == "Screen 2") | (Response == "Luultavasti kyllÃ¤" & display == "PDF" & Screen.Name == "Screen 7")) ~ 4,
                                      ((Response == "Varmasti kyllÃ¤" & display == "Reading" & Screen.Name == "Screen 2") | (Response == "Varmasti kyllÃ¤" & display == "PDF" & Screen.Name == "Screen 7")) ~ 5))

# Convincingness (source)
data_citiT <- data_citiT %>%
  mutate(Convincingness = case_when((Response == "Ei ollenkaan vakuuttava") ~ 1,
                                    (Response == "Ei vakuuttava") ~ 2,
                                    (Response == "Neutraali") ~ 3,
                                    (Response == "Vakuuttava") ~ 4,
                                    (Response == "ErittÃ¤in vakuuttava") ~ 5))

# Expertise (source)
data_citiT <- data_citiT %>%
  mutate(Expertise = case_when((Response == "Ei ollenkaan") ~ 1,
                               (Response == "VÃ¤hÃ¤n") ~ 2,
                               (Response == "Jonkin verran") ~ 3,
                               (Response == "Paljon") ~ 4,
                               (Response == "Hyvin paljon") ~ 5))

# Reliability (source)
data_citiT <- data_citiT %>%
  mutate(Reliability = case_when((Response == "ErittÃ¤in epÃ¤luotettava") ~ 1,
                                 (Response == "EpÃ¤luotettava") ~ 2,
                                 (Response == "Ei epÃ¤luotettava eikÃ¤ luotettava") ~ 3,
                                 (Response == "Luotettava") ~ 4,
                                 (Response == "ErittÃ¤in luotettava") ~ 5))

# Extract relevant information per participant per source
data_citiT_ratings <- data_citiT %>% 
  group_by(Participant.Private.ID, Order) %>% 
  summarise(SAMintensity_final = mean(SAMintensity, na.rm = TRUE),
            SAMvalence_final = mean(SAMvalence, na.rm = TRUE),
            Curiosity_pet = mean(Curiosity, na.rm = TRUE),
            Interest_pet = mean(Interest, na.rm = TRUE),
            Familiarity_pet = mean(Familiarity, na.rm = TRUE),
            Support_pet = mean(Support, na.rm = TRUE),
            Shareability_src = mean(Shareability, na.rm = TRUE),
            Convincingness_src = mean(Convincingness, na.rm = TRUE),
            Expertise_src = mean(Expertise, na.rm = TRUE),
            Reliability_src = mean(Reliability, na.rm = TRUE))

# Replace NaNs with NAs
data_citiT_ratings[is.nan(data_citiT_ratings)] <- NA

# Check row sum NAs
colSums(is.na(data_citiT_ratings))
# Source columns should have 174 NAs except for reliability, which can have more as the participant could skip the Likert scale and only respond to the open question; the petition columns or SAM items should not have any missing values

# Extract qualitative data: reason for rating, with source rating variables
data_citiT_reasons <- data_citiT %>%
  select(Participant.Private.ID, Source_topic, source_authority, source_quality, Reliability, Expertise, Convincingness, Shareability, Support, Zone.Type, Response) %>%
  filter(Zone.Type == "response_text_area")

```
